// Copyright 2016 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package reflectdata

import (
	"fmt"
	"go/constant"
	"strconv"
	"strings"

	"cmd/compile/internal/base"
	"cmd/compile/internal/compare"
	"cmd/compile/internal/ir"
	"cmd/compile/internal/objw"
	"cmd/compile/internal/typecheck"
	"cmd/compile/internal/types"
	"cmd/internal/obj"
)

// AlgType returns the fixed-width AMEMxx variants instead of the general
// AMEM kind when possible.
func AlgType(t *types.Type) types.AlgKind {
	a := types.AlgType(t)
	if a == types.AMEM {
		if t.Alignment() < int64(base.Ctxt.Arch.Alignment) && t.Alignment() < t.Size() {
			// For example, we can't treat [2]int16 as an int32 if int32s require
			// 4-byte alignment. See issue 46283.
			return a
		}
		switch t.Size() {
		case 0:
			return types.AMEM0
		case 1:
			return types.AMEM8
		case 2:
			return types.AMEM16
		case 4:
			return types.AMEM32
		case 8:
			return types.AMEM64
		case 16:
			return types.AMEM128
		}
	}

	return a
}

// genhash returns a symbol which is the closure used to compute
// the hash of a value of type t.
// Note: the generated function must match runtime.typehash exactly.
func genhash(t *types.Type) *obj.LSym {
	switch AlgType(t) {
	default:
		// genhash is only called for types that have equality
		base.Fatalf("genhash %v", t)
	case types.AMEM0:
		return sysClosure("memhash0")
	case types.AMEM8:
		return sysClosure("memhash8")
	case types.AMEM16:
		return sysClosure("memhash16")
	case types.AMEM32:
		return sysClosure("memhash32")
	case types.AMEM64:
		return sysClosure("memhash64")
	case types.AMEM128:
		return sysClosure("memhash128")
	case types.ASTRING:
		return sysClosure("strhash")
	case types.AINTER:
		return sysClosure("interhash")
	case types.ANILINTER:
		return sysClosure("nilinterhash")
	case types.AFLOAT32:
		return sysClosure("f32hash")
	case types.AFLOAT64:
		return sysClosure("f64hash")
	case types.ACPLX64:
		return sysClosure("c64hash")
	case types.ACPLX128:
		return sysClosure("c128hash")
	case types.AMEM:
		// For other sizes of plain memory, we build a closure
		// that calls memhash_varlen. The size of the memory is
		// encoded in the first slot of the closure.
		closure := TypeLinksymLookup(fmt.Sprintf(".hashfunc%d", t.Size()))
		if len(closure.P) > 0 { // already generated
			return closure
		}
		if memhashvarlen == nil {
			memhashvarlen = typecheck.LookupRuntimeFunc("memhash_varlen")
		}
		ot := 0
		ot = objw.SymPtr(closure, ot, memhashvarlen, 0)
		ot = objw.Uintptr(closure, ot, uint64(t.Size())) // size encoded in closure
		objw.Global(closure, int32(ot), obj.DUPOK|obj.RODATA)
		return closure
	case types.ASPECIAL:
		break
	}

	closure := TypeLinksymPrefix(".hashfunc", t)
	if len(closure.P) > 0 { // already generated
		return closure
	}

	// Generate hash functions for subtypes.
	// There are cases where we might not use these hashes,
	// but in that case they will get dead-code eliminated.
	// (And the closure generated by genhash will also get
	// dead-code eliminated, as we call the subtype hashers
	// directly.)
	switch t.Kind() {
	case types.TARRAY:
		genhash(t.Elem())
	case types.TSTRUCT:
		for _, f := range t.Fields() {
			genhash(f.Type)
		}
	}

	if base.Flag.LowerR != 0 {
		fmt.Printf("genhash %v %v\n", closure, t)
	}

	fn := hashFunc(t)

	// Build closure. It doesn't close over any variables, so
	// it contains just the function pointer.
	objw.SymPtr(closure, 0, fn.Linksym(), 0)
	objw.Global(closure, int32(types.PtrSize), obj.DUPOK|obj.RODATA)

	return closure
}

func hashFunc(t *types.Type) *ir.Func {
	sym := TypeSymPrefix(".hash", t)
	if sym.Def != nil {
		return sym.Def.(*ir.Name).Func
	}

	pos := base.AutogeneratedPos // less confusing than end of input
	base.Pos = pos

	// func sym(p *T, h uintptr) uintptr
	fn := ir.NewFunc(pos, pos, sym, types.NewSignature(nil,
		[]*types.Field{
			types.NewField(pos, typecheck.Lookup("p"), types.NewPtr(t)),
			types.NewField(pos, typecheck.Lookup("h"), types.Types[types.TUINTPTR]),
		},
		[]*types.Field{
			types.NewField(pos, nil, types.Types[types.TUINTPTR]),
		},
	))
	sym.Def = fn.Nname
	fn.Pragma |= ir.Noinline // TODO(mdempsky): We need to emit this during the unified frontend instead, to allow inlining.

	typecheck.DeclFunc(fn)
	np := fn.Dcl[0]
	nh := fn.Dcl[1]

	switch t.Kind() {
	case types.TARRAY:
		// An array of pure memory would be handled by the
		// standard algorithm, so the element type must not be
		// pure memory.
		hashel := hashfor(t.Elem())

		// for i := 0; i < nelem; i++
		ni := typecheck.TempAt(base.Pos, ir.CurFunc, types.Types[types.TINT])
		init := ir.NewAssignStmt(base.Pos, ni, ir.NewInt(base.Pos, 0))
		cond := ir.NewBinaryExpr(base.Pos, ir.OLT, ni, ir.NewInt(base.Pos, t.NumElem()))
		post := ir.NewAssignStmt(base.Pos, ni, ir.NewBinaryExpr(base.Pos, ir.OADD, ni, ir.NewInt(base.Pos, 1)))
		loop := ir.NewForStmt(base.Pos, nil, cond, post, nil, false)
		loop.PtrInit().Append(init)

		// h = hashel(&p[i], h)
		call := ir.NewCallExpr(base.Pos, ir.OCALL, hashel, nil)

		nx := ir.NewIndexExpr(base.Pos, np, ni)
		nx.SetBounded(true)
		na := typecheck.NodAddr(nx)
		call.Args.Append(na)
		call.Args.Append(nh)
		loop.Body.Append(ir.NewAssignStmt(base.Pos, nh, call))

		fn.Body.Append(loop)

	case types.TSTRUCT:
		// Walk the struct using memhash for runs of AMEM
		// and calling specific hash functions for the others.
		for i, fields := 0, t.Fields(); i < len(fields); {
			f := fields[i]

			// Skip blank fields.
			if f.Sym.IsBlank() {
				i++
				continue
			}

			// Hash non-memory fields with appropriate hash function.
			if !compare.IsRegularMemory(f.Type) {
				hashel := hashfor(f.Type)
				call := ir.NewCallExpr(base.Pos, ir.OCALL, hashel, nil)
				na := typecheck.NodAddr(typecheck.DotField(base.Pos, np, i))
				call.Args.Append(na)
				call.Args.Append(nh)
				fn.Body.Append(ir.NewAssignStmt(base.Pos, nh, call))
				i++
				continue
			}

			// Otherwise, hash a maximal length run of raw memory.
			size, next := compare.Memrun(t, i)

			// h = hashel(&p.first, size, h)
			hashel := hashmem(f.Type)
			call := ir.NewCallExpr(base.Pos, ir.OCALL, hashel, nil)
			na := typecheck.NodAddr(typecheck.DotField(base.Pos, np, i))
			call.Args.Append(na)
			call.Args.Append(nh)
			call.Args.Append(ir.NewInt(base.Pos, size))
			fn.Body.Append(ir.NewAssignStmt(base.Pos, nh, call))

			i = next
		}
	}

	r := ir.NewReturnStmt(base.Pos, nil)
	r.Results.Append(nh)
	fn.Body.Append(r)

	if base.Flag.LowerR != 0 {
		ir.DumpList("genhash body", fn.Body)
	}

	typecheck.FinishFuncBody()

	fn.SetDupok(true)

	ir.WithFunc(fn, func() {
		typecheck.Stmts(fn.Body)
	})

	fn.SetNilCheckDisabled(true)

	return fn
}

func runtimeHashFor(name string, t *types.Type) *ir.Name {
	return typecheck.LookupRuntime(name, t)
}

// hashfor returns the function to compute the hash of a value of type t.
func hashfor(t *types.Type) *ir.Name {
	switch types.AlgType(t) {
	case types.AMEM:
		base.Fatalf("hashfor with AMEM type")
	case types.AINTER:
		return runtimeHashFor("interhash", t)
	case types.ANILINTER:
		return runtimeHashFor("nilinterhash", t)
	case types.ASTRING:
		return runtimeHashFor("strhash", t)
	case types.AFLOAT32:
		return runtimeHashFor("f32hash", t)
	case types.AFLOAT64:
		return runtimeHashFor("f64hash", t)
	case types.ACPLX64:
		return runtimeHashFor("c64hash", t)
	case types.ACPLX128:
		return runtimeHashFor("c128hash", t)
	}

	fn := hashFunc(t)
	return fn.Nname
}

// sysClosure returns a closure which will call the
// given runtime function (with no closed-over variables).
func sysClosure(name string) *obj.LSym {
	s := typecheck.LookupRuntimeVar(name + "Â·f")
	if len(s.P) == 0 {
		f := typecheck.LookupRuntimeFunc(name)
		objw.SymPtr(s, 0, f, 0)
		objw.Global(s, int32(types.PtrSize), obj.DUPOK|obj.RODATA)
	}
	return s
}

// geneq returns a symbol which is the closure used to compute
// equality for two objects of type t.
func geneq(t *types.Type) *obj.LSym {
	switch AlgType(t) {
	case types.ANOEQ, types.ANOALG:
		// The runtime will panic if it tries to compare
		// a type with a nil equality function.
		return nil
	case types.AMEM0:
		return sysClosure("memequal0")
	case types.AMEM8:
		return sysClosure("memequal8")
	case types.AMEM16:
		return sysClosure("memequal16")
	case types.AMEM32:
		return sysClosure("memequal32")
	case types.AMEM64:
		return sysClosure("memequal64")
	case types.AMEM128:
		return sysClosure("memequal128")
	case types.ASTRING:
		return sysClosure("strequal")
	case types.AINTER:
		return sysClosure("interequal")
	case types.ANILINTER:
		return sysClosure("nilinterequal")
	case types.AFLOAT32:
		return sysClosure("f32equal")
	case types.AFLOAT64:
		return sysClosure("f64equal")
	case types.ACPLX64:
		return sysClosure("c64equal")
	case types.ACPLX128:
		return sysClosure("c128equal")
	case types.AMEM:
		// make equality closure. The size of the type
		// is encoded in the closure.
		closure := TypeLinksymLookup(fmt.Sprintf(".eqfunc%d", t.Size()))
		if len(closure.P) != 0 {
			return closure
		}
		if memequalvarlen == nil {
			memequalvarlen = typecheck.LookupRuntimeFunc("memequal_varlen")
		}
		ot := 0
		ot = objw.SymPtr(closure, ot, memequalvarlen, 0)
		ot = objw.Uintptr(closure, ot, uint64(t.Size()))
		objw.Global(closure, int32(ot), obj.DUPOK|obj.RODATA)
		return closure
	case types.ASPECIAL:
		break
	}

	closure := TypeLinksymPrefix(".eqfunc", t)
	if len(closure.P) > 0 { // already generated
		return closure
	}

	if base.Flag.LowerR != 0 {
		fmt.Printf("geneq %v\n", t)
	}

	fn := eqFunc(eqSignature(t))

	// Generate a closure which points at the function we just generated.
	objw.SymPtr(closure, 0, fn.Linksym(), 0)
	objw.Global(closure, int32(types.PtrSize), obj.DUPOK|obj.RODATA)
	return closure
}

// TODO: generate hash function from signatures also?
// They are slightly different, at least at the moment.
func eqFunc(sig string) *ir.Func {
	sym := types.TypeSymLookup(".eq." + sig)
	if sym.Def != nil {
		return sym.Def.(*ir.Name).Func
	}
	sig0 := sig

	pos := base.AutogeneratedPos // less confusing than end of input
	base.Pos = pos

	// func sym(p, q unsafe.Pointer) bool
	fn := ir.NewFunc(pos, pos, sym, types.NewSignature(nil,
		[]*types.Field{
			types.NewField(pos, typecheck.Lookup("p"), types.Types[types.TUNSAFEPTR]),
			types.NewField(pos, typecheck.Lookup("q"), types.Types[types.TUNSAFEPTR]),
		},
		[]*types.Field{
			types.NewField(pos, typecheck.Lookup("r"), types.Types[types.TBOOL]),
		},
	))
	sym.Def = fn.Nname
	fn.Pragma |= ir.Noinline // TODO(mdempsky): We need to emit this during the unified frontend instead, to allow inlining.
	typecheck.DeclFunc(fn)
	np := fn.Dcl[0]
	nq := fn.Dcl[1]
	nr := fn.Dcl[2]

	// Label to jump to if an equality test fails.
	neq := typecheck.AutoLabel(".neq")

	// Grab known alignment of argument pointers. (ptrSize is the default.)
	align := int64(types.PtrSize)
	if len(sig) > 0 && sig[0] == sigAlign {
		sig = sig[1:]
		align, sig = parseNum(sig)
	}
	unalignedOk := base.Ctxt.Arch.CanMergeLoads

	// offset from np/nq that we're currently working on
	var off int64
	var hasCall bool

	// test takes a boolean. If it evaluates to false, short circuit
	// and return false immediately. Otherwise, keep checking.
	var lastTest ir.Node
	test := func(eq ir.Node) {
		// Buffer one test in lastTest so we can use the
		// last one as the return value.
		if lastTest != nil {
			nif := ir.NewIfStmt(pos, lastTest, nil, []ir.Node{ir.NewBranchStmt(pos, ir.OGOTO, neq)})
			fn.Body.Append(nif)
		}
		lastTest = eq
	}
	// load loads data of type t from np+off and nq+off.
	// Increments off by the size of t.
	load := func(t *types.Type) (ir.Node, ir.Node) {
		c := ir.NewBasicLit(pos, types.Types[types.TUINTPTR], constant.MakeInt64(off))
		p := ir.NewBinaryExpr(pos, ir.OUNSAFEADD, np, c)
		q := ir.NewBinaryExpr(pos, ir.OUNSAFEADD, nq, c)
		x := ir.NewStarExpr(pos, ir.NewConvExpr(pos, ir.OCONVNOP, t.PtrTo(), p))
		y := ir.NewStarExpr(pos, ir.NewConvExpr(pos, ir.OCONVNOP, t.PtrTo(), q))
		off += t.Size()
		return x, y
	}
	// compare compares x and y and jumps to neq if they are not equal.
	compare := func(x, y ir.Node) {
		test(ir.NewBinaryExpr(pos, ir.OEQ, x, y))
	}

	// We keep track of string contents that we don't compare immediately.
	// We delay comparing string contents because they might be large and
	// we'd rather compare scalars farther along in the signature first.
	var pendingStrings []int64
	flushStrings := func() {
		defer func(saveOff int64) {
			off = saveOff
		}(off)
		byte := types.Types[types.TUINT8]
		for _, x := range pendingStrings {
			off = x
			ptrA, ptrB := load(byte.PtrTo())
			len, _ := load(types.Types[types.TUINTPTR])
			// Note: we already checked that the lengths are equal.
			memeq := typecheck.LookupRuntime("memequal", byte, byte)
			test(typecheck.Call(pos, memeq, []ir.Node{ptrA, ptrB, len}, false))
			hasCall = true
		}
		pendingStrings = pendingStrings[:0]
	}

	for len(sig) > 0 {
		kind := sig[0]
		sig = sig[1:]
		switch kind {
		case sigMemory:
			var n int64
			n, sig = parseNum(sig)
			if n > 64 { // TODO: why 64?
				// For big regions, call memequal.
				c := ir.NewBasicLit(pos, types.Types[types.TUINTPTR], constant.MakeInt64(off))
				p := ir.NewBinaryExpr(pos, ir.OUNSAFEADD, np, c)
				q := ir.NewBinaryExpr(pos, ir.OUNSAFEADD, nq, c)
				len := ir.NewBasicLit(pos, types.Types[types.TUINTPTR], constant.MakeInt64(n))
				byte := types.Types[types.TUINT8]
				p2 := ir.NewConvExpr(pos, ir.OCONVNOP, byte.PtrTo(), p)
				q2 := ir.NewConvExpr(pos, ir.OCONVNOP, byte.PtrTo(), q)
				memeq := typecheck.LookupRuntime("memequal", byte, byte)
				test(typecheck.Call(pos, memeq, []ir.Node{p2, q2, len}, false))
				hasCall = true
				off += n
				n = 0
			}
			n0 := n
			for n != 0 {
				switch {
				case n >= 8 && (unalignedOk || align >= 8 && off%8 == 0):
					compare(load(types.Types[types.TUINT64]))
					n -= 8
				case (n == 5 || n == 6 || n == 7) && unalignedOk && n0 >= 8:
					off -= 8 - n
					compare(load(types.Types[types.TUINT64]))
					n = 0
				case n >= 4 && (unalignedOk || align >= 4 && off%4 == 0):
					compare(load(types.Types[types.TUINT32]))
					n -= 4
				case n == 3 && unalignedOk && n0 >= 4:
					off--
					compare(load(types.Types[types.TUINT32]))
					n = 0
				case n >= 2 && (unalignedOk || align >= 2 && off%2 == 0):
					compare(load(types.Types[types.TUINT16]))
					n -= 2
				default:
					compare(load(types.Types[types.TUINT8]))
					n--
				}
			}
		case sigFloat32:
			compare(load(types.Types[types.TFLOAT32]))
		case sigFloat64:
			compare(load(types.Types[types.TFLOAT64]))
		case sigString:
			// Compare just the lengths right now.
			// Save the contents for later.
			pendingStrings = append(pendingStrings, off)
			off += int64(types.PtrSize)
			compare(load(types.Types[types.TUINTPTR]))
		case sigEface, sigIface:
			// flushStrings here to ensure that we only get a panic from
			// this interface test if all previous equality checks pass.
			flushStrings()
			typeX, typeY := load(types.Types[types.TUINTPTR].PtrTo())
			compare(typeX, typeY)
			dataX, dataY := load(types.Types[types.TUNSAFEPTR])
			var eqFn *ir.Name
			if kind == sigEface {
				eqFn = typecheck.LookupRuntime("efaceeq")
			} else {
				eqFn = typecheck.LookupRuntime("ifaceeq")
			}
			test(typecheck.Call(pos, eqFn, []ir.Node{typeX, dataX, dataY}, false))
			hasCall = true
		case sigSkip:
			var n int64
			n, sig = parseNum(sig)
			off += n
		case sigArrayStart:
			// Flush any pending test.
			flushStrings()
			// TODO: if the element comparison can't panic (no E or I), then
			// maybe we don't need to do this flushStrings?
			// On the other hand, maybe the unflushed string is not equal, but
			// a big following array is all equal.
			if lastTest != nil {
				nif := ir.NewIfStmt(pos, lastTest, nil, []ir.Node{ir.NewBranchStmt(pos, ir.OGOTO, neq)})
				fn.Body.Append(nif)
				lastTest = nil
			}

			var n int64
			n, sig = parseNum(sig)
			// Find matching closing brace.
			i := 0
			depth := 1
		findEndSquareBracket:
			for {
				if i == len(sig) {
					base.Fatalf("mismatched brackets in %s", sig0)
				}
				switch sig[i] {
				case sigArrayStart:
					depth++
				case sigArrayEnd:
					depth--
					if depth == 0 {
						break findEndSquareBracket
					}
				}
				i++
			}
			elemSig := sig[:i]
			elemSize := sigSize(elemSig)
			sig = sig[i+1:] // remaining signature after array

			// Loop N times, calling comparison function for the element.
			//     for i := off; i < off + N*elemSize; i += elemSize {
			//         if !eqfn(p+i, q+i) { goto neq }
			//     }
			elemFn := eqFunc(elemSig).Nname
			idx := typecheck.TempAt(pos, ir.CurFunc, types.Types[types.TUINTPTR])
			init := ir.NewAssignStmt(pos, idx, ir.NewInt(pos, off))
			cond := ir.NewBinaryExpr(pos, ir.OLT, idx, ir.NewInt(pos, off+n*elemSize))
			post := ir.NewAssignStmt(pos, idx, ir.NewBinaryExpr(pos, ir.OADD, idx, ir.NewInt(pos, elemSize)))

			p := ir.NewBinaryExpr(pos, ir.OUNSAFEADD, np, idx)
			q := ir.NewBinaryExpr(pos, ir.OUNSAFEADD, nq, idx)
			call := typecheck.Call(pos, elemFn, []ir.Node{p, q}, false)
			nif := ir.NewIfStmt(pos, call, nil, []ir.Node{ir.NewBranchStmt(pos, ir.OGOTO, neq)})
			loop := ir.NewForStmt(pos, init, cond, post, []ir.Node{nif}, false)
			fn.Body.Append(loop)
			off += n * elemSize

			// TODO: if the element comparison can't panic, but has strings
			// in it, maybe we do a loop first without string contents and a
			// second loop with string contents. There is no way to accomplish
			// this now they way this code works (to call the equality
			// function of the sub-signature).
		}
	}
	// Flush any pending tests.
	// The last test is used directly as a result (instead of branching using it).
	flushStrings()
	if lastTest == nil {
		lastTest = ir.NewBool(pos, true)
	}
	as := ir.NewAssignStmt(pos, nr, lastTest)
	fn.Body.Append(as)

	// ret:
	//   return
	ret := typecheck.AutoLabel(".ret")
	fn.Body.Append(ir.NewLabelStmt(pos, ret))
	fn.Body.Append(ir.NewReturnStmt(pos, nil))

	// neq:
	//   r = false
	//   return (or goto ret)
	fn.Body.Append(ir.NewLabelStmt(pos, neq))
	fn.Body.Append(ir.NewAssignStmt(pos, nr, ir.NewBool(pos, false)))
	if hasCall {
		// Epilogue is large, so share it with the equal case.
		fn.Body.Append(ir.NewBranchStmt(pos, ir.OGOTO, ret))
	} else {
		// Epilogue is small, so don't bother sharing.
		fn.Body.Append(ir.NewReturnStmt(pos, nil))
	}
	// TODO(khr): the epilogue size detection condition above isn't perfect.
	// We should really do a generic CL that shares epilogues across
	// the board. See #24936.

	if base.Flag.LowerR != 0 {
		ir.DumpList("geneq body", fn.Body)
	}

	typecheck.FinishFuncBody()

	fn.SetDupok(true)

	ir.WithFunc(fn, func() {
		typecheck.Stmts(fn.Body)
	})

	// Disable checknils while compiling this code.
	// We are comparing a struct or an array,
	// neither of which can be nil, and our comparisons
	// are shallow.
	fn.SetNilCheckDisabled(true)
	return fn
}

// EqFor returns ONAME node represents type t's equal function, and a boolean
// to indicates whether a length needs to be passed when calling the function.
// Also returns the argument type of the function (TODO: remove somehow).
func EqFor(t *types.Type) (ir.Node, bool, *types.Type) {
	switch types.AlgType(t) {
	case types.AMEM:
		return typecheck.LookupRuntime("memequal", t, t), true, t.PtrTo()
	case types.ASPECIAL:
		fn := eqFunc(eqSignature(t))
		return fn.Nname, false, types.Types[types.TUNSAFEPTR]
	}
	base.Fatalf("EqFor %v", t)
	return nil, false, nil
}

func hashmem(t *types.Type) ir.Node {
	return typecheck.LookupRuntime("memhash", t)
}

// eqSignature returns a signature of the equality function for type t.
// If two types have the same signature, they can use the same equality function.
// The signature lists the comparisons that the equality function needs
// to make, in order. So for instance, a type like:
//
//	type S struct {
//	    i int32
//	    j uint32
//	    s string
//	    e error
//	}
//
// Will have the signature "M8SI".
//
//	M8 = 8 bytes of regular memory
//	S = string
//	I = nonempty interface
//
// The content of the signature is not intended for users. It is an
// internal condensation of the comparison operations that need to be
// performed.
// (Although, note that these names might be seen in tracebacks where
// the equality test panics due to incomparable interfaces.)
//
// Full signature spec:
//
//	M%d    = %d bytes of memory that should be compared directly
//	K%d    = %d bytes of memory that should not be compared (sKip)
//	F      = float32
//	G      = float64
//	S      = string
//	I      = non-empty interface
//	E      = empty interface
//	[%d%s] = array: repeat signature %s %d times.
//	A%d    = known alignment of type pointers (defaults to ptrSize)
//
// An alignment directive is only needed on platforms that can't do
// unaligned loads.
// If an alignment directive is present, it must be first.
func eqSignature(t *types.Type) string {
	var e eqSigBuilder
	if !base.Ctxt.Arch.CanMergeLoads { // alignment only matters if we can't use unaligned loads
		if a := t.Alignment(); a != int64(types.PtrSize) {
			e.r.WriteString(fmt.Sprintf("%c%d", sigAlign, a))
		}
	}
	e.build(t)
	e.flush()
	return e.r.String()
}

const (
	sigMemory     = 'M' // followed by an integer number of bytes
	sigSkip       = 'K' // followed by an integer number of bytes
	sigFloat32    = 'F'
	sigFloat64    = 'G'
	sigString     = 'S'
	sigIface      = 'I' // non-empty interface
	sigEface      = 'E' // empty interface
	sigArrayStart = '[' // followed by an iteration count, element signature, and sigArrayEnd
	sigArrayEnd   = ']'
	sigAlign      = 'A' // followed by an integer byte alignment
)

type eqSigBuilder struct {
	r       strings.Builder
	regMem  int64 // queued up region of regular memory
	skipMem int64 // queued up region of memory to skip
}

func (e *eqSigBuilder) flush() {
	if e.regMem > 0 {
		e.r.WriteString(fmt.Sprintf("%c%d", sigMemory, e.regMem))
		e.regMem = 0
	}
	if e.skipMem > 0 {
		e.r.WriteString(fmt.Sprintf("%c%d", sigSkip, e.skipMem))
		e.skipMem = 0
	}
}
func (e *eqSigBuilder) regular(n int64) {
	if e.regMem == 0 {
		e.flush()
	}
	e.regMem += n
}
func (e *eqSigBuilder) skip(n int64) {
	if e.skipMem == 0 {
		e.flush()
	}
	e.skipMem += n
}
func (e *eqSigBuilder) float32() {
	e.flush()
	e.r.WriteByte(sigFloat32)
}
func (e *eqSigBuilder) float64() {
	e.flush()
	e.r.WriteByte(sigFloat64)
}
func (e *eqSigBuilder) string() {
	e.flush()
	e.r.WriteByte(sigString)
}
func (e *eqSigBuilder) eface() {
	e.flush()
	e.r.WriteByte(sigEface)
}
func (e *eqSigBuilder) iface() {
	e.flush()
	e.r.WriteByte(sigIface)
}

func (e *eqSigBuilder) build(t *types.Type) {
	switch t.Kind() {
	case types.TINT8, types.TUINT8, types.TBOOL:
		e.regular(1)
	case types.TINT16, types.TUINT16:
		e.regular(2)
	case types.TINT32, types.TUINT32:
		e.regular(4)
	case types.TINT64, types.TUINT64:
		e.regular(8)
	case types.TINT, types.TUINT, types.TUINTPTR, types.TPTR, types.TUNSAFEPTR, types.TCHAN:
		e.regular(int64(types.PtrSize))
	case types.TFLOAT32:
		e.float32()
	case types.TFLOAT64:
		e.float64()
	case types.TCOMPLEX64:
		e.float32()
		e.float32()
	case types.TCOMPLEX128:
		e.float64()
		e.float64()
	case types.TSTRING:
		e.string()
	case types.TINTER:
		if t.IsEmptyInterface() {
			e.eface()
		} else {
			e.iface()
		}
	case types.TSTRUCT:
		var off int64
		for _, f := range t.Fields() {
			if f.Sym.IsBlank() {
				continue
			}
			if off < f.Offset {
				e.skip(f.Offset - off)
			}
			e.build(f.Type)
			off = f.Offset + f.Type.Size()
		}
		if off < t.Size() {
			e.skip(t.Size() - off)
		}
	case types.TARRAY:
		if types.AlgType(t) == types.AMEM {
			// TODO: some "regular equality" types don't hit here,
			// like [8]sync/atomic.Pointer. Figure out how to
			// handle the subtle difference between "AMEM" and
			// "can be compared byte-by-byte for equality".
			e.regular(t.Size())
			break
		}
		et := t.Elem()
		n := t.NumElem()
		switch n {
		case 0:
		case 1:
			e.build(et)
		default:
			// To keep signatures small, we can't just repeat
			// the element signature N times. Instead, we issue
			// an array into the signature. Note that this can
			// lead to a situation where two types which could
			// share an equality function do not, like
			//   struct { a, b, c, d string }   sig: SSSS
			//   [4]string                      sig: [4S]
			// That's ok, just a tad inefficient.
			//
			// The generated loops are kind of inefficient as well,
			// so unroll the loop a bit.
			const unrollSize = 32 // make loop body compare around this many bytes
			unroll := max(1, unrollSize/et.Size())
			// Do partial loops directly.
			for n%unroll != 0 {
				e.build(et)
				n--
			}
			if n == 0 {
				break
			}
			// If we only have one loop left, do it directly.
			if n == unroll {
				for range n {
					e.build(et)
				}
				break
			}
			e.flush()
			e.r.WriteString(fmt.Sprintf("%c%d", sigArrayStart, n/unroll))
			for range unroll {
				e.build(et)
			}
			e.flush()
			e.r.WriteByte(sigArrayEnd)
		}
	default:
		base.Fatalf("eqSigBuilder %v", t)
	}
}

// Parse and remove the number at the start of s.
func parseNum(s string) (int64, string) {
	n := 0
	for n < len(s) && s[n] >= '0' && s[n] <= '9' {
		n++
	}
	x, err := strconv.ParseInt(s[:n], 10, 64)
	if err != nil {
		base.Fatalf("bad integer: %s", s[:n])
	}
	return x, s[n:]
}

// sigSize returns the size of the type described by the signature.
func sigSize(sig string) int64 {
	sig0 := sig
	var size int64
	for len(sig) > 0 {
		kind := sig[0]
		sig = sig[1:]
		switch kind {
		case sigMemory, sigSkip:
			var n int64
			n, sig = parseNum(sig)
			size += n
		case sigFloat32:
			size += 4
		case sigFloat64:
			size += 8
		case sigString, sigIface, sigEface:
			size += 2 * int64(types.PtrSize)
		case sigArrayStart:
			var n int64
			n, sig = parseNum(sig)
			// Find matching closing brace.
			i := 0
			depth := 1
		findEndSquareBracket:
			for {
				if i == len(sig) {
					base.Fatalf("mismatched brackets in %s", sig0)
				}
				switch sig[i] {
				case sigArrayStart:
					depth++
				case sigArrayEnd:
					depth--
					if depth == 0 {
						break findEndSquareBracket
					}
				}
				i++
			}
			size += n * sigSize(sig[:i])
			sig = sig[i+1:]
		}
	}
	return size
}
