// Code generated from _gen/AMD64.rules using 'go generate'; DO NOT EDIT.

package ssa

import "internal/buildcfg"
import "math"
import "cmd/internal/obj"
import "cmd/compile/internal/types"

func rewriteValueAMD64(v *Value) bool {
	switch v.Op {
	case OpAMD64ADCQ:
		return rewriteValueAMD64_OpAMD64ADCQ(v)
	case OpAMD64ADCQconst:
		return rewriteValueAMD64_OpAMD64ADCQconst(v)
	case OpAMD64ADDL:
		return rewriteValueAMD64_OpAMD64ADDL(v)
	case OpAMD64ADDLconst:
		return rewriteValueAMD64_OpAMD64ADDLconst(v)
	case OpAMD64ADDLconstmodify:
		return rewriteValueAMD64_OpAMD64ADDLconstmodify(v)
	case OpAMD64ADDLload:
		return rewriteValueAMD64_OpAMD64ADDLload(v)
	case OpAMD64ADDLmodify:
		return rewriteValueAMD64_OpAMD64ADDLmodify(v)
	case OpAMD64ADDQ:
		return rewriteValueAMD64_OpAMD64ADDQ(v)
	case OpAMD64ADDQcarry:
		return rewriteValueAMD64_OpAMD64ADDQcarry(v)
	case OpAMD64ADDQconst:
		return rewriteValueAMD64_OpAMD64ADDQconst(v)
	case OpAMD64ADDQconstmodify:
		return rewriteValueAMD64_OpAMD64ADDQconstmodify(v)
	case OpAMD64ADDQload:
		return rewriteValueAMD64_OpAMD64ADDQload(v)
	case OpAMD64ADDQmodify:
		return rewriteValueAMD64_OpAMD64ADDQmodify(v)
	case OpAMD64ADDSD:
		return rewriteValueAMD64_OpAMD64ADDSD(v)
	case OpAMD64ADDSDload:
		return rewriteValueAMD64_OpAMD64ADDSDload(v)
	case OpAMD64ADDSS:
		return rewriteValueAMD64_OpAMD64ADDSS(v)
	case OpAMD64ADDSSload:
		return rewriteValueAMD64_OpAMD64ADDSSload(v)
	case OpAMD64ANDL:
		return rewriteValueAMD64_OpAMD64ANDL(v)
	case OpAMD64ANDLconst:
		return rewriteValueAMD64_OpAMD64ANDLconst(v)
	case OpAMD64ANDLconstmodify:
		return rewriteValueAMD64_OpAMD64ANDLconstmodify(v)
	case OpAMD64ANDLload:
		return rewriteValueAMD64_OpAMD64ANDLload(v)
	case OpAMD64ANDLmodify:
		return rewriteValueAMD64_OpAMD64ANDLmodify(v)
	case OpAMD64ANDNL:
		return rewriteValueAMD64_OpAMD64ANDNL(v)
	case OpAMD64ANDNQ:
		return rewriteValueAMD64_OpAMD64ANDNQ(v)
	case OpAMD64ANDQ:
		return rewriteValueAMD64_OpAMD64ANDQ(v)
	case OpAMD64ANDQconst:
		return rewriteValueAMD64_OpAMD64ANDQconst(v)
	case OpAMD64ANDQconstmodify:
		return rewriteValueAMD64_OpAMD64ANDQconstmodify(v)
	case OpAMD64ANDQload:
		return rewriteValueAMD64_OpAMD64ANDQload(v)
	case OpAMD64ANDQmodify:
		return rewriteValueAMD64_OpAMD64ANDQmodify(v)
	case OpAMD64BSFQ:
		return rewriteValueAMD64_OpAMD64BSFQ(v)
	case OpAMD64BSWAPL:
		return rewriteValueAMD64_OpAMD64BSWAPL(v)
	case OpAMD64BSWAPQ:
		return rewriteValueAMD64_OpAMD64BSWAPQ(v)
	case OpAMD64BTCQconst:
		return rewriteValueAMD64_OpAMD64BTCQconst(v)
	case OpAMD64BTLconst:
		return rewriteValueAMD64_OpAMD64BTLconst(v)
	case OpAMD64BTQconst:
		return rewriteValueAMD64_OpAMD64BTQconst(v)
	case OpAMD64BTRQconst:
		return rewriteValueAMD64_OpAMD64BTRQconst(v)
	case OpAMD64BTSQconst:
		return rewriteValueAMD64_OpAMD64BTSQconst(v)
	case OpAMD64CMOVLCC:
		return rewriteValueAMD64_OpAMD64CMOVLCC(v)
	case OpAMD64CMOVLCS:
		return rewriteValueAMD64_OpAMD64CMOVLCS(v)
	case OpAMD64CMOVLEQ:
		return rewriteValueAMD64_OpAMD64CMOVLEQ(v)
	case OpAMD64CMOVLGE:
		return rewriteValueAMD64_OpAMD64CMOVLGE(v)
	case OpAMD64CMOVLGT:
		return rewriteValueAMD64_OpAMD64CMOVLGT(v)
	case OpAMD64CMOVLHI:
		return rewriteValueAMD64_OpAMD64CMOVLHI(v)
	case OpAMD64CMOVLLE:
		return rewriteValueAMD64_OpAMD64CMOVLLE(v)
	case OpAMD64CMOVLLS:
		return rewriteValueAMD64_OpAMD64CMOVLLS(v)
	case OpAMD64CMOVLLT:
		return rewriteValueAMD64_OpAMD64CMOVLLT(v)
	case OpAMD64CMOVLNE:
		return rewriteValueAMD64_OpAMD64CMOVLNE(v)
	case OpAMD64CMOVQCC:
		return rewriteValueAMD64_OpAMD64CMOVQCC(v)
	case OpAMD64CMOVQCS:
		return rewriteValueAMD64_OpAMD64CMOVQCS(v)
	case OpAMD64CMOVQEQ:
		return rewriteValueAMD64_OpAMD64CMOVQEQ(v)
	case OpAMD64CMOVQGE:
		return rewriteValueAMD64_OpAMD64CMOVQGE(v)
	case OpAMD64CMOVQGT:
		return rewriteValueAMD64_OpAMD64CMOVQGT(v)
	case OpAMD64CMOVQHI:
		return rewriteValueAMD64_OpAMD64CMOVQHI(v)
	case OpAMD64CMOVQLE:
		return rewriteValueAMD64_OpAMD64CMOVQLE(v)
	case OpAMD64CMOVQLS:
		return rewriteValueAMD64_OpAMD64CMOVQLS(v)
	case OpAMD64CMOVQLT:
		return rewriteValueAMD64_OpAMD64CMOVQLT(v)
	case OpAMD64CMOVQNE:
		return rewriteValueAMD64_OpAMD64CMOVQNE(v)
	case OpAMD64CMOVWCC:
		return rewriteValueAMD64_OpAMD64CMOVWCC(v)
	case OpAMD64CMOVWCS:
		return rewriteValueAMD64_OpAMD64CMOVWCS(v)
	case OpAMD64CMOVWEQ:
		return rewriteValueAMD64_OpAMD64CMOVWEQ(v)
	case OpAMD64CMOVWGE:
		return rewriteValueAMD64_OpAMD64CMOVWGE(v)
	case OpAMD64CMOVWGT:
		return rewriteValueAMD64_OpAMD64CMOVWGT(v)
	case OpAMD64CMOVWHI:
		return rewriteValueAMD64_OpAMD64CMOVWHI(v)
	case OpAMD64CMOVWLE:
		return rewriteValueAMD64_OpAMD64CMOVWLE(v)
	case OpAMD64CMOVWLS:
		return rewriteValueAMD64_OpAMD64CMOVWLS(v)
	case OpAMD64CMOVWLT:
		return rewriteValueAMD64_OpAMD64CMOVWLT(v)
	case OpAMD64CMOVWNE:
		return rewriteValueAMD64_OpAMD64CMOVWNE(v)
	case OpAMD64CMPB:
		return rewriteValueAMD64_OpAMD64CMPB(v)
	case OpAMD64CMPBconst:
		return rewriteValueAMD64_OpAMD64CMPBconst(v)
	case OpAMD64CMPBconstload:
		return rewriteValueAMD64_OpAMD64CMPBconstload(v)
	case OpAMD64CMPBload:
		return rewriteValueAMD64_OpAMD64CMPBload(v)
	case OpAMD64CMPL:
		return rewriteValueAMD64_OpAMD64CMPL(v)
	case OpAMD64CMPLconst:
		return rewriteValueAMD64_OpAMD64CMPLconst(v)
	case OpAMD64CMPLconstload:
		return rewriteValueAMD64_OpAMD64CMPLconstload(v)
	case OpAMD64CMPLload:
		return rewriteValueAMD64_OpAMD64CMPLload(v)
	case OpAMD64CMPQ:
		return rewriteValueAMD64_OpAMD64CMPQ(v)
	case OpAMD64CMPQconst:
		return rewriteValueAMD64_OpAMD64CMPQconst(v)
	case OpAMD64CMPQconstload:
		return rewriteValueAMD64_OpAMD64CMPQconstload(v)
	case OpAMD64CMPQload:
		return rewriteValueAMD64_OpAMD64CMPQload(v)
	case OpAMD64CMPW:
		return rewriteValueAMD64_OpAMD64CMPW(v)
	case OpAMD64CMPWconst:
		return rewriteValueAMD64_OpAMD64CMPWconst(v)
	case OpAMD64CMPWconstload:
		return rewriteValueAMD64_OpAMD64CMPWconstload(v)
	case OpAMD64CMPWload:
		return rewriteValueAMD64_OpAMD64CMPWload(v)
	case OpAMD64CMPXCHGLlock:
		return rewriteValueAMD64_OpAMD64CMPXCHGLlock(v)
	case OpAMD64CMPXCHGQlock:
		return rewriteValueAMD64_OpAMD64CMPXCHGQlock(v)
	case OpAMD64DIVSD:
		return rewriteValueAMD64_OpAMD64DIVSD(v)
	case OpAMD64DIVSDload:
		return rewriteValueAMD64_OpAMD64DIVSDload(v)
	case OpAMD64DIVSS:
		return rewriteValueAMD64_OpAMD64DIVSS(v)
	case OpAMD64DIVSSload:
		return rewriteValueAMD64_OpAMD64DIVSSload(v)
	case OpAMD64HMULL:
		return rewriteValueAMD64_OpAMD64HMULL(v)
	case OpAMD64HMULLU:
		return rewriteValueAMD64_OpAMD64HMULLU(v)
	case OpAMD64HMULQ:
		return rewriteValueAMD64_OpAMD64HMULQ(v)
	case OpAMD64HMULQU:
		return rewriteValueAMD64_OpAMD64HMULQU(v)
	case OpAMD64LEAL:
		return rewriteValueAMD64_OpAMD64LEAL(v)
	case OpAMD64LEAL1:
		return rewriteValueAMD64_OpAMD64LEAL1(v)
	case OpAMD64LEAL2:
		return rewriteValueAMD64_OpAMD64LEAL2(v)
	case OpAMD64LEAL4:
		return rewriteValueAMD64_OpAMD64LEAL4(v)
	case OpAMD64LEAL8:
		return rewriteValueAMD64_OpAMD64LEAL8(v)
	case OpAMD64LEAQ:
		return rewriteValueAMD64_OpAMD64LEAQ(v)
	case OpAMD64LEAQ1:
		return rewriteValueAMD64_OpAMD64LEAQ1(v)
	case OpAMD64LEAQ2:
		return rewriteValueAMD64_OpAMD64LEAQ2(v)
	case OpAMD64LEAQ4:
		return rewriteValueAMD64_OpAMD64LEAQ4(v)
	case OpAMD64LEAQ8:
		return rewriteValueAMD64_OpAMD64LEAQ8(v)
	case OpAMD64MOVBELstore:
		return rewriteValueAMD64_OpAMD64MOVBELstore(v)
	case OpAMD64MOVBEQstore:
		return rewriteValueAMD64_OpAMD64MOVBEQstore(v)
	case OpAMD64MOVBEWstore:
		return rewriteValueAMD64_OpAMD64MOVBEWstore(v)
	case OpAMD64MOVBQSX:
		return rewriteValueAMD64_OpAMD64MOVBQSX(v)
	case OpAMD64MOVBQSXload:
		return rewriteValueAMD64_OpAMD64MOVBQSXload(v)
	case OpAMD64MOVBQZX:
		return rewriteValueAMD64_OpAMD64MOVBQZX(v)
	case OpAMD64MOVBatomicload:
		return rewriteValueAMD64_OpAMD64MOVBatomicload(v)
	case OpAMD64MOVBload:
		return rewriteValueAMD64_OpAMD64MOVBload(v)
	case OpAMD64MOVBstore:
		return rewriteValueAMD64_OpAMD64MOVBstore(v)
	case OpAMD64MOVBstoreconst:
		return rewriteValueAMD64_OpAMD64MOVBstoreconst(v)
	case OpAMD64MOVLQSX:
		return rewriteValueAMD64_OpAMD64MOVLQSX(v)
	case OpAMD64MOVLQSXload:
		return rewriteValueAMD64_OpAMD64MOVLQSXload(v)
	case OpAMD64MOVLQZX:
		return rewriteValueAMD64_OpAMD64MOVLQZX(v)
	case OpAMD64MOVLatomicload:
		return rewriteValueAMD64_OpAMD64MOVLatomicload(v)
	case OpAMD64MOVLf2i:
		return rewriteValueAMD64_OpAMD64MOVLf2i(v)
	case OpAMD64MOVLi2f:
		return rewriteValueAMD64_OpAMD64MOVLi2f(v)
	case OpAMD64MOVLload:
		return rewriteValueAMD64_OpAMD64MOVLload(v)
	case OpAMD64MOVLstore:
		return rewriteValueAMD64_OpAMD64MOVLstore(v)
	case OpAMD64MOVLstoreconst:
		return rewriteValueAMD64_OpAMD64MOVLstoreconst(v)
	case OpAMD64MOVOload:
		return rewriteValueAMD64_OpAMD64MOVOload(v)
	case OpAMD64MOVOstore:
		return rewriteValueAMD64_OpAMD64MOVOstore(v)
	case OpAMD64MOVOstoreconst:
		return rewriteValueAMD64_OpAMD64MOVOstoreconst(v)
	case OpAMD64MOVQatomicload:
		return rewriteValueAMD64_OpAMD64MOVQatomicload(v)
	case OpAMD64MOVQf2i:
		return rewriteValueAMD64_OpAMD64MOVQf2i(v)
	case OpAMD64MOVQi2f:
		return rewriteValueAMD64_OpAMD64MOVQi2f(v)
	case OpAMD64MOVQload:
		return rewriteValueAMD64_OpAMD64MOVQload(v)
	case OpAMD64MOVQstore:
		return rewriteValueAMD64_OpAMD64MOVQstore(v)
	case OpAMD64MOVQstoreconst:
		return rewriteValueAMD64_OpAMD64MOVQstoreconst(v)
	case OpAMD64MOVSDload:
		return rewriteValueAMD64_OpAMD64MOVSDload(v)
	case OpAMD64MOVSDstore:
		return rewriteValueAMD64_OpAMD64MOVSDstore(v)
	case OpAMD64MOVSSload:
		return rewriteValueAMD64_OpAMD64MOVSSload(v)
	case OpAMD64MOVSSstore:
		return rewriteValueAMD64_OpAMD64MOVSSstore(v)
	case OpAMD64MOVWQSX:
		return rewriteValueAMD64_OpAMD64MOVWQSX(v)
	case OpAMD64MOVWQSXload:
		return rewriteValueAMD64_OpAMD64MOVWQSXload(v)
	case OpAMD64MOVWQZX:
		return rewriteValueAMD64_OpAMD64MOVWQZX(v)
	case OpAMD64MOVWload:
		return rewriteValueAMD64_OpAMD64MOVWload(v)
	case OpAMD64MOVWstore:
		return rewriteValueAMD64_OpAMD64MOVWstore(v)
	case OpAMD64MOVWstoreconst:
		return rewriteValueAMD64_OpAMD64MOVWstoreconst(v)
	case OpAMD64MULL:
		return rewriteValueAMD64_OpAMD64MULL(v)
	case OpAMD64MULLconst:
		return rewriteValueAMD64_OpAMD64MULLconst(v)
	case OpAMD64MULQ:
		return rewriteValueAMD64_OpAMD64MULQ(v)
	case OpAMD64MULQconst:
		return rewriteValueAMD64_OpAMD64MULQconst(v)
	case OpAMD64MULSD:
		return rewriteValueAMD64_OpAMD64MULSD(v)
	case OpAMD64MULSDload:
		return rewriteValueAMD64_OpAMD64MULSDload(v)
	case OpAMD64MULSS:
		return rewriteValueAMD64_OpAMD64MULSS(v)
	case OpAMD64MULSSload:
		return rewriteValueAMD64_OpAMD64MULSSload(v)
	case OpAMD64NEGL:
		return rewriteValueAMD64_OpAMD64NEGL(v)
	case OpAMD64NEGQ:
		return rewriteValueAMD64_OpAMD64NEGQ(v)
	case OpAMD64NOTL:
		return rewriteValueAMD64_OpAMD64NOTL(v)
	case OpAMD64NOTQ:
		return rewriteValueAMD64_OpAMD64NOTQ(v)
	case OpAMD64ORL:
		return rewriteValueAMD64_OpAMD64ORL(v)
	case OpAMD64ORLconst:
		return rewriteValueAMD64_OpAMD64ORLconst(v)
	case OpAMD64ORLconstmodify:
		return rewriteValueAMD64_OpAMD64ORLconstmodify(v)
	case OpAMD64ORLload:
		return rewriteValueAMD64_OpAMD64ORLload(v)
	case OpAMD64ORLmodify:
		return rewriteValueAMD64_OpAMD64ORLmodify(v)
	case OpAMD64ORQ:
		return rewriteValueAMD64_OpAMD64ORQ(v)
	case OpAMD64ORQconst:
		return rewriteValueAMD64_OpAMD64ORQconst(v)
	case OpAMD64ORQconstmodify:
		return rewriteValueAMD64_OpAMD64ORQconstmodify(v)
	case OpAMD64ORQload:
		return rewriteValueAMD64_OpAMD64ORQload(v)
	case OpAMD64ORQmodify:
		return rewriteValueAMD64_OpAMD64ORQmodify(v)
	case OpAMD64ROLB:
		return rewriteValueAMD64_OpAMD64ROLB(v)
	case OpAMD64ROLBconst:
		return rewriteValueAMD64_OpAMD64ROLBconst(v)
	case OpAMD64ROLL:
		return rewriteValueAMD64_OpAMD64ROLL(v)
	case OpAMD64ROLLconst:
		return rewriteValueAMD64_OpAMD64ROLLconst(v)
	case OpAMD64ROLQ:
		return rewriteValueAMD64_OpAMD64ROLQ(v)
	case OpAMD64ROLQconst:
		return rewriteValueAMD64_OpAMD64ROLQconst(v)
	case OpAMD64ROLW:
		return rewriteValueAMD64_OpAMD64ROLW(v)
	case OpAMD64ROLWconst:
		return rewriteValueAMD64_OpAMD64ROLWconst(v)
	case OpAMD64RORB:
		return rewriteValueAMD64_OpAMD64RORB(v)
	case OpAMD64RORL:
		return rewriteValueAMD64_OpAMD64RORL(v)
	case OpAMD64RORQ:
		return rewriteValueAMD64_OpAMD64RORQ(v)
	case OpAMD64RORW:
		return rewriteValueAMD64_OpAMD64RORW(v)
	case OpAMD64SARB:
		return rewriteValueAMD64_OpAMD64SARB(v)
	case OpAMD64SARBconst:
		return rewriteValueAMD64_OpAMD64SARBconst(v)
	case OpAMD64SARL:
		return rewriteValueAMD64_OpAMD64SARL(v)
	case OpAMD64SARLconst:
		return rewriteValueAMD64_OpAMD64SARLconst(v)
	case OpAMD64SARQ:
		return rewriteValueAMD64_OpAMD64SARQ(v)
	case OpAMD64SARQconst:
		return rewriteValueAMD64_OpAMD64SARQconst(v)
	case OpAMD64SARW:
		return rewriteValueAMD64_OpAMD64SARW(v)
	case OpAMD64SARWconst:
		return rewriteValueAMD64_OpAMD64SARWconst(v)
	case OpAMD64SARXLload:
		return rewriteValueAMD64_OpAMD64SARXLload(v)
	case OpAMD64SARXQload:
		return rewriteValueAMD64_OpAMD64SARXQload(v)
	case OpAMD64SBBLcarrymask:
		return rewriteValueAMD64_OpAMD64SBBLcarrymask(v)
	case OpAMD64SBBQ:
		return rewriteValueAMD64_OpAMD64SBBQ(v)
	case OpAMD64SBBQcarrymask:
		return rewriteValueAMD64_OpAMD64SBBQcarrymask(v)
	case OpAMD64SBBQconst:
		return rewriteValueAMD64_OpAMD64SBBQconst(v)
	case OpAMD64SETA:
		return rewriteValueAMD64_OpAMD64SETA(v)
	case OpAMD64SETAE:
		return rewriteValueAMD64_OpAMD64SETAE(v)
	case OpAMD64SETAEstore:
		return rewriteValueAMD64_OpAMD64SETAEstore(v)
	case OpAMD64SETAstore:
		return rewriteValueAMD64_OpAMD64SETAstore(v)
	case OpAMD64SETB:
		return rewriteValueAMD64_OpAMD64SETB(v)
	case OpAMD64SETBE:
		return rewriteValueAMD64_OpAMD64SETBE(v)
	case OpAMD64SETBEstore:
		return rewriteValueAMD64_OpAMD64SETBEstore(v)
	case OpAMD64SETBstore:
		return rewriteValueAMD64_OpAMD64SETBstore(v)
	case OpAMD64SETEQ:
		return rewriteValueAMD64_OpAMD64SETEQ(v)
	case OpAMD64SETEQstore:
		return rewriteValueAMD64_OpAMD64SETEQstore(v)
	case OpAMD64SETG:
		return rewriteValueAMD64_OpAMD64SETG(v)
	case OpAMD64SETGE:
		return rewriteValueAMD64_OpAMD64SETGE(v)
	case OpAMD64SETGEstore:
		return rewriteValueAMD64_OpAMD64SETGEstore(v)
	case OpAMD64SETGstore:
		return rewriteValueAMD64_OpAMD64SETGstore(v)
	case OpAMD64SETL:
		return rewriteValueAMD64_OpAMD64SETL(v)
	case OpAMD64SETLE:
		return rewriteValueAMD64_OpAMD64SETLE(v)
	case OpAMD64SETLEstore:
		return rewriteValueAMD64_OpAMD64SETLEstore(v)
	case OpAMD64SETLstore:
		return rewriteValueAMD64_OpAMD64SETLstore(v)
	case OpAMD64SETNE:
		return rewriteValueAMD64_OpAMD64SETNE(v)
	case OpAMD64SETNEstore:
		return rewriteValueAMD64_OpAMD64SETNEstore(v)
	case OpAMD64SHLL:
		return rewriteValueAMD64_OpAMD64SHLL(v)
	case OpAMD64SHLLconst:
		return rewriteValueAMD64_OpAMD64SHLLconst(v)
	case OpAMD64SHLQ:
		return rewriteValueAMD64_OpAMD64SHLQ(v)
	case OpAMD64SHLQconst:
		return rewriteValueAMD64_OpAMD64SHLQconst(v)
	case OpAMD64SHLXLload:
		return rewriteValueAMD64_OpAMD64SHLXLload(v)
	case OpAMD64SHLXQload:
		return rewriteValueAMD64_OpAMD64SHLXQload(v)
	case OpAMD64SHRB:
		return rewriteValueAMD64_OpAMD64SHRB(v)
	case OpAMD64SHRBconst:
		return rewriteValueAMD64_OpAMD64SHRBconst(v)
	case OpAMD64SHRL:
		return rewriteValueAMD64_OpAMD64SHRL(v)
	case OpAMD64SHRLconst:
		return rewriteValueAMD64_OpAMD64SHRLconst(v)
	case OpAMD64SHRQ:
		return rewriteValueAMD64_OpAMD64SHRQ(v)
	case OpAMD64SHRQconst:
		return rewriteValueAMD64_OpAMD64SHRQconst(v)
	case OpAMD64SHRW:
		return rewriteValueAMD64_OpAMD64SHRW(v)
	case OpAMD64SHRWconst:
		return rewriteValueAMD64_OpAMD64SHRWconst(v)
	case OpAMD64SHRXLload:
		return rewriteValueAMD64_OpAMD64SHRXLload(v)
	case OpAMD64SHRXQload:
		return rewriteValueAMD64_OpAMD64SHRXQload(v)
	case OpAMD64SUBL:
		return rewriteValueAMD64_OpAMD64SUBL(v)
	case OpAMD64SUBLconst:
		return rewriteValueAMD64_OpAMD64SUBLconst(v)
	case OpAMD64SUBLload:
		return rewriteValueAMD64_OpAMD64SUBLload(v)
	case OpAMD64SUBLmodify:
		return rewriteValueAMD64_OpAMD64SUBLmodify(v)
	case OpAMD64SUBQ:
		return rewriteValueAMD64_OpAMD64SUBQ(v)
	case OpAMD64SUBQborrow:
		return rewriteValueAMD64_OpAMD64SUBQborrow(v)
	case OpAMD64SUBQconst:
		return rewriteValueAMD64_OpAMD64SUBQconst(v)
	case OpAMD64SUBQload:
		return rewriteValueAMD64_OpAMD64SUBQload(v)
	case OpAMD64SUBQmodify:
		return rewriteValueAMD64_OpAMD64SUBQmodify(v)
	case OpAMD64SUBSD:
		return rewriteValueAMD64_OpAMD64SUBSD(v)
	case OpAMD64SUBSDload:
		return rewriteValueAMD64_OpAMD64SUBSDload(v)
	case OpAMD64SUBSS:
		return rewriteValueAMD64_OpAMD64SUBSS(v)
	case OpAMD64SUBSSload:
		return rewriteValueAMD64_OpAMD64SUBSSload(v)
	case OpAMD64TESTB:
		return rewriteValueAMD64_OpAMD64TESTB(v)
	case OpAMD64TESTBconst:
		return rewriteValueAMD64_OpAMD64TESTBconst(v)
	case OpAMD64TESTL:
		return rewriteValueAMD64_OpAMD64TESTL(v)
	case OpAMD64TESTLconst:
		return rewriteValueAMD64_OpAMD64TESTLconst(v)
	case OpAMD64TESTQ:
		return rewriteValueAMD64_OpAMD64TESTQ(v)
	case OpAMD64TESTQconst:
		return rewriteValueAMD64_OpAMD64TESTQconst(v)
	case OpAMD64TESTW:
		return rewriteValueAMD64_OpAMD64TESTW(v)
	case OpAMD64TESTWconst:
		return rewriteValueAMD64_OpAMD64TESTWconst(v)
	case OpAMD64VPMOVVec16x16ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec16x16ToM(v)
	case OpAMD64VPMOVVec16x32ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec16x32ToM(v)
	case OpAMD64VPMOVVec16x8ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec16x8ToM(v)
	case OpAMD64VPMOVVec32x16ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec32x16ToM(v)
	case OpAMD64VPMOVVec32x4ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec32x4ToM(v)
	case OpAMD64VPMOVVec32x8ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec32x8ToM(v)
	case OpAMD64VPMOVVec64x2ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec64x2ToM(v)
	case OpAMD64VPMOVVec64x4ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec64x4ToM(v)
	case OpAMD64VPMOVVec64x8ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec64x8ToM(v)
	case OpAMD64VPMOVVec8x16ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec8x16ToM(v)
	case OpAMD64VPMOVVec8x32ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec8x32ToM(v)
	case OpAMD64VPMOVVec8x64ToM:
		return rewriteValueAMD64_OpAMD64VPMOVVec8x64ToM(v)
	case OpAMD64XADDLlock:
		return rewriteValueAMD64_OpAMD64XADDLlock(v)
	case OpAMD64XADDQlock:
		return rewriteValueAMD64_OpAMD64XADDQlock(v)
	case OpAMD64XCHGL:
		return rewriteValueAMD64_OpAMD64XCHGL(v)
	case OpAMD64XCHGQ:
		return rewriteValueAMD64_OpAMD64XCHGQ(v)
	case OpAMD64XORL:
		return rewriteValueAMD64_OpAMD64XORL(v)
	case OpAMD64XORLconst:
		return rewriteValueAMD64_OpAMD64XORLconst(v)
	case OpAMD64XORLconstmodify:
		return rewriteValueAMD64_OpAMD64XORLconstmodify(v)
	case OpAMD64XORLload:
		return rewriteValueAMD64_OpAMD64XORLload(v)
	case OpAMD64XORLmodify:
		return rewriteValueAMD64_OpAMD64XORLmodify(v)
	case OpAMD64XORQ:
		return rewriteValueAMD64_OpAMD64XORQ(v)
	case OpAMD64XORQconst:
		return rewriteValueAMD64_OpAMD64XORQconst(v)
	case OpAMD64XORQconstmodify:
		return rewriteValueAMD64_OpAMD64XORQconstmodify(v)
	case OpAMD64XORQload:
		return rewriteValueAMD64_OpAMD64XORQload(v)
	case OpAMD64XORQmodify:
		return rewriteValueAMD64_OpAMD64XORQmodify(v)
	case OpAbsoluteInt16x16:
		v.Op = OpAMD64VPABSW256
		return true
	case OpAbsoluteInt16x32:
		v.Op = OpAMD64VPABSW512
		return true
	case OpAbsoluteInt16x8:
		v.Op = OpAMD64VPABSW128
		return true
	case OpAbsoluteInt32x16:
		v.Op = OpAMD64VPABSD512
		return true
	case OpAbsoluteInt32x4:
		v.Op = OpAMD64VPABSD128
		return true
	case OpAbsoluteInt32x8:
		v.Op = OpAMD64VPABSD256
		return true
	case OpAbsoluteInt64x2:
		v.Op = OpAMD64VPABSQ128
		return true
	case OpAbsoluteInt64x4:
		v.Op = OpAMD64VPABSQ256
		return true
	case OpAbsoluteInt64x8:
		v.Op = OpAMD64VPABSQ512
		return true
	case OpAbsoluteInt8x16:
		v.Op = OpAMD64VPABSB128
		return true
	case OpAbsoluteInt8x32:
		v.Op = OpAMD64VPABSB256
		return true
	case OpAbsoluteInt8x64:
		v.Op = OpAMD64VPABSB512
		return true
	case OpAdd16:
		v.Op = OpAMD64ADDL
		return true
	case OpAdd32:
		v.Op = OpAMD64ADDL
		return true
	case OpAdd32F:
		v.Op = OpAMD64ADDSS
		return true
	case OpAdd64:
		v.Op = OpAMD64ADDQ
		return true
	case OpAdd64F:
		v.Op = OpAMD64ADDSD
		return true
	case OpAdd8:
		v.Op = OpAMD64ADDL
		return true
	case OpAddFloat32x16:
		v.Op = OpAMD64VADDPS512
		return true
	case OpAddFloat32x4:
		v.Op = OpAMD64VADDPS128
		return true
	case OpAddFloat32x8:
		v.Op = OpAMD64VADDPS256
		return true
	case OpAddFloat64x2:
		v.Op = OpAMD64VADDPD128
		return true
	case OpAddFloat64x4:
		v.Op = OpAMD64VADDPD256
		return true
	case OpAddFloat64x8:
		v.Op = OpAMD64VADDPD512
		return true
	case OpAddInt16x16:
		v.Op = OpAMD64VPADDW256
		return true
	case OpAddInt16x32:
		v.Op = OpAMD64VPADDW512
		return true
	case OpAddInt16x8:
		v.Op = OpAMD64VPADDW128
		return true
	case OpAddInt32x16:
		v.Op = OpAMD64VPADDD512
		return true
	case OpAddInt32x4:
		v.Op = OpAMD64VPADDD128
		return true
	case OpAddInt32x8:
		v.Op = OpAMD64VPADDD256
		return true
	case OpAddInt64x2:
		v.Op = OpAMD64VPADDQ128
		return true
	case OpAddInt64x4:
		v.Op = OpAMD64VPADDQ256
		return true
	case OpAddInt64x8:
		v.Op = OpAMD64VPADDQ512
		return true
	case OpAddInt8x16:
		v.Op = OpAMD64VPADDB128
		return true
	case OpAddInt8x32:
		v.Op = OpAMD64VPADDB256
		return true
	case OpAddInt8x64:
		v.Op = OpAMD64VPADDB512
		return true
	case OpAddPtr:
		v.Op = OpAMD64ADDQ
		return true
	case OpAddSubFloat32x4:
		v.Op = OpAMD64VADDSUBPS128
		return true
	case OpAddSubFloat32x8:
		v.Op = OpAMD64VADDSUBPS256
		return true
	case OpAddSubFloat64x2:
		v.Op = OpAMD64VADDSUBPD128
		return true
	case OpAddSubFloat64x4:
		v.Op = OpAMD64VADDSUBPD256
		return true
	case OpAddUint16x16:
		v.Op = OpAMD64VPADDW256
		return true
	case OpAddUint16x32:
		v.Op = OpAMD64VPADDW512
		return true
	case OpAddUint16x8:
		v.Op = OpAMD64VPADDW128
		return true
	case OpAddUint32x16:
		v.Op = OpAMD64VPADDD512
		return true
	case OpAddUint32x4:
		v.Op = OpAMD64VPADDD128
		return true
	case OpAddUint32x8:
		v.Op = OpAMD64VPADDD256
		return true
	case OpAddUint64x2:
		v.Op = OpAMD64VPADDQ128
		return true
	case OpAddUint64x4:
		v.Op = OpAMD64VPADDQ256
		return true
	case OpAddUint64x8:
		v.Op = OpAMD64VPADDQ512
		return true
	case OpAddUint8x16:
		v.Op = OpAMD64VPADDB128
		return true
	case OpAddUint8x32:
		v.Op = OpAMD64VPADDB256
		return true
	case OpAddUint8x64:
		v.Op = OpAMD64VPADDB512
		return true
	case OpAddr:
		return rewriteValueAMD64_OpAddr(v)
	case OpAnd16:
		v.Op = OpAMD64ANDL
		return true
	case OpAnd32:
		v.Op = OpAMD64ANDL
		return true
	case OpAnd64:
		v.Op = OpAMD64ANDQ
		return true
	case OpAnd8:
		v.Op = OpAMD64ANDL
		return true
	case OpAndB:
		v.Op = OpAMD64ANDL
		return true
	case OpAndFloat32x16:
		v.Op = OpAMD64VANDPS512
		return true
	case OpAndFloat32x4:
		v.Op = OpAMD64VANDPS128
		return true
	case OpAndFloat32x8:
		v.Op = OpAMD64VANDPS256
		return true
	case OpAndFloat64x2:
		v.Op = OpAMD64VANDPD128
		return true
	case OpAndFloat64x4:
		v.Op = OpAMD64VANDPD256
		return true
	case OpAndFloat64x8:
		v.Op = OpAMD64VANDPD512
		return true
	case OpAndInt16x16:
		v.Op = OpAMD64VPAND256
		return true
	case OpAndInt16x8:
		v.Op = OpAMD64VPAND128
		return true
	case OpAndInt32x16:
		v.Op = OpAMD64VPANDD512
		return true
	case OpAndInt32x4:
		v.Op = OpAMD64VPAND128
		return true
	case OpAndInt32x8:
		v.Op = OpAMD64VPAND256
		return true
	case OpAndInt64x2:
		v.Op = OpAMD64VPAND128
		return true
	case OpAndInt64x4:
		v.Op = OpAMD64VPAND256
		return true
	case OpAndInt64x8:
		v.Op = OpAMD64VPANDQ512
		return true
	case OpAndInt8x16:
		v.Op = OpAMD64VPAND128
		return true
	case OpAndInt8x32:
		v.Op = OpAMD64VPAND256
		return true
	case OpAndNotFloat32x16:
		v.Op = OpAMD64VANDNPS512
		return true
	case OpAndNotFloat32x4:
		v.Op = OpAMD64VANDNPS128
		return true
	case OpAndNotFloat32x8:
		v.Op = OpAMD64VANDNPS256
		return true
	case OpAndNotFloat64x2:
		v.Op = OpAMD64VANDNPD128
		return true
	case OpAndNotFloat64x4:
		v.Op = OpAMD64VANDNPD256
		return true
	case OpAndNotFloat64x8:
		v.Op = OpAMD64VANDNPD512
		return true
	case OpAndNotInt16x16:
		v.Op = OpAMD64VPANDN256
		return true
	case OpAndNotInt16x8:
		v.Op = OpAMD64VPANDN128
		return true
	case OpAndNotInt32x16:
		v.Op = OpAMD64VPANDND512
		return true
	case OpAndNotInt32x4:
		v.Op = OpAMD64VPANDN128
		return true
	case OpAndNotInt32x8:
		v.Op = OpAMD64VPANDN256
		return true
	case OpAndNotInt64x2:
		v.Op = OpAMD64VPANDN128
		return true
	case OpAndNotInt64x4:
		v.Op = OpAMD64VPANDN256
		return true
	case OpAndNotInt64x8:
		v.Op = OpAMD64VPANDNQ512
		return true
	case OpAndNotInt8x16:
		v.Op = OpAMD64VPANDN128
		return true
	case OpAndNotInt8x32:
		v.Op = OpAMD64VPANDN256
		return true
	case OpAndNotUint16x16:
		v.Op = OpAMD64VPANDN256
		return true
	case OpAndNotUint16x8:
		v.Op = OpAMD64VPANDN128
		return true
	case OpAndNotUint32x16:
		v.Op = OpAMD64VPANDND512
		return true
	case OpAndNotUint32x4:
		v.Op = OpAMD64VPANDN128
		return true
	case OpAndNotUint32x8:
		v.Op = OpAMD64VPANDN256
		return true
	case OpAndNotUint64x2:
		v.Op = OpAMD64VPANDN128
		return true
	case OpAndNotUint64x4:
		v.Op = OpAMD64VPANDN256
		return true
	case OpAndNotUint64x8:
		v.Op = OpAMD64VPANDNQ512
		return true
	case OpAndNotUint8x16:
		v.Op = OpAMD64VPANDN128
		return true
	case OpAndNotUint8x32:
		v.Op = OpAMD64VPANDN256
		return true
	case OpAndUint16x16:
		v.Op = OpAMD64VPAND256
		return true
	case OpAndUint16x8:
		v.Op = OpAMD64VPAND128
		return true
	case OpAndUint32x16:
		v.Op = OpAMD64VPANDD512
		return true
	case OpAndUint32x4:
		v.Op = OpAMD64VPAND128
		return true
	case OpAndUint32x8:
		v.Op = OpAMD64VPAND256
		return true
	case OpAndUint64x2:
		v.Op = OpAMD64VPAND128
		return true
	case OpAndUint64x4:
		v.Op = OpAMD64VPAND256
		return true
	case OpAndUint64x8:
		v.Op = OpAMD64VPANDQ512
		return true
	case OpAndUint8x16:
		v.Op = OpAMD64VPAND128
		return true
	case OpAndUint8x32:
		v.Op = OpAMD64VPAND256
		return true
	case OpApproximateReciprocalFloat32x16:
		v.Op = OpAMD64VRCP14PS512
		return true
	case OpApproximateReciprocalFloat32x4:
		v.Op = OpAMD64VRCP14PS128
		return true
	case OpApproximateReciprocalFloat32x8:
		v.Op = OpAMD64VRCP14PS256
		return true
	case OpApproximateReciprocalFloat64x2:
		v.Op = OpAMD64VRCP14PD128
		return true
	case OpApproximateReciprocalFloat64x4:
		v.Op = OpAMD64VRCP14PD256
		return true
	case OpApproximateReciprocalFloat64x8:
		v.Op = OpAMD64VRCP14PD512
		return true
	case OpApproximateReciprocalOfSqrtFloat32x16:
		v.Op = OpAMD64VRSQRT14PS512
		return true
	case OpApproximateReciprocalOfSqrtFloat32x4:
		v.Op = OpAMD64VRSQRTPS128
		return true
	case OpApproximateReciprocalOfSqrtFloat32x8:
		v.Op = OpAMD64VRSQRTPS256
		return true
	case OpApproximateReciprocalOfSqrtFloat64x2:
		v.Op = OpAMD64VRSQRT14PD128
		return true
	case OpApproximateReciprocalOfSqrtFloat64x4:
		v.Op = OpAMD64VRSQRT14PD256
		return true
	case OpApproximateReciprocalOfSqrtFloat64x8:
		v.Op = OpAMD64VRSQRT14PD512
		return true
	case OpAtomicAdd32:
		return rewriteValueAMD64_OpAtomicAdd32(v)
	case OpAtomicAdd64:
		return rewriteValueAMD64_OpAtomicAdd64(v)
	case OpAtomicAnd32:
		return rewriteValueAMD64_OpAtomicAnd32(v)
	case OpAtomicAnd32value:
		return rewriteValueAMD64_OpAtomicAnd32value(v)
	case OpAtomicAnd64value:
		return rewriteValueAMD64_OpAtomicAnd64value(v)
	case OpAtomicAnd8:
		return rewriteValueAMD64_OpAtomicAnd8(v)
	case OpAtomicCompareAndSwap32:
		return rewriteValueAMD64_OpAtomicCompareAndSwap32(v)
	case OpAtomicCompareAndSwap64:
		return rewriteValueAMD64_OpAtomicCompareAndSwap64(v)
	case OpAtomicExchange32:
		return rewriteValueAMD64_OpAtomicExchange32(v)
	case OpAtomicExchange64:
		return rewriteValueAMD64_OpAtomicExchange64(v)
	case OpAtomicExchange8:
		return rewriteValueAMD64_OpAtomicExchange8(v)
	case OpAtomicLoad32:
		return rewriteValueAMD64_OpAtomicLoad32(v)
	case OpAtomicLoad64:
		return rewriteValueAMD64_OpAtomicLoad64(v)
	case OpAtomicLoad8:
		return rewriteValueAMD64_OpAtomicLoad8(v)
	case OpAtomicLoadPtr:
		return rewriteValueAMD64_OpAtomicLoadPtr(v)
	case OpAtomicOr32:
		return rewriteValueAMD64_OpAtomicOr32(v)
	case OpAtomicOr32value:
		return rewriteValueAMD64_OpAtomicOr32value(v)
	case OpAtomicOr64value:
		return rewriteValueAMD64_OpAtomicOr64value(v)
	case OpAtomicOr8:
		return rewriteValueAMD64_OpAtomicOr8(v)
	case OpAtomicStore32:
		return rewriteValueAMD64_OpAtomicStore32(v)
	case OpAtomicStore64:
		return rewriteValueAMD64_OpAtomicStore64(v)
	case OpAtomicStore8:
		return rewriteValueAMD64_OpAtomicStore8(v)
	case OpAtomicStorePtrNoWB:
		return rewriteValueAMD64_OpAtomicStorePtrNoWB(v)
	case OpAverageUint16x16:
		v.Op = OpAMD64VPAVGW256
		return true
	case OpAverageUint16x32:
		v.Op = OpAMD64VPAVGW512
		return true
	case OpAverageUint16x8:
		v.Op = OpAMD64VPAVGW128
		return true
	case OpAverageUint8x16:
		v.Op = OpAMD64VPAVGB128
		return true
	case OpAverageUint8x32:
		v.Op = OpAMD64VPAVGB256
		return true
	case OpAverageUint8x64:
		v.Op = OpAMD64VPAVGB512
		return true
	case OpAvg64u:
		v.Op = OpAMD64AVGQU
		return true
	case OpBitLen16:
		return rewriteValueAMD64_OpBitLen16(v)
	case OpBitLen32:
		return rewriteValueAMD64_OpBitLen32(v)
	case OpBitLen64:
		return rewriteValueAMD64_OpBitLen64(v)
	case OpBitLen8:
		return rewriteValueAMD64_OpBitLen8(v)
	case OpBswap16:
		return rewriteValueAMD64_OpBswap16(v)
	case OpBswap32:
		v.Op = OpAMD64BSWAPL
		return true
	case OpBswap64:
		v.Op = OpAMD64BSWAPQ
		return true
	case OpCeil:
		return rewriteValueAMD64_OpCeil(v)
	case OpCeilFloat32x4:
		return rewriteValueAMD64_OpCeilFloat32x4(v)
	case OpCeilFloat32x8:
		return rewriteValueAMD64_OpCeilFloat32x8(v)
	case OpCeilFloat64x2:
		return rewriteValueAMD64_OpCeilFloat64x2(v)
	case OpCeilFloat64x4:
		return rewriteValueAMD64_OpCeilFloat64x4(v)
	case OpCeilSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat32x16(v)
	case OpCeilSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat32x4(v)
	case OpCeilSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat32x8(v)
	case OpCeilSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat64x2(v)
	case OpCeilSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat64x4(v)
	case OpCeilSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat64x8(v)
	case OpCeilWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpCeilWithPrecisionFloat32x16(v)
	case OpCeilWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpCeilWithPrecisionFloat32x4(v)
	case OpCeilWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpCeilWithPrecisionFloat32x8(v)
	case OpCeilWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpCeilWithPrecisionFloat64x2(v)
	case OpCeilWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpCeilWithPrecisionFloat64x4(v)
	case OpCeilWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpCeilWithPrecisionFloat64x8(v)
	case OpClosureCall:
		v.Op = OpAMD64CALLclosure
		return true
	case OpCom16:
		v.Op = OpAMD64NOTL
		return true
	case OpCom32:
		v.Op = OpAMD64NOTL
		return true
	case OpCom64:
		v.Op = OpAMD64NOTQ
		return true
	case OpCom8:
		v.Op = OpAMD64NOTL
		return true
	case OpCondSelect:
		return rewriteValueAMD64_OpCondSelect(v)
	case OpConst16:
		return rewriteValueAMD64_OpConst16(v)
	case OpConst32:
		v.Op = OpAMD64MOVLconst
		return true
	case OpConst32F:
		v.Op = OpAMD64MOVSSconst
		return true
	case OpConst64:
		v.Op = OpAMD64MOVQconst
		return true
	case OpConst64F:
		v.Op = OpAMD64MOVSDconst
		return true
	case OpConst8:
		return rewriteValueAMD64_OpConst8(v)
	case OpConstBool:
		return rewriteValueAMD64_OpConstBool(v)
	case OpConstNil:
		return rewriteValueAMD64_OpConstNil(v)
	case OpCtz16:
		return rewriteValueAMD64_OpCtz16(v)
	case OpCtz16NonZero:
		return rewriteValueAMD64_OpCtz16NonZero(v)
	case OpCtz32:
		return rewriteValueAMD64_OpCtz32(v)
	case OpCtz32NonZero:
		return rewriteValueAMD64_OpCtz32NonZero(v)
	case OpCtz64:
		return rewriteValueAMD64_OpCtz64(v)
	case OpCtz64NonZero:
		return rewriteValueAMD64_OpCtz64NonZero(v)
	case OpCtz8:
		return rewriteValueAMD64_OpCtz8(v)
	case OpCtz8NonZero:
		return rewriteValueAMD64_OpCtz8NonZero(v)
	case OpCvt32Fto32:
		v.Op = OpAMD64CVTTSS2SL
		return true
	case OpCvt32Fto64:
		v.Op = OpAMD64CVTTSS2SQ
		return true
	case OpCvt32Fto64F:
		v.Op = OpAMD64CVTSS2SD
		return true
	case OpCvt32to32F:
		v.Op = OpAMD64CVTSL2SS
		return true
	case OpCvt32to64F:
		v.Op = OpAMD64CVTSL2SD
		return true
	case OpCvt64Fto32:
		v.Op = OpAMD64CVTTSD2SL
		return true
	case OpCvt64Fto32F:
		v.Op = OpAMD64CVTSD2SS
		return true
	case OpCvt64Fto64:
		v.Op = OpAMD64CVTTSD2SQ
		return true
	case OpCvt64to32F:
		v.Op = OpAMD64CVTSQ2SS
		return true
	case OpCvt64to64F:
		v.Op = OpAMD64CVTSQ2SD
		return true
	case OpCvtBoolToUint8:
		v.Op = OpCopy
		return true
	case OpDiffWithCeilSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat32x16(v)
	case OpDiffWithCeilSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat32x4(v)
	case OpDiffWithCeilSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat32x8(v)
	case OpDiffWithCeilSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat64x2(v)
	case OpDiffWithCeilSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat64x4(v)
	case OpDiffWithCeilSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat64x8(v)
	case OpDiffWithCeilWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat32x16(v)
	case OpDiffWithCeilWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat32x4(v)
	case OpDiffWithCeilWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat32x8(v)
	case OpDiffWithCeilWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat64x2(v)
	case OpDiffWithCeilWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat64x4(v)
	case OpDiffWithCeilWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat64x8(v)
	case OpDiffWithFloorSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat32x16(v)
	case OpDiffWithFloorSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat32x4(v)
	case OpDiffWithFloorSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat32x8(v)
	case OpDiffWithFloorSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat64x2(v)
	case OpDiffWithFloorSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat64x4(v)
	case OpDiffWithFloorSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat64x8(v)
	case OpDiffWithFloorWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat32x16(v)
	case OpDiffWithFloorWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat32x4(v)
	case OpDiffWithFloorWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat32x8(v)
	case OpDiffWithFloorWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat64x2(v)
	case OpDiffWithFloorWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat64x4(v)
	case OpDiffWithFloorWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat64x8(v)
	case OpDiffWithRoundSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat32x16(v)
	case OpDiffWithRoundSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat32x4(v)
	case OpDiffWithRoundSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat32x8(v)
	case OpDiffWithRoundSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat64x2(v)
	case OpDiffWithRoundSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat64x4(v)
	case OpDiffWithRoundSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat64x8(v)
	case OpDiffWithRoundWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat32x16(v)
	case OpDiffWithRoundWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat32x4(v)
	case OpDiffWithRoundWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat32x8(v)
	case OpDiffWithRoundWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat64x2(v)
	case OpDiffWithRoundWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat64x4(v)
	case OpDiffWithRoundWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat64x8(v)
	case OpDiffWithTruncSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat32x16(v)
	case OpDiffWithTruncSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat32x4(v)
	case OpDiffWithTruncSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat32x8(v)
	case OpDiffWithTruncSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat64x2(v)
	case OpDiffWithTruncSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat64x4(v)
	case OpDiffWithTruncSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat64x8(v)
	case OpDiffWithTruncWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat32x16(v)
	case OpDiffWithTruncWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat32x4(v)
	case OpDiffWithTruncWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat32x8(v)
	case OpDiffWithTruncWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat64x2(v)
	case OpDiffWithTruncWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat64x4(v)
	case OpDiffWithTruncWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat64x8(v)
	case OpDiv128u:
		v.Op = OpAMD64DIVQU2
		return true
	case OpDiv16:
		return rewriteValueAMD64_OpDiv16(v)
	case OpDiv16u:
		return rewriteValueAMD64_OpDiv16u(v)
	case OpDiv32:
		return rewriteValueAMD64_OpDiv32(v)
	case OpDiv32F:
		v.Op = OpAMD64DIVSS
		return true
	case OpDiv32u:
		return rewriteValueAMD64_OpDiv32u(v)
	case OpDiv64:
		return rewriteValueAMD64_OpDiv64(v)
	case OpDiv64F:
		v.Op = OpAMD64DIVSD
		return true
	case OpDiv64u:
		return rewriteValueAMD64_OpDiv64u(v)
	case OpDiv8:
		return rewriteValueAMD64_OpDiv8(v)
	case OpDiv8u:
		return rewriteValueAMD64_OpDiv8u(v)
	case OpDivFloat32x16:
		v.Op = OpAMD64VDIVPS512
		return true
	case OpDivFloat32x4:
		v.Op = OpAMD64VDIVPS128
		return true
	case OpDivFloat32x8:
		v.Op = OpAMD64VDIVPS256
		return true
	case OpDivFloat64x2:
		v.Op = OpAMD64VDIVPD128
		return true
	case OpDivFloat64x4:
		v.Op = OpAMD64VDIVPD256
		return true
	case OpDivFloat64x8:
		v.Op = OpAMD64VDIVPD512
		return true
	case OpDotProdBroadcastFloat64x2:
		return rewriteValueAMD64_OpDotProdBroadcastFloat64x2(v)
	case OpEq16:
		return rewriteValueAMD64_OpEq16(v)
	case OpEq32:
		return rewriteValueAMD64_OpEq32(v)
	case OpEq32F:
		return rewriteValueAMD64_OpEq32F(v)
	case OpEq64:
		return rewriteValueAMD64_OpEq64(v)
	case OpEq64F:
		return rewriteValueAMD64_OpEq64F(v)
	case OpEq8:
		return rewriteValueAMD64_OpEq8(v)
	case OpEqB:
		return rewriteValueAMD64_OpEqB(v)
	case OpEqPtr:
		return rewriteValueAMD64_OpEqPtr(v)
	case OpEqualFloat32x16:
		return rewriteValueAMD64_OpEqualFloat32x16(v)
	case OpEqualFloat32x4:
		return rewriteValueAMD64_OpEqualFloat32x4(v)
	case OpEqualFloat32x8:
		return rewriteValueAMD64_OpEqualFloat32x8(v)
	case OpEqualFloat64x2:
		return rewriteValueAMD64_OpEqualFloat64x2(v)
	case OpEqualFloat64x4:
		return rewriteValueAMD64_OpEqualFloat64x4(v)
	case OpEqualFloat64x8:
		return rewriteValueAMD64_OpEqualFloat64x8(v)
	case OpEqualInt16x16:
		v.Op = OpAMD64VPCMPEQW256
		return true
	case OpEqualInt16x32:
		return rewriteValueAMD64_OpEqualInt16x32(v)
	case OpEqualInt16x8:
		v.Op = OpAMD64VPCMPEQW128
		return true
	case OpEqualInt32x16:
		return rewriteValueAMD64_OpEqualInt32x16(v)
	case OpEqualInt32x4:
		v.Op = OpAMD64VPCMPEQD128
		return true
	case OpEqualInt32x8:
		v.Op = OpAMD64VPCMPEQD256
		return true
	case OpEqualInt64x2:
		v.Op = OpAMD64VPCMPEQQ128
		return true
	case OpEqualInt64x4:
		v.Op = OpAMD64VPCMPEQQ256
		return true
	case OpEqualInt64x8:
		return rewriteValueAMD64_OpEqualInt64x8(v)
	case OpEqualInt8x16:
		v.Op = OpAMD64VPCMPEQB128
		return true
	case OpEqualInt8x32:
		v.Op = OpAMD64VPCMPEQB256
		return true
	case OpEqualInt8x64:
		return rewriteValueAMD64_OpEqualInt8x64(v)
	case OpEqualUint16x16:
		return rewriteValueAMD64_OpEqualUint16x16(v)
	case OpEqualUint16x32:
		return rewriteValueAMD64_OpEqualUint16x32(v)
	case OpEqualUint16x8:
		return rewriteValueAMD64_OpEqualUint16x8(v)
	case OpEqualUint32x16:
		return rewriteValueAMD64_OpEqualUint32x16(v)
	case OpEqualUint32x4:
		return rewriteValueAMD64_OpEqualUint32x4(v)
	case OpEqualUint32x8:
		return rewriteValueAMD64_OpEqualUint32x8(v)
	case OpEqualUint64x2:
		return rewriteValueAMD64_OpEqualUint64x2(v)
	case OpEqualUint64x4:
		return rewriteValueAMD64_OpEqualUint64x4(v)
	case OpEqualUint64x8:
		return rewriteValueAMD64_OpEqualUint64x8(v)
	case OpEqualUint8x16:
		return rewriteValueAMD64_OpEqualUint8x16(v)
	case OpEqualUint8x32:
		return rewriteValueAMD64_OpEqualUint8x32(v)
	case OpEqualUint8x64:
		return rewriteValueAMD64_OpEqualUint8x64(v)
	case OpFMA:
		return rewriteValueAMD64_OpFMA(v)
	case OpFloor:
		return rewriteValueAMD64_OpFloor(v)
	case OpFloorFloat32x4:
		return rewriteValueAMD64_OpFloorFloat32x4(v)
	case OpFloorFloat32x8:
		return rewriteValueAMD64_OpFloorFloat32x8(v)
	case OpFloorFloat64x2:
		return rewriteValueAMD64_OpFloorFloat64x2(v)
	case OpFloorFloat64x4:
		return rewriteValueAMD64_OpFloorFloat64x4(v)
	case OpFloorSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat32x16(v)
	case OpFloorSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat32x4(v)
	case OpFloorSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat32x8(v)
	case OpFloorSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat64x2(v)
	case OpFloorSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat64x4(v)
	case OpFloorSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat64x8(v)
	case OpFloorWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpFloorWithPrecisionFloat32x16(v)
	case OpFloorWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpFloorWithPrecisionFloat32x4(v)
	case OpFloorWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpFloorWithPrecisionFloat32x8(v)
	case OpFloorWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpFloorWithPrecisionFloat64x2(v)
	case OpFloorWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpFloorWithPrecisionFloat64x4(v)
	case OpFloorWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpFloorWithPrecisionFloat64x8(v)
	case OpFusedMultiplyAddFloat32x16:
		v.Op = OpAMD64VFMADD213PS512
		return true
	case OpFusedMultiplyAddFloat32x4:
		v.Op = OpAMD64VFMADD213PS128
		return true
	case OpFusedMultiplyAddFloat32x8:
		v.Op = OpAMD64VFMADD213PS256
		return true
	case OpFusedMultiplyAddFloat64x2:
		v.Op = OpAMD64VFMADD213PD128
		return true
	case OpFusedMultiplyAddFloat64x4:
		v.Op = OpAMD64VFMADD213PD256
		return true
	case OpFusedMultiplyAddFloat64x8:
		v.Op = OpAMD64VFMADD213PD512
		return true
	case OpFusedMultiplyAddSubFloat32x16:
		v.Op = OpAMD64VFMADDSUB213PS512
		return true
	case OpFusedMultiplyAddSubFloat32x4:
		v.Op = OpAMD64VFMADDSUB213PS128
		return true
	case OpFusedMultiplyAddSubFloat32x8:
		v.Op = OpAMD64VFMADDSUB213PS256
		return true
	case OpFusedMultiplyAddSubFloat64x2:
		v.Op = OpAMD64VFMADDSUB213PD128
		return true
	case OpFusedMultiplyAddSubFloat64x4:
		v.Op = OpAMD64VFMADDSUB213PD256
		return true
	case OpFusedMultiplyAddSubFloat64x8:
		v.Op = OpAMD64VFMADDSUB213PD512
		return true
	case OpFusedMultiplySubAddFloat32x16:
		v.Op = OpAMD64VFMSUBADD213PS512
		return true
	case OpFusedMultiplySubAddFloat32x4:
		v.Op = OpAMD64VFMSUBADD213PS128
		return true
	case OpFusedMultiplySubAddFloat32x8:
		v.Op = OpAMD64VFMSUBADD213PS256
		return true
	case OpFusedMultiplySubAddFloat64x2:
		v.Op = OpAMD64VFMSUBADD213PD128
		return true
	case OpFusedMultiplySubAddFloat64x4:
		v.Op = OpAMD64VFMSUBADD213PD256
		return true
	case OpFusedMultiplySubAddFloat64x8:
		v.Op = OpAMD64VFMSUBADD213PD512
		return true
	case OpGetCallerPC:
		v.Op = OpAMD64LoweredGetCallerPC
		return true
	case OpGetCallerSP:
		v.Op = OpAMD64LoweredGetCallerSP
		return true
	case OpGetClosurePtr:
		v.Op = OpAMD64LoweredGetClosurePtr
		return true
	case OpGetElemInt16x8:
		return rewriteValueAMD64_OpGetElemInt16x8(v)
	case OpGetElemInt32x4:
		return rewriteValueAMD64_OpGetElemInt32x4(v)
	case OpGetElemInt64x2:
		return rewriteValueAMD64_OpGetElemInt64x2(v)
	case OpGetElemInt8x16:
		return rewriteValueAMD64_OpGetElemInt8x16(v)
	case OpGetElemUint16x8:
		return rewriteValueAMD64_OpGetElemUint16x8(v)
	case OpGetElemUint32x4:
		return rewriteValueAMD64_OpGetElemUint32x4(v)
	case OpGetElemUint64x2:
		return rewriteValueAMD64_OpGetElemUint64x2(v)
	case OpGetElemUint8x16:
		return rewriteValueAMD64_OpGetElemUint8x16(v)
	case OpGetG:
		return rewriteValueAMD64_OpGetG(v)
	case OpGreaterEqualFloat32x16:
		return rewriteValueAMD64_OpGreaterEqualFloat32x16(v)
	case OpGreaterEqualFloat32x4:
		return rewriteValueAMD64_OpGreaterEqualFloat32x4(v)
	case OpGreaterEqualFloat32x8:
		return rewriteValueAMD64_OpGreaterEqualFloat32x8(v)
	case OpGreaterEqualFloat64x2:
		return rewriteValueAMD64_OpGreaterEqualFloat64x2(v)
	case OpGreaterEqualFloat64x4:
		return rewriteValueAMD64_OpGreaterEqualFloat64x4(v)
	case OpGreaterEqualFloat64x8:
		return rewriteValueAMD64_OpGreaterEqualFloat64x8(v)
	case OpGreaterEqualInt16x16:
		return rewriteValueAMD64_OpGreaterEqualInt16x16(v)
	case OpGreaterEqualInt16x32:
		return rewriteValueAMD64_OpGreaterEqualInt16x32(v)
	case OpGreaterEqualInt16x8:
		return rewriteValueAMD64_OpGreaterEqualInt16x8(v)
	case OpGreaterEqualInt32x16:
		return rewriteValueAMD64_OpGreaterEqualInt32x16(v)
	case OpGreaterEqualInt32x4:
		return rewriteValueAMD64_OpGreaterEqualInt32x4(v)
	case OpGreaterEqualInt32x8:
		return rewriteValueAMD64_OpGreaterEqualInt32x8(v)
	case OpGreaterEqualInt64x2:
		return rewriteValueAMD64_OpGreaterEqualInt64x2(v)
	case OpGreaterEqualInt64x4:
		return rewriteValueAMD64_OpGreaterEqualInt64x4(v)
	case OpGreaterEqualInt64x8:
		return rewriteValueAMD64_OpGreaterEqualInt64x8(v)
	case OpGreaterEqualInt8x16:
		return rewriteValueAMD64_OpGreaterEqualInt8x16(v)
	case OpGreaterEqualInt8x32:
		return rewriteValueAMD64_OpGreaterEqualInt8x32(v)
	case OpGreaterEqualInt8x64:
		return rewriteValueAMD64_OpGreaterEqualInt8x64(v)
	case OpGreaterEqualUint16x16:
		return rewriteValueAMD64_OpGreaterEqualUint16x16(v)
	case OpGreaterEqualUint16x32:
		return rewriteValueAMD64_OpGreaterEqualUint16x32(v)
	case OpGreaterEqualUint16x8:
		return rewriteValueAMD64_OpGreaterEqualUint16x8(v)
	case OpGreaterEqualUint32x16:
		return rewriteValueAMD64_OpGreaterEqualUint32x16(v)
	case OpGreaterEqualUint32x4:
		return rewriteValueAMD64_OpGreaterEqualUint32x4(v)
	case OpGreaterEqualUint32x8:
		return rewriteValueAMD64_OpGreaterEqualUint32x8(v)
	case OpGreaterEqualUint64x2:
		return rewriteValueAMD64_OpGreaterEqualUint64x2(v)
	case OpGreaterEqualUint64x4:
		return rewriteValueAMD64_OpGreaterEqualUint64x4(v)
	case OpGreaterEqualUint64x8:
		return rewriteValueAMD64_OpGreaterEqualUint64x8(v)
	case OpGreaterEqualUint8x16:
		return rewriteValueAMD64_OpGreaterEqualUint8x16(v)
	case OpGreaterEqualUint8x32:
		return rewriteValueAMD64_OpGreaterEqualUint8x32(v)
	case OpGreaterEqualUint8x64:
		return rewriteValueAMD64_OpGreaterEqualUint8x64(v)
	case OpGreaterFloat32x16:
		return rewriteValueAMD64_OpGreaterFloat32x16(v)
	case OpGreaterFloat32x4:
		return rewriteValueAMD64_OpGreaterFloat32x4(v)
	case OpGreaterFloat32x8:
		return rewriteValueAMD64_OpGreaterFloat32x8(v)
	case OpGreaterFloat64x2:
		return rewriteValueAMD64_OpGreaterFloat64x2(v)
	case OpGreaterFloat64x4:
		return rewriteValueAMD64_OpGreaterFloat64x4(v)
	case OpGreaterFloat64x8:
		return rewriteValueAMD64_OpGreaterFloat64x8(v)
	case OpGreaterInt16x16:
		v.Op = OpAMD64VPCMPGTW256
		return true
	case OpGreaterInt16x32:
		return rewriteValueAMD64_OpGreaterInt16x32(v)
	case OpGreaterInt16x8:
		v.Op = OpAMD64VPCMPGTW128
		return true
	case OpGreaterInt32x16:
		return rewriteValueAMD64_OpGreaterInt32x16(v)
	case OpGreaterInt32x4:
		v.Op = OpAMD64VPCMPGTD128
		return true
	case OpGreaterInt32x8:
		v.Op = OpAMD64VPCMPGTD256
		return true
	case OpGreaterInt64x2:
		return rewriteValueAMD64_OpGreaterInt64x2(v)
	case OpGreaterInt64x4:
		v.Op = OpAMD64VPCMPGTQ256
		return true
	case OpGreaterInt64x8:
		return rewriteValueAMD64_OpGreaterInt64x8(v)
	case OpGreaterInt8x16:
		v.Op = OpAMD64VPCMPGTB128
		return true
	case OpGreaterInt8x32:
		v.Op = OpAMD64VPCMPGTB256
		return true
	case OpGreaterInt8x64:
		return rewriteValueAMD64_OpGreaterInt8x64(v)
	case OpGreaterUint16x16:
		return rewriteValueAMD64_OpGreaterUint16x16(v)
	case OpGreaterUint16x32:
		return rewriteValueAMD64_OpGreaterUint16x32(v)
	case OpGreaterUint16x8:
		return rewriteValueAMD64_OpGreaterUint16x8(v)
	case OpGreaterUint32x16:
		return rewriteValueAMD64_OpGreaterUint32x16(v)
	case OpGreaterUint32x4:
		return rewriteValueAMD64_OpGreaterUint32x4(v)
	case OpGreaterUint32x8:
		return rewriteValueAMD64_OpGreaterUint32x8(v)
	case OpGreaterUint64x2:
		return rewriteValueAMD64_OpGreaterUint64x2(v)
	case OpGreaterUint64x4:
		return rewriteValueAMD64_OpGreaterUint64x4(v)
	case OpGreaterUint64x8:
		return rewriteValueAMD64_OpGreaterUint64x8(v)
	case OpGreaterUint8x16:
		return rewriteValueAMD64_OpGreaterUint8x16(v)
	case OpGreaterUint8x32:
		return rewriteValueAMD64_OpGreaterUint8x32(v)
	case OpGreaterUint8x64:
		return rewriteValueAMD64_OpGreaterUint8x64(v)
	case OpHasCPUFeature:
		return rewriteValueAMD64_OpHasCPUFeature(v)
	case OpHmul32:
		v.Op = OpAMD64HMULL
		return true
	case OpHmul32u:
		v.Op = OpAMD64HMULLU
		return true
	case OpHmul64:
		v.Op = OpAMD64HMULQ
		return true
	case OpHmul64u:
		v.Op = OpAMD64HMULQU
		return true
	case OpInterCall:
		v.Op = OpAMD64CALLinter
		return true
	case OpIsInBounds:
		return rewriteValueAMD64_OpIsInBounds(v)
	case OpIsNanFloat32x16:
		return rewriteValueAMD64_OpIsNanFloat32x16(v)
	case OpIsNanFloat32x4:
		return rewriteValueAMD64_OpIsNanFloat32x4(v)
	case OpIsNanFloat32x8:
		return rewriteValueAMD64_OpIsNanFloat32x8(v)
	case OpIsNanFloat64x2:
		return rewriteValueAMD64_OpIsNanFloat64x2(v)
	case OpIsNanFloat64x4:
		return rewriteValueAMD64_OpIsNanFloat64x4(v)
	case OpIsNanFloat64x8:
		return rewriteValueAMD64_OpIsNanFloat64x8(v)
	case OpIsNonNil:
		return rewriteValueAMD64_OpIsNonNil(v)
	case OpIsSliceInBounds:
		return rewriteValueAMD64_OpIsSliceInBounds(v)
	case OpLeq16:
		return rewriteValueAMD64_OpLeq16(v)
	case OpLeq16U:
		return rewriteValueAMD64_OpLeq16U(v)
	case OpLeq32:
		return rewriteValueAMD64_OpLeq32(v)
	case OpLeq32F:
		return rewriteValueAMD64_OpLeq32F(v)
	case OpLeq32U:
		return rewriteValueAMD64_OpLeq32U(v)
	case OpLeq64:
		return rewriteValueAMD64_OpLeq64(v)
	case OpLeq64F:
		return rewriteValueAMD64_OpLeq64F(v)
	case OpLeq64U:
		return rewriteValueAMD64_OpLeq64U(v)
	case OpLeq8:
		return rewriteValueAMD64_OpLeq8(v)
	case OpLeq8U:
		return rewriteValueAMD64_OpLeq8U(v)
	case OpLess16:
		return rewriteValueAMD64_OpLess16(v)
	case OpLess16U:
		return rewriteValueAMD64_OpLess16U(v)
	case OpLess32:
		return rewriteValueAMD64_OpLess32(v)
	case OpLess32F:
		return rewriteValueAMD64_OpLess32F(v)
	case OpLess32U:
		return rewriteValueAMD64_OpLess32U(v)
	case OpLess64:
		return rewriteValueAMD64_OpLess64(v)
	case OpLess64F:
		return rewriteValueAMD64_OpLess64F(v)
	case OpLess64U:
		return rewriteValueAMD64_OpLess64U(v)
	case OpLess8:
		return rewriteValueAMD64_OpLess8(v)
	case OpLess8U:
		return rewriteValueAMD64_OpLess8U(v)
	case OpLessEqualFloat32x16:
		return rewriteValueAMD64_OpLessEqualFloat32x16(v)
	case OpLessEqualFloat32x4:
		return rewriteValueAMD64_OpLessEqualFloat32x4(v)
	case OpLessEqualFloat32x8:
		return rewriteValueAMD64_OpLessEqualFloat32x8(v)
	case OpLessEqualFloat64x2:
		return rewriteValueAMD64_OpLessEqualFloat64x2(v)
	case OpLessEqualFloat64x4:
		return rewriteValueAMD64_OpLessEqualFloat64x4(v)
	case OpLessEqualFloat64x8:
		return rewriteValueAMD64_OpLessEqualFloat64x8(v)
	case OpLessEqualInt16x16:
		return rewriteValueAMD64_OpLessEqualInt16x16(v)
	case OpLessEqualInt16x32:
		return rewriteValueAMD64_OpLessEqualInt16x32(v)
	case OpLessEqualInt16x8:
		return rewriteValueAMD64_OpLessEqualInt16x8(v)
	case OpLessEqualInt32x16:
		return rewriteValueAMD64_OpLessEqualInt32x16(v)
	case OpLessEqualInt32x4:
		return rewriteValueAMD64_OpLessEqualInt32x4(v)
	case OpLessEqualInt32x8:
		return rewriteValueAMD64_OpLessEqualInt32x8(v)
	case OpLessEqualInt64x2:
		return rewriteValueAMD64_OpLessEqualInt64x2(v)
	case OpLessEqualInt64x4:
		return rewriteValueAMD64_OpLessEqualInt64x4(v)
	case OpLessEqualInt64x8:
		return rewriteValueAMD64_OpLessEqualInt64x8(v)
	case OpLessEqualInt8x16:
		return rewriteValueAMD64_OpLessEqualInt8x16(v)
	case OpLessEqualInt8x32:
		return rewriteValueAMD64_OpLessEqualInt8x32(v)
	case OpLessEqualInt8x64:
		return rewriteValueAMD64_OpLessEqualInt8x64(v)
	case OpLessEqualUint16x16:
		return rewriteValueAMD64_OpLessEqualUint16x16(v)
	case OpLessEqualUint16x32:
		return rewriteValueAMD64_OpLessEqualUint16x32(v)
	case OpLessEqualUint16x8:
		return rewriteValueAMD64_OpLessEqualUint16x8(v)
	case OpLessEqualUint32x16:
		return rewriteValueAMD64_OpLessEqualUint32x16(v)
	case OpLessEqualUint32x4:
		return rewriteValueAMD64_OpLessEqualUint32x4(v)
	case OpLessEqualUint32x8:
		return rewriteValueAMD64_OpLessEqualUint32x8(v)
	case OpLessEqualUint64x2:
		return rewriteValueAMD64_OpLessEqualUint64x2(v)
	case OpLessEqualUint64x4:
		return rewriteValueAMD64_OpLessEqualUint64x4(v)
	case OpLessEqualUint64x8:
		return rewriteValueAMD64_OpLessEqualUint64x8(v)
	case OpLessEqualUint8x16:
		return rewriteValueAMD64_OpLessEqualUint8x16(v)
	case OpLessEqualUint8x32:
		return rewriteValueAMD64_OpLessEqualUint8x32(v)
	case OpLessEqualUint8x64:
		return rewriteValueAMD64_OpLessEqualUint8x64(v)
	case OpLessFloat32x16:
		return rewriteValueAMD64_OpLessFloat32x16(v)
	case OpLessFloat32x4:
		return rewriteValueAMD64_OpLessFloat32x4(v)
	case OpLessFloat32x8:
		return rewriteValueAMD64_OpLessFloat32x8(v)
	case OpLessFloat64x2:
		return rewriteValueAMD64_OpLessFloat64x2(v)
	case OpLessFloat64x4:
		return rewriteValueAMD64_OpLessFloat64x4(v)
	case OpLessFloat64x8:
		return rewriteValueAMD64_OpLessFloat64x8(v)
	case OpLessInt16x16:
		return rewriteValueAMD64_OpLessInt16x16(v)
	case OpLessInt16x32:
		return rewriteValueAMD64_OpLessInt16x32(v)
	case OpLessInt16x8:
		return rewriteValueAMD64_OpLessInt16x8(v)
	case OpLessInt32x16:
		return rewriteValueAMD64_OpLessInt32x16(v)
	case OpLessInt32x4:
		return rewriteValueAMD64_OpLessInt32x4(v)
	case OpLessInt32x8:
		return rewriteValueAMD64_OpLessInt32x8(v)
	case OpLessInt64x2:
		return rewriteValueAMD64_OpLessInt64x2(v)
	case OpLessInt64x4:
		return rewriteValueAMD64_OpLessInt64x4(v)
	case OpLessInt64x8:
		return rewriteValueAMD64_OpLessInt64x8(v)
	case OpLessInt8x16:
		return rewriteValueAMD64_OpLessInt8x16(v)
	case OpLessInt8x32:
		return rewriteValueAMD64_OpLessInt8x32(v)
	case OpLessInt8x64:
		return rewriteValueAMD64_OpLessInt8x64(v)
	case OpLessUint16x16:
		return rewriteValueAMD64_OpLessUint16x16(v)
	case OpLessUint16x32:
		return rewriteValueAMD64_OpLessUint16x32(v)
	case OpLessUint16x8:
		return rewriteValueAMD64_OpLessUint16x8(v)
	case OpLessUint32x16:
		return rewriteValueAMD64_OpLessUint32x16(v)
	case OpLessUint32x4:
		return rewriteValueAMD64_OpLessUint32x4(v)
	case OpLessUint32x8:
		return rewriteValueAMD64_OpLessUint32x8(v)
	case OpLessUint64x2:
		return rewriteValueAMD64_OpLessUint64x2(v)
	case OpLessUint64x4:
		return rewriteValueAMD64_OpLessUint64x4(v)
	case OpLessUint64x8:
		return rewriteValueAMD64_OpLessUint64x8(v)
	case OpLessUint8x16:
		return rewriteValueAMD64_OpLessUint8x16(v)
	case OpLessUint8x32:
		return rewriteValueAMD64_OpLessUint8x32(v)
	case OpLessUint8x64:
		return rewriteValueAMD64_OpLessUint8x64(v)
	case OpLoad:
		return rewriteValueAMD64_OpLoad(v)
	case OpLocalAddr:
		return rewriteValueAMD64_OpLocalAddr(v)
	case OpLsh16x16:
		return rewriteValueAMD64_OpLsh16x16(v)
	case OpLsh16x32:
		return rewriteValueAMD64_OpLsh16x32(v)
	case OpLsh16x64:
		return rewriteValueAMD64_OpLsh16x64(v)
	case OpLsh16x8:
		return rewriteValueAMD64_OpLsh16x8(v)
	case OpLsh32x16:
		return rewriteValueAMD64_OpLsh32x16(v)
	case OpLsh32x32:
		return rewriteValueAMD64_OpLsh32x32(v)
	case OpLsh32x64:
		return rewriteValueAMD64_OpLsh32x64(v)
	case OpLsh32x8:
		return rewriteValueAMD64_OpLsh32x8(v)
	case OpLsh64x16:
		return rewriteValueAMD64_OpLsh64x16(v)
	case OpLsh64x32:
		return rewriteValueAMD64_OpLsh64x32(v)
	case OpLsh64x64:
		return rewriteValueAMD64_OpLsh64x64(v)
	case OpLsh64x8:
		return rewriteValueAMD64_OpLsh64x8(v)
	case OpLsh8x16:
		return rewriteValueAMD64_OpLsh8x16(v)
	case OpLsh8x32:
		return rewriteValueAMD64_OpLsh8x32(v)
	case OpLsh8x64:
		return rewriteValueAMD64_OpLsh8x64(v)
	case OpLsh8x8:
		return rewriteValueAMD64_OpLsh8x8(v)
	case OpMaskedAbsoluteInt16x16:
		return rewriteValueAMD64_OpMaskedAbsoluteInt16x16(v)
	case OpMaskedAbsoluteInt16x32:
		return rewriteValueAMD64_OpMaskedAbsoluteInt16x32(v)
	case OpMaskedAbsoluteInt16x8:
		return rewriteValueAMD64_OpMaskedAbsoluteInt16x8(v)
	case OpMaskedAbsoluteInt32x16:
		return rewriteValueAMD64_OpMaskedAbsoluteInt32x16(v)
	case OpMaskedAbsoluteInt32x4:
		return rewriteValueAMD64_OpMaskedAbsoluteInt32x4(v)
	case OpMaskedAbsoluteInt32x8:
		return rewriteValueAMD64_OpMaskedAbsoluteInt32x8(v)
	case OpMaskedAbsoluteInt64x2:
		return rewriteValueAMD64_OpMaskedAbsoluteInt64x2(v)
	case OpMaskedAbsoluteInt64x4:
		return rewriteValueAMD64_OpMaskedAbsoluteInt64x4(v)
	case OpMaskedAbsoluteInt64x8:
		return rewriteValueAMD64_OpMaskedAbsoluteInt64x8(v)
	case OpMaskedAbsoluteInt8x16:
		return rewriteValueAMD64_OpMaskedAbsoluteInt8x16(v)
	case OpMaskedAbsoluteInt8x32:
		return rewriteValueAMD64_OpMaskedAbsoluteInt8x32(v)
	case OpMaskedAbsoluteInt8x64:
		return rewriteValueAMD64_OpMaskedAbsoluteInt8x64(v)
	case OpMaskedAddFloat32x16:
		return rewriteValueAMD64_OpMaskedAddFloat32x16(v)
	case OpMaskedAddFloat32x4:
		return rewriteValueAMD64_OpMaskedAddFloat32x4(v)
	case OpMaskedAddFloat32x8:
		return rewriteValueAMD64_OpMaskedAddFloat32x8(v)
	case OpMaskedAddFloat64x2:
		return rewriteValueAMD64_OpMaskedAddFloat64x2(v)
	case OpMaskedAddFloat64x4:
		return rewriteValueAMD64_OpMaskedAddFloat64x4(v)
	case OpMaskedAddFloat64x8:
		return rewriteValueAMD64_OpMaskedAddFloat64x8(v)
	case OpMaskedAddInt16x16:
		return rewriteValueAMD64_OpMaskedAddInt16x16(v)
	case OpMaskedAddInt16x32:
		return rewriteValueAMD64_OpMaskedAddInt16x32(v)
	case OpMaskedAddInt16x8:
		return rewriteValueAMD64_OpMaskedAddInt16x8(v)
	case OpMaskedAddInt32x16:
		return rewriteValueAMD64_OpMaskedAddInt32x16(v)
	case OpMaskedAddInt32x4:
		return rewriteValueAMD64_OpMaskedAddInt32x4(v)
	case OpMaskedAddInt32x8:
		return rewriteValueAMD64_OpMaskedAddInt32x8(v)
	case OpMaskedAddInt64x2:
		return rewriteValueAMD64_OpMaskedAddInt64x2(v)
	case OpMaskedAddInt64x4:
		return rewriteValueAMD64_OpMaskedAddInt64x4(v)
	case OpMaskedAddInt64x8:
		return rewriteValueAMD64_OpMaskedAddInt64x8(v)
	case OpMaskedAddInt8x16:
		return rewriteValueAMD64_OpMaskedAddInt8x16(v)
	case OpMaskedAddInt8x32:
		return rewriteValueAMD64_OpMaskedAddInt8x32(v)
	case OpMaskedAddInt8x64:
		return rewriteValueAMD64_OpMaskedAddInt8x64(v)
	case OpMaskedAddUint16x16:
		return rewriteValueAMD64_OpMaskedAddUint16x16(v)
	case OpMaskedAddUint16x32:
		return rewriteValueAMD64_OpMaskedAddUint16x32(v)
	case OpMaskedAddUint16x8:
		return rewriteValueAMD64_OpMaskedAddUint16x8(v)
	case OpMaskedAddUint32x16:
		return rewriteValueAMD64_OpMaskedAddUint32x16(v)
	case OpMaskedAddUint32x4:
		return rewriteValueAMD64_OpMaskedAddUint32x4(v)
	case OpMaskedAddUint32x8:
		return rewriteValueAMD64_OpMaskedAddUint32x8(v)
	case OpMaskedAddUint64x2:
		return rewriteValueAMD64_OpMaskedAddUint64x2(v)
	case OpMaskedAddUint64x4:
		return rewriteValueAMD64_OpMaskedAddUint64x4(v)
	case OpMaskedAddUint64x8:
		return rewriteValueAMD64_OpMaskedAddUint64x8(v)
	case OpMaskedAddUint8x16:
		return rewriteValueAMD64_OpMaskedAddUint8x16(v)
	case OpMaskedAddUint8x32:
		return rewriteValueAMD64_OpMaskedAddUint8x32(v)
	case OpMaskedAddUint8x64:
		return rewriteValueAMD64_OpMaskedAddUint8x64(v)
	case OpMaskedAndFloat32x16:
		return rewriteValueAMD64_OpMaskedAndFloat32x16(v)
	case OpMaskedAndFloat32x4:
		return rewriteValueAMD64_OpMaskedAndFloat32x4(v)
	case OpMaskedAndFloat32x8:
		return rewriteValueAMD64_OpMaskedAndFloat32x8(v)
	case OpMaskedAndFloat64x2:
		return rewriteValueAMD64_OpMaskedAndFloat64x2(v)
	case OpMaskedAndFloat64x4:
		return rewriteValueAMD64_OpMaskedAndFloat64x4(v)
	case OpMaskedAndFloat64x8:
		return rewriteValueAMD64_OpMaskedAndFloat64x8(v)
	case OpMaskedAndInt32x16:
		return rewriteValueAMD64_OpMaskedAndInt32x16(v)
	case OpMaskedAndInt32x4:
		return rewriteValueAMD64_OpMaskedAndInt32x4(v)
	case OpMaskedAndInt32x8:
		return rewriteValueAMD64_OpMaskedAndInt32x8(v)
	case OpMaskedAndInt64x2:
		return rewriteValueAMD64_OpMaskedAndInt64x2(v)
	case OpMaskedAndInt64x4:
		return rewriteValueAMD64_OpMaskedAndInt64x4(v)
	case OpMaskedAndInt64x8:
		return rewriteValueAMD64_OpMaskedAndInt64x8(v)
	case OpMaskedAndNotFloat32x16:
		return rewriteValueAMD64_OpMaskedAndNotFloat32x16(v)
	case OpMaskedAndNotFloat32x4:
		return rewriteValueAMD64_OpMaskedAndNotFloat32x4(v)
	case OpMaskedAndNotFloat32x8:
		return rewriteValueAMD64_OpMaskedAndNotFloat32x8(v)
	case OpMaskedAndNotFloat64x2:
		return rewriteValueAMD64_OpMaskedAndNotFloat64x2(v)
	case OpMaskedAndNotFloat64x4:
		return rewriteValueAMD64_OpMaskedAndNotFloat64x4(v)
	case OpMaskedAndNotFloat64x8:
		return rewriteValueAMD64_OpMaskedAndNotFloat64x8(v)
	case OpMaskedAndNotInt32x16:
		return rewriteValueAMD64_OpMaskedAndNotInt32x16(v)
	case OpMaskedAndNotInt32x4:
		return rewriteValueAMD64_OpMaskedAndNotInt32x4(v)
	case OpMaskedAndNotInt32x8:
		return rewriteValueAMD64_OpMaskedAndNotInt32x8(v)
	case OpMaskedAndNotInt64x2:
		return rewriteValueAMD64_OpMaskedAndNotInt64x2(v)
	case OpMaskedAndNotInt64x4:
		return rewriteValueAMD64_OpMaskedAndNotInt64x4(v)
	case OpMaskedAndNotInt64x8:
		return rewriteValueAMD64_OpMaskedAndNotInt64x8(v)
	case OpMaskedAndNotUint32x16:
		return rewriteValueAMD64_OpMaskedAndNotUint32x16(v)
	case OpMaskedAndNotUint32x4:
		return rewriteValueAMD64_OpMaskedAndNotUint32x4(v)
	case OpMaskedAndNotUint32x8:
		return rewriteValueAMD64_OpMaskedAndNotUint32x8(v)
	case OpMaskedAndNotUint64x2:
		return rewriteValueAMD64_OpMaskedAndNotUint64x2(v)
	case OpMaskedAndNotUint64x4:
		return rewriteValueAMD64_OpMaskedAndNotUint64x4(v)
	case OpMaskedAndNotUint64x8:
		return rewriteValueAMD64_OpMaskedAndNotUint64x8(v)
	case OpMaskedAndUint32x16:
		return rewriteValueAMD64_OpMaskedAndUint32x16(v)
	case OpMaskedAndUint32x4:
		return rewriteValueAMD64_OpMaskedAndUint32x4(v)
	case OpMaskedAndUint32x8:
		return rewriteValueAMD64_OpMaskedAndUint32x8(v)
	case OpMaskedAndUint64x2:
		return rewriteValueAMD64_OpMaskedAndUint64x2(v)
	case OpMaskedAndUint64x4:
		return rewriteValueAMD64_OpMaskedAndUint64x4(v)
	case OpMaskedAndUint64x8:
		return rewriteValueAMD64_OpMaskedAndUint64x8(v)
	case OpMaskedApproximateReciprocalFloat32x16:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalFloat32x16(v)
	case OpMaskedApproximateReciprocalFloat32x4:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalFloat32x4(v)
	case OpMaskedApproximateReciprocalFloat32x8:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalFloat32x8(v)
	case OpMaskedApproximateReciprocalFloat64x2:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalFloat64x2(v)
	case OpMaskedApproximateReciprocalFloat64x4:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalFloat64x4(v)
	case OpMaskedApproximateReciprocalFloat64x8:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalFloat64x8(v)
	case OpMaskedApproximateReciprocalOfSqrtFloat32x16:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat32x16(v)
	case OpMaskedApproximateReciprocalOfSqrtFloat32x4:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat32x4(v)
	case OpMaskedApproximateReciprocalOfSqrtFloat32x8:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat32x8(v)
	case OpMaskedApproximateReciprocalOfSqrtFloat64x2:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat64x2(v)
	case OpMaskedApproximateReciprocalOfSqrtFloat64x4:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat64x4(v)
	case OpMaskedApproximateReciprocalOfSqrtFloat64x8:
		return rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat64x8(v)
	case OpMaskedAverageUint16x16:
		return rewriteValueAMD64_OpMaskedAverageUint16x16(v)
	case OpMaskedAverageUint16x32:
		return rewriteValueAMD64_OpMaskedAverageUint16x32(v)
	case OpMaskedAverageUint16x8:
		return rewriteValueAMD64_OpMaskedAverageUint16x8(v)
	case OpMaskedAverageUint8x16:
		return rewriteValueAMD64_OpMaskedAverageUint8x16(v)
	case OpMaskedAverageUint8x32:
		return rewriteValueAMD64_OpMaskedAverageUint8x32(v)
	case OpMaskedAverageUint8x64:
		return rewriteValueAMD64_OpMaskedAverageUint8x64(v)
	case OpMaskedCeilSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat32x16(v)
	case OpMaskedCeilSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat32x4(v)
	case OpMaskedCeilSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat32x8(v)
	case OpMaskedCeilSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat64x2(v)
	case OpMaskedCeilSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat64x4(v)
	case OpMaskedCeilSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat64x8(v)
	case OpMaskedCeilWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat32x16(v)
	case OpMaskedCeilWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat32x4(v)
	case OpMaskedCeilWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat32x8(v)
	case OpMaskedCeilWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat64x2(v)
	case OpMaskedCeilWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat64x4(v)
	case OpMaskedCeilWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat64x8(v)
	case OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x16(v)
	case OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x4(v)
	case OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x8(v)
	case OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x2(v)
	case OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x4(v)
	case OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x8(v)
	case OpMaskedDiffWithCeilWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat32x16(v)
	case OpMaskedDiffWithCeilWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat32x4(v)
	case OpMaskedDiffWithCeilWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat32x8(v)
	case OpMaskedDiffWithCeilWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat64x2(v)
	case OpMaskedDiffWithCeilWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat64x4(v)
	case OpMaskedDiffWithCeilWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat64x8(v)
	case OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x16(v)
	case OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x4(v)
	case OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x8(v)
	case OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x2(v)
	case OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x4(v)
	case OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x8(v)
	case OpMaskedDiffWithFloorWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat32x16(v)
	case OpMaskedDiffWithFloorWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat32x4(v)
	case OpMaskedDiffWithFloorWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat32x8(v)
	case OpMaskedDiffWithFloorWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat64x2(v)
	case OpMaskedDiffWithFloorWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat64x4(v)
	case OpMaskedDiffWithFloorWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat64x8(v)
	case OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x16(v)
	case OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x4(v)
	case OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x8(v)
	case OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x2(v)
	case OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x4(v)
	case OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x8(v)
	case OpMaskedDiffWithRoundWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat32x16(v)
	case OpMaskedDiffWithRoundWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat32x4(v)
	case OpMaskedDiffWithRoundWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat32x8(v)
	case OpMaskedDiffWithRoundWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat64x2(v)
	case OpMaskedDiffWithRoundWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat64x4(v)
	case OpMaskedDiffWithRoundWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat64x8(v)
	case OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x16(v)
	case OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x4(v)
	case OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x8(v)
	case OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x2(v)
	case OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x4(v)
	case OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x8(v)
	case OpMaskedDiffWithTruncWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat32x16(v)
	case OpMaskedDiffWithTruncWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat32x4(v)
	case OpMaskedDiffWithTruncWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat32x8(v)
	case OpMaskedDiffWithTruncWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat64x2(v)
	case OpMaskedDiffWithTruncWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat64x4(v)
	case OpMaskedDiffWithTruncWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat64x8(v)
	case OpMaskedDivFloat32x16:
		return rewriteValueAMD64_OpMaskedDivFloat32x16(v)
	case OpMaskedDivFloat32x4:
		return rewriteValueAMD64_OpMaskedDivFloat32x4(v)
	case OpMaskedDivFloat32x8:
		return rewriteValueAMD64_OpMaskedDivFloat32x8(v)
	case OpMaskedDivFloat64x2:
		return rewriteValueAMD64_OpMaskedDivFloat64x2(v)
	case OpMaskedDivFloat64x4:
		return rewriteValueAMD64_OpMaskedDivFloat64x4(v)
	case OpMaskedDivFloat64x8:
		return rewriteValueAMD64_OpMaskedDivFloat64x8(v)
	case OpMaskedEqualFloat32x16:
		return rewriteValueAMD64_OpMaskedEqualFloat32x16(v)
	case OpMaskedEqualFloat32x4:
		return rewriteValueAMD64_OpMaskedEqualFloat32x4(v)
	case OpMaskedEqualFloat32x8:
		return rewriteValueAMD64_OpMaskedEqualFloat32x8(v)
	case OpMaskedEqualFloat64x2:
		return rewriteValueAMD64_OpMaskedEqualFloat64x2(v)
	case OpMaskedEqualFloat64x4:
		return rewriteValueAMD64_OpMaskedEqualFloat64x4(v)
	case OpMaskedEqualFloat64x8:
		return rewriteValueAMD64_OpMaskedEqualFloat64x8(v)
	case OpMaskedEqualInt16x16:
		return rewriteValueAMD64_OpMaskedEqualInt16x16(v)
	case OpMaskedEqualInt16x32:
		return rewriteValueAMD64_OpMaskedEqualInt16x32(v)
	case OpMaskedEqualInt16x8:
		return rewriteValueAMD64_OpMaskedEqualInt16x8(v)
	case OpMaskedEqualInt32x16:
		return rewriteValueAMD64_OpMaskedEqualInt32x16(v)
	case OpMaskedEqualInt32x4:
		return rewriteValueAMD64_OpMaskedEqualInt32x4(v)
	case OpMaskedEqualInt32x8:
		return rewriteValueAMD64_OpMaskedEqualInt32x8(v)
	case OpMaskedEqualInt64x2:
		return rewriteValueAMD64_OpMaskedEqualInt64x2(v)
	case OpMaskedEqualInt64x4:
		return rewriteValueAMD64_OpMaskedEqualInt64x4(v)
	case OpMaskedEqualInt64x8:
		return rewriteValueAMD64_OpMaskedEqualInt64x8(v)
	case OpMaskedEqualInt8x16:
		return rewriteValueAMD64_OpMaskedEqualInt8x16(v)
	case OpMaskedEqualInt8x32:
		return rewriteValueAMD64_OpMaskedEqualInt8x32(v)
	case OpMaskedEqualInt8x64:
		return rewriteValueAMD64_OpMaskedEqualInt8x64(v)
	case OpMaskedEqualUint16x16:
		return rewriteValueAMD64_OpMaskedEqualUint16x16(v)
	case OpMaskedEqualUint16x32:
		return rewriteValueAMD64_OpMaskedEqualUint16x32(v)
	case OpMaskedEqualUint16x8:
		return rewriteValueAMD64_OpMaskedEqualUint16x8(v)
	case OpMaskedEqualUint32x16:
		return rewriteValueAMD64_OpMaskedEqualUint32x16(v)
	case OpMaskedEqualUint32x4:
		return rewriteValueAMD64_OpMaskedEqualUint32x4(v)
	case OpMaskedEqualUint32x8:
		return rewriteValueAMD64_OpMaskedEqualUint32x8(v)
	case OpMaskedEqualUint64x2:
		return rewriteValueAMD64_OpMaskedEqualUint64x2(v)
	case OpMaskedEqualUint64x4:
		return rewriteValueAMD64_OpMaskedEqualUint64x4(v)
	case OpMaskedEqualUint64x8:
		return rewriteValueAMD64_OpMaskedEqualUint64x8(v)
	case OpMaskedEqualUint8x16:
		return rewriteValueAMD64_OpMaskedEqualUint8x16(v)
	case OpMaskedEqualUint8x32:
		return rewriteValueAMD64_OpMaskedEqualUint8x32(v)
	case OpMaskedEqualUint8x64:
		return rewriteValueAMD64_OpMaskedEqualUint8x64(v)
	case OpMaskedFloorSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat32x16(v)
	case OpMaskedFloorSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat32x4(v)
	case OpMaskedFloorSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat32x8(v)
	case OpMaskedFloorSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat64x2(v)
	case OpMaskedFloorSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat64x4(v)
	case OpMaskedFloorSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat64x8(v)
	case OpMaskedFloorWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat32x16(v)
	case OpMaskedFloorWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat32x4(v)
	case OpMaskedFloorWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat32x8(v)
	case OpMaskedFloorWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat64x2(v)
	case OpMaskedFloorWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat64x4(v)
	case OpMaskedFloorWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat64x8(v)
	case OpMaskedFusedMultiplyAddFloat32x16:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat32x16(v)
	case OpMaskedFusedMultiplyAddFloat32x4:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat32x4(v)
	case OpMaskedFusedMultiplyAddFloat32x8:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat32x8(v)
	case OpMaskedFusedMultiplyAddFloat64x2:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat64x2(v)
	case OpMaskedFusedMultiplyAddFloat64x4:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat64x4(v)
	case OpMaskedFusedMultiplyAddFloat64x8:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat64x8(v)
	case OpMaskedFusedMultiplyAddSubFloat32x16:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat32x16(v)
	case OpMaskedFusedMultiplyAddSubFloat32x4:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat32x4(v)
	case OpMaskedFusedMultiplyAddSubFloat32x8:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat32x8(v)
	case OpMaskedFusedMultiplyAddSubFloat64x2:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat64x2(v)
	case OpMaskedFusedMultiplyAddSubFloat64x4:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat64x4(v)
	case OpMaskedFusedMultiplyAddSubFloat64x8:
		return rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat64x8(v)
	case OpMaskedFusedMultiplySubAddFloat32x16:
		return rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat32x16(v)
	case OpMaskedFusedMultiplySubAddFloat32x4:
		return rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat32x4(v)
	case OpMaskedFusedMultiplySubAddFloat32x8:
		return rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat32x8(v)
	case OpMaskedFusedMultiplySubAddFloat64x2:
		return rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat64x2(v)
	case OpMaskedFusedMultiplySubAddFloat64x4:
		return rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat64x4(v)
	case OpMaskedFusedMultiplySubAddFloat64x8:
		return rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat64x8(v)
	case OpMaskedGreaterEqualFloat32x16:
		return rewriteValueAMD64_OpMaskedGreaterEqualFloat32x16(v)
	case OpMaskedGreaterEqualFloat32x4:
		return rewriteValueAMD64_OpMaskedGreaterEqualFloat32x4(v)
	case OpMaskedGreaterEqualFloat32x8:
		return rewriteValueAMD64_OpMaskedGreaterEqualFloat32x8(v)
	case OpMaskedGreaterEqualFloat64x2:
		return rewriteValueAMD64_OpMaskedGreaterEqualFloat64x2(v)
	case OpMaskedGreaterEqualFloat64x4:
		return rewriteValueAMD64_OpMaskedGreaterEqualFloat64x4(v)
	case OpMaskedGreaterEqualFloat64x8:
		return rewriteValueAMD64_OpMaskedGreaterEqualFloat64x8(v)
	case OpMaskedGreaterEqualInt16x16:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt16x16(v)
	case OpMaskedGreaterEqualInt16x32:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt16x32(v)
	case OpMaskedGreaterEqualInt16x8:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt16x8(v)
	case OpMaskedGreaterEqualInt32x16:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt32x16(v)
	case OpMaskedGreaterEqualInt32x4:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt32x4(v)
	case OpMaskedGreaterEqualInt32x8:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt32x8(v)
	case OpMaskedGreaterEqualInt64x2:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt64x2(v)
	case OpMaskedGreaterEqualInt64x4:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt64x4(v)
	case OpMaskedGreaterEqualInt64x8:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt64x8(v)
	case OpMaskedGreaterEqualInt8x16:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt8x16(v)
	case OpMaskedGreaterEqualInt8x32:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt8x32(v)
	case OpMaskedGreaterEqualInt8x64:
		return rewriteValueAMD64_OpMaskedGreaterEqualInt8x64(v)
	case OpMaskedGreaterEqualUint16x16:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint16x16(v)
	case OpMaskedGreaterEqualUint16x32:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint16x32(v)
	case OpMaskedGreaterEqualUint16x8:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint16x8(v)
	case OpMaskedGreaterEqualUint32x16:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint32x16(v)
	case OpMaskedGreaterEqualUint32x4:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint32x4(v)
	case OpMaskedGreaterEqualUint32x8:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint32x8(v)
	case OpMaskedGreaterEqualUint64x2:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint64x2(v)
	case OpMaskedGreaterEqualUint64x4:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint64x4(v)
	case OpMaskedGreaterEqualUint64x8:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint64x8(v)
	case OpMaskedGreaterEqualUint8x16:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint8x16(v)
	case OpMaskedGreaterEqualUint8x32:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint8x32(v)
	case OpMaskedGreaterEqualUint8x64:
		return rewriteValueAMD64_OpMaskedGreaterEqualUint8x64(v)
	case OpMaskedGreaterFloat32x16:
		return rewriteValueAMD64_OpMaskedGreaterFloat32x16(v)
	case OpMaskedGreaterFloat32x4:
		return rewriteValueAMD64_OpMaskedGreaterFloat32x4(v)
	case OpMaskedGreaterFloat32x8:
		return rewriteValueAMD64_OpMaskedGreaterFloat32x8(v)
	case OpMaskedGreaterFloat64x2:
		return rewriteValueAMD64_OpMaskedGreaterFloat64x2(v)
	case OpMaskedGreaterFloat64x4:
		return rewriteValueAMD64_OpMaskedGreaterFloat64x4(v)
	case OpMaskedGreaterFloat64x8:
		return rewriteValueAMD64_OpMaskedGreaterFloat64x8(v)
	case OpMaskedGreaterInt16x16:
		return rewriteValueAMD64_OpMaskedGreaterInt16x16(v)
	case OpMaskedGreaterInt16x32:
		return rewriteValueAMD64_OpMaskedGreaterInt16x32(v)
	case OpMaskedGreaterInt16x8:
		return rewriteValueAMD64_OpMaskedGreaterInt16x8(v)
	case OpMaskedGreaterInt32x16:
		return rewriteValueAMD64_OpMaskedGreaterInt32x16(v)
	case OpMaskedGreaterInt32x4:
		return rewriteValueAMD64_OpMaskedGreaterInt32x4(v)
	case OpMaskedGreaterInt32x8:
		return rewriteValueAMD64_OpMaskedGreaterInt32x8(v)
	case OpMaskedGreaterInt64x2:
		return rewriteValueAMD64_OpMaskedGreaterInt64x2(v)
	case OpMaskedGreaterInt64x4:
		return rewriteValueAMD64_OpMaskedGreaterInt64x4(v)
	case OpMaskedGreaterInt64x8:
		return rewriteValueAMD64_OpMaskedGreaterInt64x8(v)
	case OpMaskedGreaterInt8x16:
		return rewriteValueAMD64_OpMaskedGreaterInt8x16(v)
	case OpMaskedGreaterInt8x32:
		return rewriteValueAMD64_OpMaskedGreaterInt8x32(v)
	case OpMaskedGreaterInt8x64:
		return rewriteValueAMD64_OpMaskedGreaterInt8x64(v)
	case OpMaskedGreaterUint16x16:
		return rewriteValueAMD64_OpMaskedGreaterUint16x16(v)
	case OpMaskedGreaterUint16x32:
		return rewriteValueAMD64_OpMaskedGreaterUint16x32(v)
	case OpMaskedGreaterUint16x8:
		return rewriteValueAMD64_OpMaskedGreaterUint16x8(v)
	case OpMaskedGreaterUint32x16:
		return rewriteValueAMD64_OpMaskedGreaterUint32x16(v)
	case OpMaskedGreaterUint32x4:
		return rewriteValueAMD64_OpMaskedGreaterUint32x4(v)
	case OpMaskedGreaterUint32x8:
		return rewriteValueAMD64_OpMaskedGreaterUint32x8(v)
	case OpMaskedGreaterUint64x2:
		return rewriteValueAMD64_OpMaskedGreaterUint64x2(v)
	case OpMaskedGreaterUint64x4:
		return rewriteValueAMD64_OpMaskedGreaterUint64x4(v)
	case OpMaskedGreaterUint64x8:
		return rewriteValueAMD64_OpMaskedGreaterUint64x8(v)
	case OpMaskedGreaterUint8x16:
		return rewriteValueAMD64_OpMaskedGreaterUint8x16(v)
	case OpMaskedGreaterUint8x32:
		return rewriteValueAMD64_OpMaskedGreaterUint8x32(v)
	case OpMaskedGreaterUint8x64:
		return rewriteValueAMD64_OpMaskedGreaterUint8x64(v)
	case OpMaskedIsNanFloat32x16:
		return rewriteValueAMD64_OpMaskedIsNanFloat32x16(v)
	case OpMaskedIsNanFloat32x4:
		return rewriteValueAMD64_OpMaskedIsNanFloat32x4(v)
	case OpMaskedIsNanFloat32x8:
		return rewriteValueAMD64_OpMaskedIsNanFloat32x8(v)
	case OpMaskedIsNanFloat64x2:
		return rewriteValueAMD64_OpMaskedIsNanFloat64x2(v)
	case OpMaskedIsNanFloat64x4:
		return rewriteValueAMD64_OpMaskedIsNanFloat64x4(v)
	case OpMaskedIsNanFloat64x8:
		return rewriteValueAMD64_OpMaskedIsNanFloat64x8(v)
	case OpMaskedLessEqualFloat32x16:
		return rewriteValueAMD64_OpMaskedLessEqualFloat32x16(v)
	case OpMaskedLessEqualFloat32x4:
		return rewriteValueAMD64_OpMaskedLessEqualFloat32x4(v)
	case OpMaskedLessEqualFloat32x8:
		return rewriteValueAMD64_OpMaskedLessEqualFloat32x8(v)
	case OpMaskedLessEqualFloat64x2:
		return rewriteValueAMD64_OpMaskedLessEqualFloat64x2(v)
	case OpMaskedLessEqualFloat64x4:
		return rewriteValueAMD64_OpMaskedLessEqualFloat64x4(v)
	case OpMaskedLessEqualFloat64x8:
		return rewriteValueAMD64_OpMaskedLessEqualFloat64x8(v)
	case OpMaskedLessEqualInt16x16:
		return rewriteValueAMD64_OpMaskedLessEqualInt16x16(v)
	case OpMaskedLessEqualInt16x32:
		return rewriteValueAMD64_OpMaskedLessEqualInt16x32(v)
	case OpMaskedLessEqualInt16x8:
		return rewriteValueAMD64_OpMaskedLessEqualInt16x8(v)
	case OpMaskedLessEqualInt32x16:
		return rewriteValueAMD64_OpMaskedLessEqualInt32x16(v)
	case OpMaskedLessEqualInt32x4:
		return rewriteValueAMD64_OpMaskedLessEqualInt32x4(v)
	case OpMaskedLessEqualInt32x8:
		return rewriteValueAMD64_OpMaskedLessEqualInt32x8(v)
	case OpMaskedLessEqualInt64x2:
		return rewriteValueAMD64_OpMaskedLessEqualInt64x2(v)
	case OpMaskedLessEqualInt64x4:
		return rewriteValueAMD64_OpMaskedLessEqualInt64x4(v)
	case OpMaskedLessEqualInt64x8:
		return rewriteValueAMD64_OpMaskedLessEqualInt64x8(v)
	case OpMaskedLessEqualInt8x16:
		return rewriteValueAMD64_OpMaskedLessEqualInt8x16(v)
	case OpMaskedLessEqualInt8x32:
		return rewriteValueAMD64_OpMaskedLessEqualInt8x32(v)
	case OpMaskedLessEqualInt8x64:
		return rewriteValueAMD64_OpMaskedLessEqualInt8x64(v)
	case OpMaskedLessEqualUint16x16:
		return rewriteValueAMD64_OpMaskedLessEqualUint16x16(v)
	case OpMaskedLessEqualUint16x32:
		return rewriteValueAMD64_OpMaskedLessEqualUint16x32(v)
	case OpMaskedLessEqualUint16x8:
		return rewriteValueAMD64_OpMaskedLessEqualUint16x8(v)
	case OpMaskedLessEqualUint32x16:
		return rewriteValueAMD64_OpMaskedLessEqualUint32x16(v)
	case OpMaskedLessEqualUint32x4:
		return rewriteValueAMD64_OpMaskedLessEqualUint32x4(v)
	case OpMaskedLessEqualUint32x8:
		return rewriteValueAMD64_OpMaskedLessEqualUint32x8(v)
	case OpMaskedLessEqualUint64x2:
		return rewriteValueAMD64_OpMaskedLessEqualUint64x2(v)
	case OpMaskedLessEqualUint64x4:
		return rewriteValueAMD64_OpMaskedLessEqualUint64x4(v)
	case OpMaskedLessEqualUint64x8:
		return rewriteValueAMD64_OpMaskedLessEqualUint64x8(v)
	case OpMaskedLessEqualUint8x16:
		return rewriteValueAMD64_OpMaskedLessEqualUint8x16(v)
	case OpMaskedLessEqualUint8x32:
		return rewriteValueAMD64_OpMaskedLessEqualUint8x32(v)
	case OpMaskedLessEqualUint8x64:
		return rewriteValueAMD64_OpMaskedLessEqualUint8x64(v)
	case OpMaskedLessFloat32x16:
		return rewriteValueAMD64_OpMaskedLessFloat32x16(v)
	case OpMaskedLessFloat32x4:
		return rewriteValueAMD64_OpMaskedLessFloat32x4(v)
	case OpMaskedLessFloat32x8:
		return rewriteValueAMD64_OpMaskedLessFloat32x8(v)
	case OpMaskedLessFloat64x2:
		return rewriteValueAMD64_OpMaskedLessFloat64x2(v)
	case OpMaskedLessFloat64x4:
		return rewriteValueAMD64_OpMaskedLessFloat64x4(v)
	case OpMaskedLessFloat64x8:
		return rewriteValueAMD64_OpMaskedLessFloat64x8(v)
	case OpMaskedLessInt16x16:
		return rewriteValueAMD64_OpMaskedLessInt16x16(v)
	case OpMaskedLessInt16x32:
		return rewriteValueAMD64_OpMaskedLessInt16x32(v)
	case OpMaskedLessInt16x8:
		return rewriteValueAMD64_OpMaskedLessInt16x8(v)
	case OpMaskedLessInt32x16:
		return rewriteValueAMD64_OpMaskedLessInt32x16(v)
	case OpMaskedLessInt32x4:
		return rewriteValueAMD64_OpMaskedLessInt32x4(v)
	case OpMaskedLessInt32x8:
		return rewriteValueAMD64_OpMaskedLessInt32x8(v)
	case OpMaskedLessInt64x2:
		return rewriteValueAMD64_OpMaskedLessInt64x2(v)
	case OpMaskedLessInt64x4:
		return rewriteValueAMD64_OpMaskedLessInt64x4(v)
	case OpMaskedLessInt64x8:
		return rewriteValueAMD64_OpMaskedLessInt64x8(v)
	case OpMaskedLessInt8x16:
		return rewriteValueAMD64_OpMaskedLessInt8x16(v)
	case OpMaskedLessInt8x32:
		return rewriteValueAMD64_OpMaskedLessInt8x32(v)
	case OpMaskedLessInt8x64:
		return rewriteValueAMD64_OpMaskedLessInt8x64(v)
	case OpMaskedLessUint16x16:
		return rewriteValueAMD64_OpMaskedLessUint16x16(v)
	case OpMaskedLessUint16x32:
		return rewriteValueAMD64_OpMaskedLessUint16x32(v)
	case OpMaskedLessUint16x8:
		return rewriteValueAMD64_OpMaskedLessUint16x8(v)
	case OpMaskedLessUint32x16:
		return rewriteValueAMD64_OpMaskedLessUint32x16(v)
	case OpMaskedLessUint32x4:
		return rewriteValueAMD64_OpMaskedLessUint32x4(v)
	case OpMaskedLessUint32x8:
		return rewriteValueAMD64_OpMaskedLessUint32x8(v)
	case OpMaskedLessUint64x2:
		return rewriteValueAMD64_OpMaskedLessUint64x2(v)
	case OpMaskedLessUint64x4:
		return rewriteValueAMD64_OpMaskedLessUint64x4(v)
	case OpMaskedLessUint64x8:
		return rewriteValueAMD64_OpMaskedLessUint64x8(v)
	case OpMaskedLessUint8x16:
		return rewriteValueAMD64_OpMaskedLessUint8x16(v)
	case OpMaskedLessUint8x32:
		return rewriteValueAMD64_OpMaskedLessUint8x32(v)
	case OpMaskedLessUint8x64:
		return rewriteValueAMD64_OpMaskedLessUint8x64(v)
	case OpMaskedMaxFloat32x16:
		return rewriteValueAMD64_OpMaskedMaxFloat32x16(v)
	case OpMaskedMaxFloat32x4:
		return rewriteValueAMD64_OpMaskedMaxFloat32x4(v)
	case OpMaskedMaxFloat32x8:
		return rewriteValueAMD64_OpMaskedMaxFloat32x8(v)
	case OpMaskedMaxFloat64x2:
		return rewriteValueAMD64_OpMaskedMaxFloat64x2(v)
	case OpMaskedMaxFloat64x4:
		return rewriteValueAMD64_OpMaskedMaxFloat64x4(v)
	case OpMaskedMaxFloat64x8:
		return rewriteValueAMD64_OpMaskedMaxFloat64x8(v)
	case OpMaskedMaxInt16x16:
		return rewriteValueAMD64_OpMaskedMaxInt16x16(v)
	case OpMaskedMaxInt16x32:
		return rewriteValueAMD64_OpMaskedMaxInt16x32(v)
	case OpMaskedMaxInt16x8:
		return rewriteValueAMD64_OpMaskedMaxInt16x8(v)
	case OpMaskedMaxInt32x16:
		return rewriteValueAMD64_OpMaskedMaxInt32x16(v)
	case OpMaskedMaxInt32x4:
		return rewriteValueAMD64_OpMaskedMaxInt32x4(v)
	case OpMaskedMaxInt32x8:
		return rewriteValueAMD64_OpMaskedMaxInt32x8(v)
	case OpMaskedMaxInt64x2:
		return rewriteValueAMD64_OpMaskedMaxInt64x2(v)
	case OpMaskedMaxInt64x4:
		return rewriteValueAMD64_OpMaskedMaxInt64x4(v)
	case OpMaskedMaxInt64x8:
		return rewriteValueAMD64_OpMaskedMaxInt64x8(v)
	case OpMaskedMaxInt8x16:
		return rewriteValueAMD64_OpMaskedMaxInt8x16(v)
	case OpMaskedMaxInt8x32:
		return rewriteValueAMD64_OpMaskedMaxInt8x32(v)
	case OpMaskedMaxInt8x64:
		return rewriteValueAMD64_OpMaskedMaxInt8x64(v)
	case OpMaskedMaxUint16x16:
		return rewriteValueAMD64_OpMaskedMaxUint16x16(v)
	case OpMaskedMaxUint16x32:
		return rewriteValueAMD64_OpMaskedMaxUint16x32(v)
	case OpMaskedMaxUint16x8:
		return rewriteValueAMD64_OpMaskedMaxUint16x8(v)
	case OpMaskedMaxUint32x16:
		return rewriteValueAMD64_OpMaskedMaxUint32x16(v)
	case OpMaskedMaxUint32x4:
		return rewriteValueAMD64_OpMaskedMaxUint32x4(v)
	case OpMaskedMaxUint32x8:
		return rewriteValueAMD64_OpMaskedMaxUint32x8(v)
	case OpMaskedMaxUint64x2:
		return rewriteValueAMD64_OpMaskedMaxUint64x2(v)
	case OpMaskedMaxUint64x4:
		return rewriteValueAMD64_OpMaskedMaxUint64x4(v)
	case OpMaskedMaxUint64x8:
		return rewriteValueAMD64_OpMaskedMaxUint64x8(v)
	case OpMaskedMaxUint8x16:
		return rewriteValueAMD64_OpMaskedMaxUint8x16(v)
	case OpMaskedMaxUint8x32:
		return rewriteValueAMD64_OpMaskedMaxUint8x32(v)
	case OpMaskedMaxUint8x64:
		return rewriteValueAMD64_OpMaskedMaxUint8x64(v)
	case OpMaskedMinFloat32x16:
		return rewriteValueAMD64_OpMaskedMinFloat32x16(v)
	case OpMaskedMinFloat32x4:
		return rewriteValueAMD64_OpMaskedMinFloat32x4(v)
	case OpMaskedMinFloat32x8:
		return rewriteValueAMD64_OpMaskedMinFloat32x8(v)
	case OpMaskedMinFloat64x2:
		return rewriteValueAMD64_OpMaskedMinFloat64x2(v)
	case OpMaskedMinFloat64x4:
		return rewriteValueAMD64_OpMaskedMinFloat64x4(v)
	case OpMaskedMinFloat64x8:
		return rewriteValueAMD64_OpMaskedMinFloat64x8(v)
	case OpMaskedMinInt16x16:
		return rewriteValueAMD64_OpMaskedMinInt16x16(v)
	case OpMaskedMinInt16x32:
		return rewriteValueAMD64_OpMaskedMinInt16x32(v)
	case OpMaskedMinInt16x8:
		return rewriteValueAMD64_OpMaskedMinInt16x8(v)
	case OpMaskedMinInt32x16:
		return rewriteValueAMD64_OpMaskedMinInt32x16(v)
	case OpMaskedMinInt32x4:
		return rewriteValueAMD64_OpMaskedMinInt32x4(v)
	case OpMaskedMinInt32x8:
		return rewriteValueAMD64_OpMaskedMinInt32x8(v)
	case OpMaskedMinInt64x2:
		return rewriteValueAMD64_OpMaskedMinInt64x2(v)
	case OpMaskedMinInt64x4:
		return rewriteValueAMD64_OpMaskedMinInt64x4(v)
	case OpMaskedMinInt64x8:
		return rewriteValueAMD64_OpMaskedMinInt64x8(v)
	case OpMaskedMinInt8x16:
		return rewriteValueAMD64_OpMaskedMinInt8x16(v)
	case OpMaskedMinInt8x32:
		return rewriteValueAMD64_OpMaskedMinInt8x32(v)
	case OpMaskedMinInt8x64:
		return rewriteValueAMD64_OpMaskedMinInt8x64(v)
	case OpMaskedMinUint16x16:
		return rewriteValueAMD64_OpMaskedMinUint16x16(v)
	case OpMaskedMinUint16x32:
		return rewriteValueAMD64_OpMaskedMinUint16x32(v)
	case OpMaskedMinUint16x8:
		return rewriteValueAMD64_OpMaskedMinUint16x8(v)
	case OpMaskedMinUint32x16:
		return rewriteValueAMD64_OpMaskedMinUint32x16(v)
	case OpMaskedMinUint32x4:
		return rewriteValueAMD64_OpMaskedMinUint32x4(v)
	case OpMaskedMinUint32x8:
		return rewriteValueAMD64_OpMaskedMinUint32x8(v)
	case OpMaskedMinUint64x2:
		return rewriteValueAMD64_OpMaskedMinUint64x2(v)
	case OpMaskedMinUint64x4:
		return rewriteValueAMD64_OpMaskedMinUint64x4(v)
	case OpMaskedMinUint64x8:
		return rewriteValueAMD64_OpMaskedMinUint64x8(v)
	case OpMaskedMinUint8x16:
		return rewriteValueAMD64_OpMaskedMinUint8x16(v)
	case OpMaskedMinUint8x32:
		return rewriteValueAMD64_OpMaskedMinUint8x32(v)
	case OpMaskedMinUint8x64:
		return rewriteValueAMD64_OpMaskedMinUint8x64(v)
	case OpMaskedMulByPowOf2Float32x16:
		return rewriteValueAMD64_OpMaskedMulByPowOf2Float32x16(v)
	case OpMaskedMulByPowOf2Float32x4:
		return rewriteValueAMD64_OpMaskedMulByPowOf2Float32x4(v)
	case OpMaskedMulByPowOf2Float32x8:
		return rewriteValueAMD64_OpMaskedMulByPowOf2Float32x8(v)
	case OpMaskedMulByPowOf2Float64x2:
		return rewriteValueAMD64_OpMaskedMulByPowOf2Float64x2(v)
	case OpMaskedMulByPowOf2Float64x4:
		return rewriteValueAMD64_OpMaskedMulByPowOf2Float64x4(v)
	case OpMaskedMulByPowOf2Float64x8:
		return rewriteValueAMD64_OpMaskedMulByPowOf2Float64x8(v)
	case OpMaskedMulEvenWidenInt64x2:
		return rewriteValueAMD64_OpMaskedMulEvenWidenInt64x2(v)
	case OpMaskedMulEvenWidenInt64x4:
		return rewriteValueAMD64_OpMaskedMulEvenWidenInt64x4(v)
	case OpMaskedMulEvenWidenInt64x8:
		return rewriteValueAMD64_OpMaskedMulEvenWidenInt64x8(v)
	case OpMaskedMulEvenWidenUint64x2:
		return rewriteValueAMD64_OpMaskedMulEvenWidenUint64x2(v)
	case OpMaskedMulEvenWidenUint64x4:
		return rewriteValueAMD64_OpMaskedMulEvenWidenUint64x4(v)
	case OpMaskedMulEvenWidenUint64x8:
		return rewriteValueAMD64_OpMaskedMulEvenWidenUint64x8(v)
	case OpMaskedMulFloat32x16:
		return rewriteValueAMD64_OpMaskedMulFloat32x16(v)
	case OpMaskedMulFloat32x4:
		return rewriteValueAMD64_OpMaskedMulFloat32x4(v)
	case OpMaskedMulFloat32x8:
		return rewriteValueAMD64_OpMaskedMulFloat32x8(v)
	case OpMaskedMulFloat64x2:
		return rewriteValueAMD64_OpMaskedMulFloat64x2(v)
	case OpMaskedMulFloat64x4:
		return rewriteValueAMD64_OpMaskedMulFloat64x4(v)
	case OpMaskedMulFloat64x8:
		return rewriteValueAMD64_OpMaskedMulFloat64x8(v)
	case OpMaskedMulHighInt16x16:
		return rewriteValueAMD64_OpMaskedMulHighInt16x16(v)
	case OpMaskedMulHighInt16x32:
		return rewriteValueAMD64_OpMaskedMulHighInt16x32(v)
	case OpMaskedMulHighInt16x8:
		return rewriteValueAMD64_OpMaskedMulHighInt16x8(v)
	case OpMaskedMulHighUint16x16:
		return rewriteValueAMD64_OpMaskedMulHighUint16x16(v)
	case OpMaskedMulHighUint16x32:
		return rewriteValueAMD64_OpMaskedMulHighUint16x32(v)
	case OpMaskedMulHighUint16x8:
		return rewriteValueAMD64_OpMaskedMulHighUint16x8(v)
	case OpMaskedMulLowInt16x16:
		return rewriteValueAMD64_OpMaskedMulLowInt16x16(v)
	case OpMaskedMulLowInt16x32:
		return rewriteValueAMD64_OpMaskedMulLowInt16x32(v)
	case OpMaskedMulLowInt16x8:
		return rewriteValueAMD64_OpMaskedMulLowInt16x8(v)
	case OpMaskedMulLowInt32x16:
		return rewriteValueAMD64_OpMaskedMulLowInt32x16(v)
	case OpMaskedMulLowInt32x4:
		return rewriteValueAMD64_OpMaskedMulLowInt32x4(v)
	case OpMaskedMulLowInt32x8:
		return rewriteValueAMD64_OpMaskedMulLowInt32x8(v)
	case OpMaskedMulLowInt64x2:
		return rewriteValueAMD64_OpMaskedMulLowInt64x2(v)
	case OpMaskedMulLowInt64x4:
		return rewriteValueAMD64_OpMaskedMulLowInt64x4(v)
	case OpMaskedMulLowInt64x8:
		return rewriteValueAMD64_OpMaskedMulLowInt64x8(v)
	case OpMaskedNotEqualFloat32x16:
		return rewriteValueAMD64_OpMaskedNotEqualFloat32x16(v)
	case OpMaskedNotEqualFloat32x4:
		return rewriteValueAMD64_OpMaskedNotEqualFloat32x4(v)
	case OpMaskedNotEqualFloat32x8:
		return rewriteValueAMD64_OpMaskedNotEqualFloat32x8(v)
	case OpMaskedNotEqualFloat64x2:
		return rewriteValueAMD64_OpMaskedNotEqualFloat64x2(v)
	case OpMaskedNotEqualFloat64x4:
		return rewriteValueAMD64_OpMaskedNotEqualFloat64x4(v)
	case OpMaskedNotEqualFloat64x8:
		return rewriteValueAMD64_OpMaskedNotEqualFloat64x8(v)
	case OpMaskedNotEqualInt16x16:
		return rewriteValueAMD64_OpMaskedNotEqualInt16x16(v)
	case OpMaskedNotEqualInt16x32:
		return rewriteValueAMD64_OpMaskedNotEqualInt16x32(v)
	case OpMaskedNotEqualInt16x8:
		return rewriteValueAMD64_OpMaskedNotEqualInt16x8(v)
	case OpMaskedNotEqualInt32x16:
		return rewriteValueAMD64_OpMaskedNotEqualInt32x16(v)
	case OpMaskedNotEqualInt32x4:
		return rewriteValueAMD64_OpMaskedNotEqualInt32x4(v)
	case OpMaskedNotEqualInt32x8:
		return rewriteValueAMD64_OpMaskedNotEqualInt32x8(v)
	case OpMaskedNotEqualInt64x2:
		return rewriteValueAMD64_OpMaskedNotEqualInt64x2(v)
	case OpMaskedNotEqualInt64x4:
		return rewriteValueAMD64_OpMaskedNotEqualInt64x4(v)
	case OpMaskedNotEqualInt64x8:
		return rewriteValueAMD64_OpMaskedNotEqualInt64x8(v)
	case OpMaskedNotEqualInt8x16:
		return rewriteValueAMD64_OpMaskedNotEqualInt8x16(v)
	case OpMaskedNotEqualInt8x32:
		return rewriteValueAMD64_OpMaskedNotEqualInt8x32(v)
	case OpMaskedNotEqualInt8x64:
		return rewriteValueAMD64_OpMaskedNotEqualInt8x64(v)
	case OpMaskedNotEqualUint16x16:
		return rewriteValueAMD64_OpMaskedNotEqualUint16x16(v)
	case OpMaskedNotEqualUint16x32:
		return rewriteValueAMD64_OpMaskedNotEqualUint16x32(v)
	case OpMaskedNotEqualUint16x8:
		return rewriteValueAMD64_OpMaskedNotEqualUint16x8(v)
	case OpMaskedNotEqualUint32x16:
		return rewriteValueAMD64_OpMaskedNotEqualUint32x16(v)
	case OpMaskedNotEqualUint32x4:
		return rewriteValueAMD64_OpMaskedNotEqualUint32x4(v)
	case OpMaskedNotEqualUint32x8:
		return rewriteValueAMD64_OpMaskedNotEqualUint32x8(v)
	case OpMaskedNotEqualUint64x2:
		return rewriteValueAMD64_OpMaskedNotEqualUint64x2(v)
	case OpMaskedNotEqualUint64x4:
		return rewriteValueAMD64_OpMaskedNotEqualUint64x4(v)
	case OpMaskedNotEqualUint64x8:
		return rewriteValueAMD64_OpMaskedNotEqualUint64x8(v)
	case OpMaskedNotEqualUint8x16:
		return rewriteValueAMD64_OpMaskedNotEqualUint8x16(v)
	case OpMaskedNotEqualUint8x32:
		return rewriteValueAMD64_OpMaskedNotEqualUint8x32(v)
	case OpMaskedNotEqualUint8x64:
		return rewriteValueAMD64_OpMaskedNotEqualUint8x64(v)
	case OpMaskedOrFloat32x16:
		return rewriteValueAMD64_OpMaskedOrFloat32x16(v)
	case OpMaskedOrFloat32x4:
		return rewriteValueAMD64_OpMaskedOrFloat32x4(v)
	case OpMaskedOrFloat32x8:
		return rewriteValueAMD64_OpMaskedOrFloat32x8(v)
	case OpMaskedOrFloat64x2:
		return rewriteValueAMD64_OpMaskedOrFloat64x2(v)
	case OpMaskedOrFloat64x4:
		return rewriteValueAMD64_OpMaskedOrFloat64x4(v)
	case OpMaskedOrFloat64x8:
		return rewriteValueAMD64_OpMaskedOrFloat64x8(v)
	case OpMaskedOrInt32x16:
		return rewriteValueAMD64_OpMaskedOrInt32x16(v)
	case OpMaskedOrInt32x4:
		return rewriteValueAMD64_OpMaskedOrInt32x4(v)
	case OpMaskedOrInt32x8:
		return rewriteValueAMD64_OpMaskedOrInt32x8(v)
	case OpMaskedOrInt64x2:
		return rewriteValueAMD64_OpMaskedOrInt64x2(v)
	case OpMaskedOrInt64x4:
		return rewriteValueAMD64_OpMaskedOrInt64x4(v)
	case OpMaskedOrInt64x8:
		return rewriteValueAMD64_OpMaskedOrInt64x8(v)
	case OpMaskedOrUint32x16:
		return rewriteValueAMD64_OpMaskedOrUint32x16(v)
	case OpMaskedOrUint32x4:
		return rewriteValueAMD64_OpMaskedOrUint32x4(v)
	case OpMaskedOrUint32x8:
		return rewriteValueAMD64_OpMaskedOrUint32x8(v)
	case OpMaskedOrUint64x2:
		return rewriteValueAMD64_OpMaskedOrUint64x2(v)
	case OpMaskedOrUint64x4:
		return rewriteValueAMD64_OpMaskedOrUint64x4(v)
	case OpMaskedOrUint64x8:
		return rewriteValueAMD64_OpMaskedOrUint64x8(v)
	case OpMaskedPairDotProdAccumulateInt32x16:
		return rewriteValueAMD64_OpMaskedPairDotProdAccumulateInt32x16(v)
	case OpMaskedPairDotProdAccumulateInt32x4:
		return rewriteValueAMD64_OpMaskedPairDotProdAccumulateInt32x4(v)
	case OpMaskedPairDotProdAccumulateInt32x8:
		return rewriteValueAMD64_OpMaskedPairDotProdAccumulateInt32x8(v)
	case OpMaskedPairDotProdInt16x16:
		return rewriteValueAMD64_OpMaskedPairDotProdInt16x16(v)
	case OpMaskedPairDotProdInt16x32:
		return rewriteValueAMD64_OpMaskedPairDotProdInt16x32(v)
	case OpMaskedPairDotProdInt16x8:
		return rewriteValueAMD64_OpMaskedPairDotProdInt16x8(v)
	case OpMaskedPopCountInt16x16:
		return rewriteValueAMD64_OpMaskedPopCountInt16x16(v)
	case OpMaskedPopCountInt16x32:
		return rewriteValueAMD64_OpMaskedPopCountInt16x32(v)
	case OpMaskedPopCountInt16x8:
		return rewriteValueAMD64_OpMaskedPopCountInt16x8(v)
	case OpMaskedPopCountInt32x16:
		return rewriteValueAMD64_OpMaskedPopCountInt32x16(v)
	case OpMaskedPopCountInt32x4:
		return rewriteValueAMD64_OpMaskedPopCountInt32x4(v)
	case OpMaskedPopCountInt32x8:
		return rewriteValueAMD64_OpMaskedPopCountInt32x8(v)
	case OpMaskedPopCountInt64x2:
		return rewriteValueAMD64_OpMaskedPopCountInt64x2(v)
	case OpMaskedPopCountInt64x4:
		return rewriteValueAMD64_OpMaskedPopCountInt64x4(v)
	case OpMaskedPopCountInt64x8:
		return rewriteValueAMD64_OpMaskedPopCountInt64x8(v)
	case OpMaskedPopCountInt8x16:
		return rewriteValueAMD64_OpMaskedPopCountInt8x16(v)
	case OpMaskedPopCountInt8x32:
		return rewriteValueAMD64_OpMaskedPopCountInt8x32(v)
	case OpMaskedPopCountInt8x64:
		return rewriteValueAMD64_OpMaskedPopCountInt8x64(v)
	case OpMaskedPopCountUint16x16:
		return rewriteValueAMD64_OpMaskedPopCountUint16x16(v)
	case OpMaskedPopCountUint16x32:
		return rewriteValueAMD64_OpMaskedPopCountUint16x32(v)
	case OpMaskedPopCountUint16x8:
		return rewriteValueAMD64_OpMaskedPopCountUint16x8(v)
	case OpMaskedPopCountUint32x16:
		return rewriteValueAMD64_OpMaskedPopCountUint32x16(v)
	case OpMaskedPopCountUint32x4:
		return rewriteValueAMD64_OpMaskedPopCountUint32x4(v)
	case OpMaskedPopCountUint32x8:
		return rewriteValueAMD64_OpMaskedPopCountUint32x8(v)
	case OpMaskedPopCountUint64x2:
		return rewriteValueAMD64_OpMaskedPopCountUint64x2(v)
	case OpMaskedPopCountUint64x4:
		return rewriteValueAMD64_OpMaskedPopCountUint64x4(v)
	case OpMaskedPopCountUint64x8:
		return rewriteValueAMD64_OpMaskedPopCountUint64x8(v)
	case OpMaskedPopCountUint8x16:
		return rewriteValueAMD64_OpMaskedPopCountUint8x16(v)
	case OpMaskedPopCountUint8x32:
		return rewriteValueAMD64_OpMaskedPopCountUint8x32(v)
	case OpMaskedPopCountUint8x64:
		return rewriteValueAMD64_OpMaskedPopCountUint8x64(v)
	case OpMaskedRotateAllLeftInt32x16:
		return rewriteValueAMD64_OpMaskedRotateAllLeftInt32x16(v)
	case OpMaskedRotateAllLeftInt32x4:
		return rewriteValueAMD64_OpMaskedRotateAllLeftInt32x4(v)
	case OpMaskedRotateAllLeftInt32x8:
		return rewriteValueAMD64_OpMaskedRotateAllLeftInt32x8(v)
	case OpMaskedRotateAllLeftInt64x2:
		return rewriteValueAMD64_OpMaskedRotateAllLeftInt64x2(v)
	case OpMaskedRotateAllLeftInt64x4:
		return rewriteValueAMD64_OpMaskedRotateAllLeftInt64x4(v)
	case OpMaskedRotateAllLeftInt64x8:
		return rewriteValueAMD64_OpMaskedRotateAllLeftInt64x8(v)
	case OpMaskedRotateAllLeftUint32x16:
		return rewriteValueAMD64_OpMaskedRotateAllLeftUint32x16(v)
	case OpMaskedRotateAllLeftUint32x4:
		return rewriteValueAMD64_OpMaskedRotateAllLeftUint32x4(v)
	case OpMaskedRotateAllLeftUint32x8:
		return rewriteValueAMD64_OpMaskedRotateAllLeftUint32x8(v)
	case OpMaskedRotateAllLeftUint64x2:
		return rewriteValueAMD64_OpMaskedRotateAllLeftUint64x2(v)
	case OpMaskedRotateAllLeftUint64x4:
		return rewriteValueAMD64_OpMaskedRotateAllLeftUint64x4(v)
	case OpMaskedRotateAllLeftUint64x8:
		return rewriteValueAMD64_OpMaskedRotateAllLeftUint64x8(v)
	case OpMaskedRotateAllRightInt32x16:
		return rewriteValueAMD64_OpMaskedRotateAllRightInt32x16(v)
	case OpMaskedRotateAllRightInt32x4:
		return rewriteValueAMD64_OpMaskedRotateAllRightInt32x4(v)
	case OpMaskedRotateAllRightInt32x8:
		return rewriteValueAMD64_OpMaskedRotateAllRightInt32x8(v)
	case OpMaskedRotateAllRightInt64x2:
		return rewriteValueAMD64_OpMaskedRotateAllRightInt64x2(v)
	case OpMaskedRotateAllRightInt64x4:
		return rewriteValueAMD64_OpMaskedRotateAllRightInt64x4(v)
	case OpMaskedRotateAllRightInt64x8:
		return rewriteValueAMD64_OpMaskedRotateAllRightInt64x8(v)
	case OpMaskedRotateAllRightUint32x16:
		return rewriteValueAMD64_OpMaskedRotateAllRightUint32x16(v)
	case OpMaskedRotateAllRightUint32x4:
		return rewriteValueAMD64_OpMaskedRotateAllRightUint32x4(v)
	case OpMaskedRotateAllRightUint32x8:
		return rewriteValueAMD64_OpMaskedRotateAllRightUint32x8(v)
	case OpMaskedRotateAllRightUint64x2:
		return rewriteValueAMD64_OpMaskedRotateAllRightUint64x2(v)
	case OpMaskedRotateAllRightUint64x4:
		return rewriteValueAMD64_OpMaskedRotateAllRightUint64x4(v)
	case OpMaskedRotateAllRightUint64x8:
		return rewriteValueAMD64_OpMaskedRotateAllRightUint64x8(v)
	case OpMaskedRotateLeftInt32x16:
		return rewriteValueAMD64_OpMaskedRotateLeftInt32x16(v)
	case OpMaskedRotateLeftInt32x4:
		return rewriteValueAMD64_OpMaskedRotateLeftInt32x4(v)
	case OpMaskedRotateLeftInt32x8:
		return rewriteValueAMD64_OpMaskedRotateLeftInt32x8(v)
	case OpMaskedRotateLeftInt64x2:
		return rewriteValueAMD64_OpMaskedRotateLeftInt64x2(v)
	case OpMaskedRotateLeftInt64x4:
		return rewriteValueAMD64_OpMaskedRotateLeftInt64x4(v)
	case OpMaskedRotateLeftInt64x8:
		return rewriteValueAMD64_OpMaskedRotateLeftInt64x8(v)
	case OpMaskedRotateLeftUint32x16:
		return rewriteValueAMD64_OpMaskedRotateLeftUint32x16(v)
	case OpMaskedRotateLeftUint32x4:
		return rewriteValueAMD64_OpMaskedRotateLeftUint32x4(v)
	case OpMaskedRotateLeftUint32x8:
		return rewriteValueAMD64_OpMaskedRotateLeftUint32x8(v)
	case OpMaskedRotateLeftUint64x2:
		return rewriteValueAMD64_OpMaskedRotateLeftUint64x2(v)
	case OpMaskedRotateLeftUint64x4:
		return rewriteValueAMD64_OpMaskedRotateLeftUint64x4(v)
	case OpMaskedRotateLeftUint64x8:
		return rewriteValueAMD64_OpMaskedRotateLeftUint64x8(v)
	case OpMaskedRotateRightInt32x16:
		return rewriteValueAMD64_OpMaskedRotateRightInt32x16(v)
	case OpMaskedRotateRightInt32x4:
		return rewriteValueAMD64_OpMaskedRotateRightInt32x4(v)
	case OpMaskedRotateRightInt32x8:
		return rewriteValueAMD64_OpMaskedRotateRightInt32x8(v)
	case OpMaskedRotateRightInt64x2:
		return rewriteValueAMD64_OpMaskedRotateRightInt64x2(v)
	case OpMaskedRotateRightInt64x4:
		return rewriteValueAMD64_OpMaskedRotateRightInt64x4(v)
	case OpMaskedRotateRightInt64x8:
		return rewriteValueAMD64_OpMaskedRotateRightInt64x8(v)
	case OpMaskedRotateRightUint32x16:
		return rewriteValueAMD64_OpMaskedRotateRightUint32x16(v)
	case OpMaskedRotateRightUint32x4:
		return rewriteValueAMD64_OpMaskedRotateRightUint32x4(v)
	case OpMaskedRotateRightUint32x8:
		return rewriteValueAMD64_OpMaskedRotateRightUint32x8(v)
	case OpMaskedRotateRightUint64x2:
		return rewriteValueAMD64_OpMaskedRotateRightUint64x2(v)
	case OpMaskedRotateRightUint64x4:
		return rewriteValueAMD64_OpMaskedRotateRightUint64x4(v)
	case OpMaskedRotateRightUint64x8:
		return rewriteValueAMD64_OpMaskedRotateRightUint64x8(v)
	case OpMaskedRoundSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat32x16(v)
	case OpMaskedRoundSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat32x4(v)
	case OpMaskedRoundSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat32x8(v)
	case OpMaskedRoundSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat64x2(v)
	case OpMaskedRoundSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat64x4(v)
	case OpMaskedRoundSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat64x8(v)
	case OpMaskedRoundWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat32x16(v)
	case OpMaskedRoundWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat32x4(v)
	case OpMaskedRoundWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat32x8(v)
	case OpMaskedRoundWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat64x2(v)
	case OpMaskedRoundWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat64x4(v)
	case OpMaskedRoundWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat64x8(v)
	case OpMaskedSaturatedAddInt16x16:
		return rewriteValueAMD64_OpMaskedSaturatedAddInt16x16(v)
	case OpMaskedSaturatedAddInt16x32:
		return rewriteValueAMD64_OpMaskedSaturatedAddInt16x32(v)
	case OpMaskedSaturatedAddInt16x8:
		return rewriteValueAMD64_OpMaskedSaturatedAddInt16x8(v)
	case OpMaskedSaturatedAddInt8x16:
		return rewriteValueAMD64_OpMaskedSaturatedAddInt8x16(v)
	case OpMaskedSaturatedAddInt8x32:
		return rewriteValueAMD64_OpMaskedSaturatedAddInt8x32(v)
	case OpMaskedSaturatedAddInt8x64:
		return rewriteValueAMD64_OpMaskedSaturatedAddInt8x64(v)
	case OpMaskedSaturatedAddUint16x16:
		return rewriteValueAMD64_OpMaskedSaturatedAddUint16x16(v)
	case OpMaskedSaturatedAddUint16x32:
		return rewriteValueAMD64_OpMaskedSaturatedAddUint16x32(v)
	case OpMaskedSaturatedAddUint16x8:
		return rewriteValueAMD64_OpMaskedSaturatedAddUint16x8(v)
	case OpMaskedSaturatedAddUint8x16:
		return rewriteValueAMD64_OpMaskedSaturatedAddUint8x16(v)
	case OpMaskedSaturatedAddUint8x32:
		return rewriteValueAMD64_OpMaskedSaturatedAddUint8x32(v)
	case OpMaskedSaturatedAddUint8x64:
		return rewriteValueAMD64_OpMaskedSaturatedAddUint8x64(v)
	case OpMaskedSaturatedPairDotProdAccumulateInt32x16:
		return rewriteValueAMD64_OpMaskedSaturatedPairDotProdAccumulateInt32x16(v)
	case OpMaskedSaturatedPairDotProdAccumulateInt32x4:
		return rewriteValueAMD64_OpMaskedSaturatedPairDotProdAccumulateInt32x4(v)
	case OpMaskedSaturatedPairDotProdAccumulateInt32x8:
		return rewriteValueAMD64_OpMaskedSaturatedPairDotProdAccumulateInt32x8(v)
	case OpMaskedSaturatedSubInt16x16:
		return rewriteValueAMD64_OpMaskedSaturatedSubInt16x16(v)
	case OpMaskedSaturatedSubInt16x32:
		return rewriteValueAMD64_OpMaskedSaturatedSubInt16x32(v)
	case OpMaskedSaturatedSubInt16x8:
		return rewriteValueAMD64_OpMaskedSaturatedSubInt16x8(v)
	case OpMaskedSaturatedSubInt8x16:
		return rewriteValueAMD64_OpMaskedSaturatedSubInt8x16(v)
	case OpMaskedSaturatedSubInt8x32:
		return rewriteValueAMD64_OpMaskedSaturatedSubInt8x32(v)
	case OpMaskedSaturatedSubInt8x64:
		return rewriteValueAMD64_OpMaskedSaturatedSubInt8x64(v)
	case OpMaskedSaturatedSubUint16x16:
		return rewriteValueAMD64_OpMaskedSaturatedSubUint16x16(v)
	case OpMaskedSaturatedSubUint16x32:
		return rewriteValueAMD64_OpMaskedSaturatedSubUint16x32(v)
	case OpMaskedSaturatedSubUint16x8:
		return rewriteValueAMD64_OpMaskedSaturatedSubUint16x8(v)
	case OpMaskedSaturatedSubUint8x16:
		return rewriteValueAMD64_OpMaskedSaturatedSubUint8x16(v)
	case OpMaskedSaturatedSubUint8x32:
		return rewriteValueAMD64_OpMaskedSaturatedSubUint8x32(v)
	case OpMaskedSaturatedSubUint8x64:
		return rewriteValueAMD64_OpMaskedSaturatedSubUint8x64(v)
	case OpMaskedSaturatedUnsignedSignedPairDotProdUint8x16:
		return rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedPairDotProdUint8x16(v)
	case OpMaskedSaturatedUnsignedSignedPairDotProdUint8x32:
		return rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedPairDotProdUint8x32(v)
	case OpMaskedSaturatedUnsignedSignedPairDotProdUint8x64:
		return rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedPairDotProdUint8x64(v)
	case OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x16:
		return rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x16(v)
	case OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x4:
		return rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x4(v)
	case OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x8:
		return rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x8(v)
	case OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x16:
		return rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x16(v)
	case OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x4:
		return rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x4(v)
	case OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x8:
		return rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x8(v)
	case OpMaskedShiftAllLeftAndFillUpperFromInt16x16:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt16x16(v)
	case OpMaskedShiftAllLeftAndFillUpperFromInt16x32:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt16x32(v)
	case OpMaskedShiftAllLeftAndFillUpperFromInt16x8:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt16x8(v)
	case OpMaskedShiftAllLeftAndFillUpperFromInt32x16:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt32x16(v)
	case OpMaskedShiftAllLeftAndFillUpperFromInt32x4:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt32x4(v)
	case OpMaskedShiftAllLeftAndFillUpperFromInt32x8:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt32x8(v)
	case OpMaskedShiftAllLeftAndFillUpperFromInt64x2:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt64x2(v)
	case OpMaskedShiftAllLeftAndFillUpperFromInt64x4:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt64x4(v)
	case OpMaskedShiftAllLeftAndFillUpperFromInt64x8:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt64x8(v)
	case OpMaskedShiftAllLeftAndFillUpperFromUint16x16:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint16x16(v)
	case OpMaskedShiftAllLeftAndFillUpperFromUint16x32:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint16x32(v)
	case OpMaskedShiftAllLeftAndFillUpperFromUint16x8:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint16x8(v)
	case OpMaskedShiftAllLeftAndFillUpperFromUint32x16:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint32x16(v)
	case OpMaskedShiftAllLeftAndFillUpperFromUint32x4:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint32x4(v)
	case OpMaskedShiftAllLeftAndFillUpperFromUint32x8:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint32x8(v)
	case OpMaskedShiftAllLeftAndFillUpperFromUint64x2:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint64x2(v)
	case OpMaskedShiftAllLeftAndFillUpperFromUint64x4:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint64x4(v)
	case OpMaskedShiftAllLeftAndFillUpperFromUint64x8:
		return rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint64x8(v)
	case OpMaskedShiftAllLeftInt64x2:
		return rewriteValueAMD64_OpMaskedShiftAllLeftInt64x2(v)
	case OpMaskedShiftAllLeftInt64x4:
		return rewriteValueAMD64_OpMaskedShiftAllLeftInt64x4(v)
	case OpMaskedShiftAllLeftInt64x8:
		return rewriteValueAMD64_OpMaskedShiftAllLeftInt64x8(v)
	case OpMaskedShiftAllLeftUint64x2:
		return rewriteValueAMD64_OpMaskedShiftAllLeftUint64x2(v)
	case OpMaskedShiftAllLeftUint64x4:
		return rewriteValueAMD64_OpMaskedShiftAllLeftUint64x4(v)
	case OpMaskedShiftAllLeftUint64x8:
		return rewriteValueAMD64_OpMaskedShiftAllLeftUint64x8(v)
	case OpMaskedShiftAllRightAndFillUpperFromInt16x16:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt16x16(v)
	case OpMaskedShiftAllRightAndFillUpperFromInt16x32:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt16x32(v)
	case OpMaskedShiftAllRightAndFillUpperFromInt16x8:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt16x8(v)
	case OpMaskedShiftAllRightAndFillUpperFromInt32x16:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt32x16(v)
	case OpMaskedShiftAllRightAndFillUpperFromInt32x4:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt32x4(v)
	case OpMaskedShiftAllRightAndFillUpperFromInt32x8:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt32x8(v)
	case OpMaskedShiftAllRightAndFillUpperFromInt64x2:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt64x2(v)
	case OpMaskedShiftAllRightAndFillUpperFromInt64x4:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt64x4(v)
	case OpMaskedShiftAllRightAndFillUpperFromInt64x8:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt64x8(v)
	case OpMaskedShiftAllRightAndFillUpperFromUint16x16:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint16x16(v)
	case OpMaskedShiftAllRightAndFillUpperFromUint16x32:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint16x32(v)
	case OpMaskedShiftAllRightAndFillUpperFromUint16x8:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint16x8(v)
	case OpMaskedShiftAllRightAndFillUpperFromUint32x16:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint32x16(v)
	case OpMaskedShiftAllRightAndFillUpperFromUint32x4:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint32x4(v)
	case OpMaskedShiftAllRightAndFillUpperFromUint32x8:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint32x8(v)
	case OpMaskedShiftAllRightAndFillUpperFromUint64x2:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint64x2(v)
	case OpMaskedShiftAllRightAndFillUpperFromUint64x4:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint64x4(v)
	case OpMaskedShiftAllRightAndFillUpperFromUint64x8:
		return rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint64x8(v)
	case OpMaskedShiftAllRightInt64x2:
		return rewriteValueAMD64_OpMaskedShiftAllRightInt64x2(v)
	case OpMaskedShiftAllRightInt64x4:
		return rewriteValueAMD64_OpMaskedShiftAllRightInt64x4(v)
	case OpMaskedShiftAllRightInt64x8:
		return rewriteValueAMD64_OpMaskedShiftAllRightInt64x8(v)
	case OpMaskedShiftAllRightSignExtendedInt64x2:
		return rewriteValueAMD64_OpMaskedShiftAllRightSignExtendedInt64x2(v)
	case OpMaskedShiftAllRightSignExtendedInt64x4:
		return rewriteValueAMD64_OpMaskedShiftAllRightSignExtendedInt64x4(v)
	case OpMaskedShiftAllRightSignExtendedInt64x8:
		return rewriteValueAMD64_OpMaskedShiftAllRightSignExtendedInt64x8(v)
	case OpMaskedShiftAllRightUint64x2:
		return rewriteValueAMD64_OpMaskedShiftAllRightUint64x2(v)
	case OpMaskedShiftAllRightUint64x4:
		return rewriteValueAMD64_OpMaskedShiftAllRightUint64x4(v)
	case OpMaskedShiftAllRightUint64x8:
		return rewriteValueAMD64_OpMaskedShiftAllRightUint64x8(v)
	case OpMaskedShiftLeftAndFillUpperFromInt16x16:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt16x16(v)
	case OpMaskedShiftLeftAndFillUpperFromInt16x32:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt16x32(v)
	case OpMaskedShiftLeftAndFillUpperFromInt16x8:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt16x8(v)
	case OpMaskedShiftLeftAndFillUpperFromInt32x16:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt32x16(v)
	case OpMaskedShiftLeftAndFillUpperFromInt32x4:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt32x4(v)
	case OpMaskedShiftLeftAndFillUpperFromInt32x8:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt32x8(v)
	case OpMaskedShiftLeftAndFillUpperFromInt64x2:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt64x2(v)
	case OpMaskedShiftLeftAndFillUpperFromInt64x4:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt64x4(v)
	case OpMaskedShiftLeftAndFillUpperFromInt64x8:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt64x8(v)
	case OpMaskedShiftLeftAndFillUpperFromUint16x16:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint16x16(v)
	case OpMaskedShiftLeftAndFillUpperFromUint16x32:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint16x32(v)
	case OpMaskedShiftLeftAndFillUpperFromUint16x8:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint16x8(v)
	case OpMaskedShiftLeftAndFillUpperFromUint32x16:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint32x16(v)
	case OpMaskedShiftLeftAndFillUpperFromUint32x4:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint32x4(v)
	case OpMaskedShiftLeftAndFillUpperFromUint32x8:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint32x8(v)
	case OpMaskedShiftLeftAndFillUpperFromUint64x2:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint64x2(v)
	case OpMaskedShiftLeftAndFillUpperFromUint64x4:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint64x4(v)
	case OpMaskedShiftLeftAndFillUpperFromUint64x8:
		return rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint64x8(v)
	case OpMaskedShiftLeftInt16x16:
		return rewriteValueAMD64_OpMaskedShiftLeftInt16x16(v)
	case OpMaskedShiftLeftInt16x32:
		return rewriteValueAMD64_OpMaskedShiftLeftInt16x32(v)
	case OpMaskedShiftLeftInt16x8:
		return rewriteValueAMD64_OpMaskedShiftLeftInt16x8(v)
	case OpMaskedShiftLeftInt32x16:
		return rewriteValueAMD64_OpMaskedShiftLeftInt32x16(v)
	case OpMaskedShiftLeftInt32x4:
		return rewriteValueAMD64_OpMaskedShiftLeftInt32x4(v)
	case OpMaskedShiftLeftInt32x8:
		return rewriteValueAMD64_OpMaskedShiftLeftInt32x8(v)
	case OpMaskedShiftLeftInt64x2:
		return rewriteValueAMD64_OpMaskedShiftLeftInt64x2(v)
	case OpMaskedShiftLeftInt64x4:
		return rewriteValueAMD64_OpMaskedShiftLeftInt64x4(v)
	case OpMaskedShiftLeftInt64x8:
		return rewriteValueAMD64_OpMaskedShiftLeftInt64x8(v)
	case OpMaskedShiftLeftUint16x16:
		return rewriteValueAMD64_OpMaskedShiftLeftUint16x16(v)
	case OpMaskedShiftLeftUint16x32:
		return rewriteValueAMD64_OpMaskedShiftLeftUint16x32(v)
	case OpMaskedShiftLeftUint16x8:
		return rewriteValueAMD64_OpMaskedShiftLeftUint16x8(v)
	case OpMaskedShiftLeftUint32x16:
		return rewriteValueAMD64_OpMaskedShiftLeftUint32x16(v)
	case OpMaskedShiftLeftUint32x4:
		return rewriteValueAMD64_OpMaskedShiftLeftUint32x4(v)
	case OpMaskedShiftLeftUint32x8:
		return rewriteValueAMD64_OpMaskedShiftLeftUint32x8(v)
	case OpMaskedShiftLeftUint64x2:
		return rewriteValueAMD64_OpMaskedShiftLeftUint64x2(v)
	case OpMaskedShiftLeftUint64x4:
		return rewriteValueAMD64_OpMaskedShiftLeftUint64x4(v)
	case OpMaskedShiftLeftUint64x8:
		return rewriteValueAMD64_OpMaskedShiftLeftUint64x8(v)
	case OpMaskedShiftRightAndFillUpperFromInt16x16:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt16x16(v)
	case OpMaskedShiftRightAndFillUpperFromInt16x32:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt16x32(v)
	case OpMaskedShiftRightAndFillUpperFromInt16x8:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt16x8(v)
	case OpMaskedShiftRightAndFillUpperFromInt32x16:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt32x16(v)
	case OpMaskedShiftRightAndFillUpperFromInt32x4:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt32x4(v)
	case OpMaskedShiftRightAndFillUpperFromInt32x8:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt32x8(v)
	case OpMaskedShiftRightAndFillUpperFromInt64x2:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt64x2(v)
	case OpMaskedShiftRightAndFillUpperFromInt64x4:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt64x4(v)
	case OpMaskedShiftRightAndFillUpperFromInt64x8:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt64x8(v)
	case OpMaskedShiftRightAndFillUpperFromUint16x16:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint16x16(v)
	case OpMaskedShiftRightAndFillUpperFromUint16x32:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint16x32(v)
	case OpMaskedShiftRightAndFillUpperFromUint16x8:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint16x8(v)
	case OpMaskedShiftRightAndFillUpperFromUint32x16:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint32x16(v)
	case OpMaskedShiftRightAndFillUpperFromUint32x4:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint32x4(v)
	case OpMaskedShiftRightAndFillUpperFromUint32x8:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint32x8(v)
	case OpMaskedShiftRightAndFillUpperFromUint64x2:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint64x2(v)
	case OpMaskedShiftRightAndFillUpperFromUint64x4:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint64x4(v)
	case OpMaskedShiftRightAndFillUpperFromUint64x8:
		return rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint64x8(v)
	case OpMaskedShiftRightInt16x16:
		return rewriteValueAMD64_OpMaskedShiftRightInt16x16(v)
	case OpMaskedShiftRightInt16x32:
		return rewriteValueAMD64_OpMaskedShiftRightInt16x32(v)
	case OpMaskedShiftRightInt16x8:
		return rewriteValueAMD64_OpMaskedShiftRightInt16x8(v)
	case OpMaskedShiftRightInt32x16:
		return rewriteValueAMD64_OpMaskedShiftRightInt32x16(v)
	case OpMaskedShiftRightInt32x4:
		return rewriteValueAMD64_OpMaskedShiftRightInt32x4(v)
	case OpMaskedShiftRightInt32x8:
		return rewriteValueAMD64_OpMaskedShiftRightInt32x8(v)
	case OpMaskedShiftRightInt64x2:
		return rewriteValueAMD64_OpMaskedShiftRightInt64x2(v)
	case OpMaskedShiftRightInt64x4:
		return rewriteValueAMD64_OpMaskedShiftRightInt64x4(v)
	case OpMaskedShiftRightInt64x8:
		return rewriteValueAMD64_OpMaskedShiftRightInt64x8(v)
	case OpMaskedShiftRightSignExtendedInt16x16:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt16x16(v)
	case OpMaskedShiftRightSignExtendedInt16x32:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt16x32(v)
	case OpMaskedShiftRightSignExtendedInt16x8:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt16x8(v)
	case OpMaskedShiftRightSignExtendedInt32x16:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt32x16(v)
	case OpMaskedShiftRightSignExtendedInt32x4:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt32x4(v)
	case OpMaskedShiftRightSignExtendedInt32x8:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt32x8(v)
	case OpMaskedShiftRightSignExtendedInt64x2:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt64x2(v)
	case OpMaskedShiftRightSignExtendedInt64x4:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt64x4(v)
	case OpMaskedShiftRightSignExtendedInt64x8:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt64x8(v)
	case OpMaskedShiftRightSignExtendedUint16x16:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint16x16(v)
	case OpMaskedShiftRightSignExtendedUint16x32:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint16x32(v)
	case OpMaskedShiftRightSignExtendedUint16x8:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint16x8(v)
	case OpMaskedShiftRightSignExtendedUint32x16:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint32x16(v)
	case OpMaskedShiftRightSignExtendedUint32x4:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint32x4(v)
	case OpMaskedShiftRightSignExtendedUint32x8:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint32x8(v)
	case OpMaskedShiftRightSignExtendedUint64x2:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint64x2(v)
	case OpMaskedShiftRightSignExtendedUint64x4:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint64x4(v)
	case OpMaskedShiftRightSignExtendedUint64x8:
		return rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint64x8(v)
	case OpMaskedShiftRightUint16x16:
		return rewriteValueAMD64_OpMaskedShiftRightUint16x16(v)
	case OpMaskedShiftRightUint16x32:
		return rewriteValueAMD64_OpMaskedShiftRightUint16x32(v)
	case OpMaskedShiftRightUint16x8:
		return rewriteValueAMD64_OpMaskedShiftRightUint16x8(v)
	case OpMaskedShiftRightUint32x16:
		return rewriteValueAMD64_OpMaskedShiftRightUint32x16(v)
	case OpMaskedShiftRightUint32x4:
		return rewriteValueAMD64_OpMaskedShiftRightUint32x4(v)
	case OpMaskedShiftRightUint32x8:
		return rewriteValueAMD64_OpMaskedShiftRightUint32x8(v)
	case OpMaskedShiftRightUint64x2:
		return rewriteValueAMD64_OpMaskedShiftRightUint64x2(v)
	case OpMaskedShiftRightUint64x4:
		return rewriteValueAMD64_OpMaskedShiftRightUint64x4(v)
	case OpMaskedShiftRightUint64x8:
		return rewriteValueAMD64_OpMaskedShiftRightUint64x8(v)
	case OpMaskedSqrtFloat32x16:
		return rewriteValueAMD64_OpMaskedSqrtFloat32x16(v)
	case OpMaskedSqrtFloat32x4:
		return rewriteValueAMD64_OpMaskedSqrtFloat32x4(v)
	case OpMaskedSqrtFloat32x8:
		return rewriteValueAMD64_OpMaskedSqrtFloat32x8(v)
	case OpMaskedSqrtFloat64x2:
		return rewriteValueAMD64_OpMaskedSqrtFloat64x2(v)
	case OpMaskedSqrtFloat64x4:
		return rewriteValueAMD64_OpMaskedSqrtFloat64x4(v)
	case OpMaskedSqrtFloat64x8:
		return rewriteValueAMD64_OpMaskedSqrtFloat64x8(v)
	case OpMaskedSubFloat32x16:
		return rewriteValueAMD64_OpMaskedSubFloat32x16(v)
	case OpMaskedSubFloat32x4:
		return rewriteValueAMD64_OpMaskedSubFloat32x4(v)
	case OpMaskedSubFloat32x8:
		return rewriteValueAMD64_OpMaskedSubFloat32x8(v)
	case OpMaskedSubFloat64x2:
		return rewriteValueAMD64_OpMaskedSubFloat64x2(v)
	case OpMaskedSubFloat64x4:
		return rewriteValueAMD64_OpMaskedSubFloat64x4(v)
	case OpMaskedSubFloat64x8:
		return rewriteValueAMD64_OpMaskedSubFloat64x8(v)
	case OpMaskedSubInt16x16:
		return rewriteValueAMD64_OpMaskedSubInt16x16(v)
	case OpMaskedSubInt16x32:
		return rewriteValueAMD64_OpMaskedSubInt16x32(v)
	case OpMaskedSubInt16x8:
		return rewriteValueAMD64_OpMaskedSubInt16x8(v)
	case OpMaskedSubInt32x16:
		return rewriteValueAMD64_OpMaskedSubInt32x16(v)
	case OpMaskedSubInt32x4:
		return rewriteValueAMD64_OpMaskedSubInt32x4(v)
	case OpMaskedSubInt32x8:
		return rewriteValueAMD64_OpMaskedSubInt32x8(v)
	case OpMaskedSubInt64x2:
		return rewriteValueAMD64_OpMaskedSubInt64x2(v)
	case OpMaskedSubInt64x4:
		return rewriteValueAMD64_OpMaskedSubInt64x4(v)
	case OpMaskedSubInt64x8:
		return rewriteValueAMD64_OpMaskedSubInt64x8(v)
	case OpMaskedSubInt8x16:
		return rewriteValueAMD64_OpMaskedSubInt8x16(v)
	case OpMaskedSubInt8x32:
		return rewriteValueAMD64_OpMaskedSubInt8x32(v)
	case OpMaskedSubInt8x64:
		return rewriteValueAMD64_OpMaskedSubInt8x64(v)
	case OpMaskedSubUint16x16:
		return rewriteValueAMD64_OpMaskedSubUint16x16(v)
	case OpMaskedSubUint16x32:
		return rewriteValueAMD64_OpMaskedSubUint16x32(v)
	case OpMaskedSubUint16x8:
		return rewriteValueAMD64_OpMaskedSubUint16x8(v)
	case OpMaskedSubUint32x16:
		return rewriteValueAMD64_OpMaskedSubUint32x16(v)
	case OpMaskedSubUint32x4:
		return rewriteValueAMD64_OpMaskedSubUint32x4(v)
	case OpMaskedSubUint32x8:
		return rewriteValueAMD64_OpMaskedSubUint32x8(v)
	case OpMaskedSubUint64x2:
		return rewriteValueAMD64_OpMaskedSubUint64x2(v)
	case OpMaskedSubUint64x4:
		return rewriteValueAMD64_OpMaskedSubUint64x4(v)
	case OpMaskedSubUint64x8:
		return rewriteValueAMD64_OpMaskedSubUint64x8(v)
	case OpMaskedSubUint8x16:
		return rewriteValueAMD64_OpMaskedSubUint8x16(v)
	case OpMaskedSubUint8x32:
		return rewriteValueAMD64_OpMaskedSubUint8x32(v)
	case OpMaskedSubUint8x64:
		return rewriteValueAMD64_OpMaskedSubUint8x64(v)
	case OpMaskedTruncSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat32x16(v)
	case OpMaskedTruncSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat32x4(v)
	case OpMaskedTruncSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat32x8(v)
	case OpMaskedTruncSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat64x2(v)
	case OpMaskedTruncSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat64x4(v)
	case OpMaskedTruncSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat64x8(v)
	case OpMaskedTruncWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat32x16(v)
	case OpMaskedTruncWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat32x4(v)
	case OpMaskedTruncWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat32x8(v)
	case OpMaskedTruncWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat64x2(v)
	case OpMaskedTruncWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat64x4(v)
	case OpMaskedTruncWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat64x8(v)
	case OpMaskedUnsignedSignedQuadDotProdAccumulateInt32x16:
		return rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateInt32x16(v)
	case OpMaskedUnsignedSignedQuadDotProdAccumulateInt32x4:
		return rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateInt32x4(v)
	case OpMaskedUnsignedSignedQuadDotProdAccumulateInt32x8:
		return rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateInt32x8(v)
	case OpMaskedUnsignedSignedQuadDotProdAccumulateUint32x16:
		return rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateUint32x16(v)
	case OpMaskedUnsignedSignedQuadDotProdAccumulateUint32x4:
		return rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateUint32x4(v)
	case OpMaskedUnsignedSignedQuadDotProdAccumulateUint32x8:
		return rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateUint32x8(v)
	case OpMaskedXorFloat32x16:
		return rewriteValueAMD64_OpMaskedXorFloat32x16(v)
	case OpMaskedXorFloat32x4:
		return rewriteValueAMD64_OpMaskedXorFloat32x4(v)
	case OpMaskedXorFloat32x8:
		return rewriteValueAMD64_OpMaskedXorFloat32x8(v)
	case OpMaskedXorFloat64x2:
		return rewriteValueAMD64_OpMaskedXorFloat64x2(v)
	case OpMaskedXorFloat64x4:
		return rewriteValueAMD64_OpMaskedXorFloat64x4(v)
	case OpMaskedXorFloat64x8:
		return rewriteValueAMD64_OpMaskedXorFloat64x8(v)
	case OpMaskedXorInt32x16:
		return rewriteValueAMD64_OpMaskedXorInt32x16(v)
	case OpMaskedXorInt32x4:
		return rewriteValueAMD64_OpMaskedXorInt32x4(v)
	case OpMaskedXorInt32x8:
		return rewriteValueAMD64_OpMaskedXorInt32x8(v)
	case OpMaskedXorInt64x2:
		return rewriteValueAMD64_OpMaskedXorInt64x2(v)
	case OpMaskedXorInt64x4:
		return rewriteValueAMD64_OpMaskedXorInt64x4(v)
	case OpMaskedXorInt64x8:
		return rewriteValueAMD64_OpMaskedXorInt64x8(v)
	case OpMaskedXorUint32x16:
		return rewriteValueAMD64_OpMaskedXorUint32x16(v)
	case OpMaskedXorUint32x4:
		return rewriteValueAMD64_OpMaskedXorUint32x4(v)
	case OpMaskedXorUint32x8:
		return rewriteValueAMD64_OpMaskedXorUint32x8(v)
	case OpMaskedXorUint64x2:
		return rewriteValueAMD64_OpMaskedXorUint64x2(v)
	case OpMaskedXorUint64x4:
		return rewriteValueAMD64_OpMaskedXorUint64x4(v)
	case OpMaskedXorUint64x8:
		return rewriteValueAMD64_OpMaskedXorUint64x8(v)
	case OpMax32F:
		return rewriteValueAMD64_OpMax32F(v)
	case OpMax64F:
		return rewriteValueAMD64_OpMax64F(v)
	case OpMaxFloat32x16:
		v.Op = OpAMD64VMAXPS512
		return true
	case OpMaxFloat32x4:
		v.Op = OpAMD64VMAXPS128
		return true
	case OpMaxFloat32x8:
		v.Op = OpAMD64VMAXPS256
		return true
	case OpMaxFloat64x2:
		v.Op = OpAMD64VMAXPD128
		return true
	case OpMaxFloat64x4:
		v.Op = OpAMD64VMAXPD256
		return true
	case OpMaxFloat64x8:
		v.Op = OpAMD64VMAXPD512
		return true
	case OpMaxInt16x16:
		v.Op = OpAMD64VPMAXSW256
		return true
	case OpMaxInt16x32:
		v.Op = OpAMD64VPMAXSW512
		return true
	case OpMaxInt16x8:
		v.Op = OpAMD64VPMAXSW128
		return true
	case OpMaxInt32x16:
		v.Op = OpAMD64VPMAXSD512
		return true
	case OpMaxInt32x4:
		v.Op = OpAMD64VPMAXSD128
		return true
	case OpMaxInt32x8:
		v.Op = OpAMD64VPMAXSD256
		return true
	case OpMaxInt64x2:
		v.Op = OpAMD64VPMAXSQ128
		return true
	case OpMaxInt64x4:
		v.Op = OpAMD64VPMAXSQ256
		return true
	case OpMaxInt64x8:
		v.Op = OpAMD64VPMAXSQ512
		return true
	case OpMaxInt8x16:
		v.Op = OpAMD64VPMAXSB128
		return true
	case OpMaxInt8x32:
		v.Op = OpAMD64VPMAXSB256
		return true
	case OpMaxInt8x64:
		v.Op = OpAMD64VPMAXSB512
		return true
	case OpMaxUint16x16:
		v.Op = OpAMD64VPMAXUW256
		return true
	case OpMaxUint16x32:
		v.Op = OpAMD64VPMAXUW512
		return true
	case OpMaxUint16x8:
		v.Op = OpAMD64VPMAXUW128
		return true
	case OpMaxUint32x16:
		v.Op = OpAMD64VPMAXUD512
		return true
	case OpMaxUint32x4:
		v.Op = OpAMD64VPMAXUD128
		return true
	case OpMaxUint32x8:
		v.Op = OpAMD64VPMAXUD256
		return true
	case OpMaxUint64x2:
		v.Op = OpAMD64VPMAXUQ128
		return true
	case OpMaxUint64x4:
		v.Op = OpAMD64VPMAXUQ256
		return true
	case OpMaxUint64x8:
		v.Op = OpAMD64VPMAXUQ512
		return true
	case OpMaxUint8x16:
		v.Op = OpAMD64VPMAXUB128
		return true
	case OpMaxUint8x32:
		v.Op = OpAMD64VPMAXUB256
		return true
	case OpMaxUint8x64:
		v.Op = OpAMD64VPMAXUB512
		return true
	case OpMin32F:
		return rewriteValueAMD64_OpMin32F(v)
	case OpMin64F:
		return rewriteValueAMD64_OpMin64F(v)
	case OpMinFloat32x16:
		v.Op = OpAMD64VMINPS512
		return true
	case OpMinFloat32x4:
		v.Op = OpAMD64VMINPS128
		return true
	case OpMinFloat32x8:
		v.Op = OpAMD64VMINPS256
		return true
	case OpMinFloat64x2:
		v.Op = OpAMD64VMINPD128
		return true
	case OpMinFloat64x4:
		v.Op = OpAMD64VMINPD256
		return true
	case OpMinFloat64x8:
		v.Op = OpAMD64VMINPD512
		return true
	case OpMinInt16x16:
		v.Op = OpAMD64VPMINSW256
		return true
	case OpMinInt16x32:
		v.Op = OpAMD64VPMINSW512
		return true
	case OpMinInt16x8:
		v.Op = OpAMD64VPMINSW128
		return true
	case OpMinInt32x16:
		v.Op = OpAMD64VPMINSD512
		return true
	case OpMinInt32x4:
		v.Op = OpAMD64VPMINSD128
		return true
	case OpMinInt32x8:
		v.Op = OpAMD64VPMINSD256
		return true
	case OpMinInt64x2:
		v.Op = OpAMD64VPMINSQ128
		return true
	case OpMinInt64x4:
		v.Op = OpAMD64VPMINSQ256
		return true
	case OpMinInt64x8:
		v.Op = OpAMD64VPMINSQ512
		return true
	case OpMinInt8x16:
		v.Op = OpAMD64VPMINSB128
		return true
	case OpMinInt8x32:
		v.Op = OpAMD64VPMINSB256
		return true
	case OpMinInt8x64:
		v.Op = OpAMD64VPMINSB512
		return true
	case OpMinUint16x16:
		v.Op = OpAMD64VPMINUW256
		return true
	case OpMinUint16x32:
		v.Op = OpAMD64VPMINUW512
		return true
	case OpMinUint16x8:
		v.Op = OpAMD64VPMINUW128
		return true
	case OpMinUint32x16:
		v.Op = OpAMD64VPMINUD512
		return true
	case OpMinUint32x4:
		v.Op = OpAMD64VPMINUD128
		return true
	case OpMinUint32x8:
		v.Op = OpAMD64VPMINUD256
		return true
	case OpMinUint64x2:
		v.Op = OpAMD64VPMINUQ128
		return true
	case OpMinUint64x4:
		v.Op = OpAMD64VPMINUQ256
		return true
	case OpMinUint64x8:
		v.Op = OpAMD64VPMINUQ512
		return true
	case OpMinUint8x16:
		v.Op = OpAMD64VPMINUB128
		return true
	case OpMinUint8x32:
		v.Op = OpAMD64VPMINUB256
		return true
	case OpMinUint8x64:
		v.Op = OpAMD64VPMINUB512
		return true
	case OpMod16:
		return rewriteValueAMD64_OpMod16(v)
	case OpMod16u:
		return rewriteValueAMD64_OpMod16u(v)
	case OpMod32:
		return rewriteValueAMD64_OpMod32(v)
	case OpMod32u:
		return rewriteValueAMD64_OpMod32u(v)
	case OpMod64:
		return rewriteValueAMD64_OpMod64(v)
	case OpMod64u:
		return rewriteValueAMD64_OpMod64u(v)
	case OpMod8:
		return rewriteValueAMD64_OpMod8(v)
	case OpMod8u:
		return rewriteValueAMD64_OpMod8u(v)
	case OpMove:
		return rewriteValueAMD64_OpMove(v)
	case OpMul16:
		v.Op = OpAMD64MULL
		return true
	case OpMul32:
		v.Op = OpAMD64MULL
		return true
	case OpMul32F:
		v.Op = OpAMD64MULSS
		return true
	case OpMul64:
		v.Op = OpAMD64MULQ
		return true
	case OpMul64F:
		v.Op = OpAMD64MULSD
		return true
	case OpMul64uhilo:
		v.Op = OpAMD64MULQU2
		return true
	case OpMul8:
		v.Op = OpAMD64MULL
		return true
	case OpMulByPowOf2Float32x16:
		v.Op = OpAMD64VSCALEFPS512
		return true
	case OpMulByPowOf2Float32x4:
		v.Op = OpAMD64VSCALEFPS128
		return true
	case OpMulByPowOf2Float32x8:
		v.Op = OpAMD64VSCALEFPS256
		return true
	case OpMulByPowOf2Float64x2:
		v.Op = OpAMD64VSCALEFPD128
		return true
	case OpMulByPowOf2Float64x4:
		v.Op = OpAMD64VSCALEFPD256
		return true
	case OpMulByPowOf2Float64x8:
		v.Op = OpAMD64VSCALEFPD512
		return true
	case OpMulEvenWidenInt32x4:
		v.Op = OpAMD64VPMULDQ128
		return true
	case OpMulEvenWidenInt32x8:
		v.Op = OpAMD64VPMULDQ256
		return true
	case OpMulEvenWidenInt64x2:
		v.Op = OpAMD64VPMULDQ128
		return true
	case OpMulEvenWidenInt64x4:
		v.Op = OpAMD64VPMULDQ256
		return true
	case OpMulEvenWidenInt64x8:
		v.Op = OpAMD64VPMULDQ512
		return true
	case OpMulEvenWidenUint32x4:
		v.Op = OpAMD64VPMULUDQ128
		return true
	case OpMulEvenWidenUint32x8:
		v.Op = OpAMD64VPMULUDQ256
		return true
	case OpMulEvenWidenUint64x2:
		v.Op = OpAMD64VPMULUDQ128
		return true
	case OpMulEvenWidenUint64x4:
		v.Op = OpAMD64VPMULUDQ256
		return true
	case OpMulEvenWidenUint64x8:
		v.Op = OpAMD64VPMULUDQ512
		return true
	case OpMulFloat32x16:
		v.Op = OpAMD64VMULPS512
		return true
	case OpMulFloat32x4:
		v.Op = OpAMD64VMULPS128
		return true
	case OpMulFloat32x8:
		v.Op = OpAMD64VMULPS256
		return true
	case OpMulFloat64x2:
		v.Op = OpAMD64VMULPD128
		return true
	case OpMulFloat64x4:
		v.Op = OpAMD64VMULPD256
		return true
	case OpMulFloat64x8:
		v.Op = OpAMD64VMULPD512
		return true
	case OpMulHighInt16x16:
		v.Op = OpAMD64VPMULHW256
		return true
	case OpMulHighInt16x32:
		v.Op = OpAMD64VPMULHW512
		return true
	case OpMulHighInt16x8:
		v.Op = OpAMD64VPMULHW128
		return true
	case OpMulHighUint16x16:
		v.Op = OpAMD64VPMULHUW256
		return true
	case OpMulHighUint16x32:
		v.Op = OpAMD64VPMULHUW512
		return true
	case OpMulHighUint16x8:
		v.Op = OpAMD64VPMULHUW128
		return true
	case OpMulLowInt16x16:
		v.Op = OpAMD64VPMULLW256
		return true
	case OpMulLowInt16x32:
		v.Op = OpAMD64VPMULLW512
		return true
	case OpMulLowInt16x8:
		v.Op = OpAMD64VPMULLW128
		return true
	case OpMulLowInt32x16:
		v.Op = OpAMD64VPMULLD512
		return true
	case OpMulLowInt32x4:
		v.Op = OpAMD64VPMULLD128
		return true
	case OpMulLowInt32x8:
		v.Op = OpAMD64VPMULLD256
		return true
	case OpMulLowInt64x2:
		v.Op = OpAMD64VPMULLQ128
		return true
	case OpMulLowInt64x4:
		v.Op = OpAMD64VPMULLQ256
		return true
	case OpMulLowInt64x8:
		v.Op = OpAMD64VPMULLQ512
		return true
	case OpNeg16:
		v.Op = OpAMD64NEGL
		return true
	case OpNeg32:
		v.Op = OpAMD64NEGL
		return true
	case OpNeg32F:
		return rewriteValueAMD64_OpNeg32F(v)
	case OpNeg64:
		v.Op = OpAMD64NEGQ
		return true
	case OpNeg64F:
		return rewriteValueAMD64_OpNeg64F(v)
	case OpNeg8:
		v.Op = OpAMD64NEGL
		return true
	case OpNeq16:
		return rewriteValueAMD64_OpNeq16(v)
	case OpNeq32:
		return rewriteValueAMD64_OpNeq32(v)
	case OpNeq32F:
		return rewriteValueAMD64_OpNeq32F(v)
	case OpNeq64:
		return rewriteValueAMD64_OpNeq64(v)
	case OpNeq64F:
		return rewriteValueAMD64_OpNeq64F(v)
	case OpNeq8:
		return rewriteValueAMD64_OpNeq8(v)
	case OpNeqB:
		return rewriteValueAMD64_OpNeqB(v)
	case OpNeqPtr:
		return rewriteValueAMD64_OpNeqPtr(v)
	case OpNilCheck:
		v.Op = OpAMD64LoweredNilCheck
		return true
	case OpNot:
		return rewriteValueAMD64_OpNot(v)
	case OpNotEqualFloat32x16:
		return rewriteValueAMD64_OpNotEqualFloat32x16(v)
	case OpNotEqualFloat32x4:
		return rewriteValueAMD64_OpNotEqualFloat32x4(v)
	case OpNotEqualFloat32x8:
		return rewriteValueAMD64_OpNotEqualFloat32x8(v)
	case OpNotEqualFloat64x2:
		return rewriteValueAMD64_OpNotEqualFloat64x2(v)
	case OpNotEqualFloat64x4:
		return rewriteValueAMD64_OpNotEqualFloat64x4(v)
	case OpNotEqualFloat64x8:
		return rewriteValueAMD64_OpNotEqualFloat64x8(v)
	case OpNotEqualInt16x16:
		return rewriteValueAMD64_OpNotEqualInt16x16(v)
	case OpNotEqualInt16x32:
		return rewriteValueAMD64_OpNotEqualInt16x32(v)
	case OpNotEqualInt16x8:
		return rewriteValueAMD64_OpNotEqualInt16x8(v)
	case OpNotEqualInt32x16:
		return rewriteValueAMD64_OpNotEqualInt32x16(v)
	case OpNotEqualInt32x4:
		return rewriteValueAMD64_OpNotEqualInt32x4(v)
	case OpNotEqualInt32x8:
		return rewriteValueAMD64_OpNotEqualInt32x8(v)
	case OpNotEqualInt64x2:
		return rewriteValueAMD64_OpNotEqualInt64x2(v)
	case OpNotEqualInt64x4:
		return rewriteValueAMD64_OpNotEqualInt64x4(v)
	case OpNotEqualInt64x8:
		return rewriteValueAMD64_OpNotEqualInt64x8(v)
	case OpNotEqualInt8x16:
		return rewriteValueAMD64_OpNotEqualInt8x16(v)
	case OpNotEqualInt8x32:
		return rewriteValueAMD64_OpNotEqualInt8x32(v)
	case OpNotEqualInt8x64:
		return rewriteValueAMD64_OpNotEqualInt8x64(v)
	case OpNotEqualUint16x16:
		return rewriteValueAMD64_OpNotEqualUint16x16(v)
	case OpNotEqualUint16x32:
		return rewriteValueAMD64_OpNotEqualUint16x32(v)
	case OpNotEqualUint16x8:
		return rewriteValueAMD64_OpNotEqualUint16x8(v)
	case OpNotEqualUint32x16:
		return rewriteValueAMD64_OpNotEqualUint32x16(v)
	case OpNotEqualUint32x4:
		return rewriteValueAMD64_OpNotEqualUint32x4(v)
	case OpNotEqualUint32x8:
		return rewriteValueAMD64_OpNotEqualUint32x8(v)
	case OpNotEqualUint64x2:
		return rewriteValueAMD64_OpNotEqualUint64x2(v)
	case OpNotEqualUint64x4:
		return rewriteValueAMD64_OpNotEqualUint64x4(v)
	case OpNotEqualUint64x8:
		return rewriteValueAMD64_OpNotEqualUint64x8(v)
	case OpNotEqualUint8x16:
		return rewriteValueAMD64_OpNotEqualUint8x16(v)
	case OpNotEqualUint8x32:
		return rewriteValueAMD64_OpNotEqualUint8x32(v)
	case OpNotEqualUint8x64:
		return rewriteValueAMD64_OpNotEqualUint8x64(v)
	case OpOffPtr:
		return rewriteValueAMD64_OpOffPtr(v)
	case OpOr16:
		v.Op = OpAMD64ORL
		return true
	case OpOr32:
		v.Op = OpAMD64ORL
		return true
	case OpOr64:
		v.Op = OpAMD64ORQ
		return true
	case OpOr8:
		v.Op = OpAMD64ORL
		return true
	case OpOrB:
		v.Op = OpAMD64ORL
		return true
	case OpOrFloat32x16:
		v.Op = OpAMD64VORPS512
		return true
	case OpOrFloat32x4:
		v.Op = OpAMD64VORPS128
		return true
	case OpOrFloat32x8:
		v.Op = OpAMD64VORPS256
		return true
	case OpOrFloat64x2:
		v.Op = OpAMD64VORPD128
		return true
	case OpOrFloat64x4:
		v.Op = OpAMD64VORPD256
		return true
	case OpOrFloat64x8:
		v.Op = OpAMD64VORPD512
		return true
	case OpOrInt16x16:
		v.Op = OpAMD64VPOR256
		return true
	case OpOrInt16x8:
		v.Op = OpAMD64VPOR128
		return true
	case OpOrInt32x16:
		v.Op = OpAMD64VPORD512
		return true
	case OpOrInt32x4:
		v.Op = OpAMD64VPOR128
		return true
	case OpOrInt32x8:
		v.Op = OpAMD64VPOR256
		return true
	case OpOrInt64x2:
		v.Op = OpAMD64VPOR128
		return true
	case OpOrInt64x4:
		v.Op = OpAMD64VPOR256
		return true
	case OpOrInt64x8:
		v.Op = OpAMD64VPORQ512
		return true
	case OpOrInt8x16:
		v.Op = OpAMD64VPOR128
		return true
	case OpOrInt8x32:
		v.Op = OpAMD64VPOR256
		return true
	case OpOrUint16x16:
		v.Op = OpAMD64VPOR256
		return true
	case OpOrUint16x8:
		v.Op = OpAMD64VPOR128
		return true
	case OpOrUint32x16:
		v.Op = OpAMD64VPORD512
		return true
	case OpOrUint32x4:
		v.Op = OpAMD64VPOR128
		return true
	case OpOrUint32x8:
		v.Op = OpAMD64VPOR256
		return true
	case OpOrUint64x2:
		v.Op = OpAMD64VPOR128
		return true
	case OpOrUint64x4:
		v.Op = OpAMD64VPOR256
		return true
	case OpOrUint64x8:
		v.Op = OpAMD64VPORQ512
		return true
	case OpOrUint8x16:
		v.Op = OpAMD64VPOR128
		return true
	case OpOrUint8x32:
		v.Op = OpAMD64VPOR256
		return true
	case OpPairDotProdAccumulateInt32x16:
		v.Op = OpAMD64VPDPWSSD512
		return true
	case OpPairDotProdAccumulateInt32x4:
		v.Op = OpAMD64VPDPWSSD128
		return true
	case OpPairDotProdAccumulateInt32x8:
		v.Op = OpAMD64VPDPWSSD256
		return true
	case OpPairDotProdInt16x16:
		v.Op = OpAMD64VPMADDWD256
		return true
	case OpPairDotProdInt16x32:
		v.Op = OpAMD64VPMADDWD512
		return true
	case OpPairDotProdInt16x8:
		v.Op = OpAMD64VPMADDWD128
		return true
	case OpPairwiseAddFloat32x4:
		v.Op = OpAMD64VHADDPS128
		return true
	case OpPairwiseAddFloat32x8:
		v.Op = OpAMD64VHADDPS256
		return true
	case OpPairwiseAddFloat64x2:
		v.Op = OpAMD64VHADDPD128
		return true
	case OpPairwiseAddFloat64x4:
		v.Op = OpAMD64VHADDPD256
		return true
	case OpPairwiseAddInt16x16:
		v.Op = OpAMD64VPHADDW256
		return true
	case OpPairwiseAddInt16x8:
		v.Op = OpAMD64VPHADDW128
		return true
	case OpPairwiseAddInt32x4:
		v.Op = OpAMD64VPHADDD128
		return true
	case OpPairwiseAddInt32x8:
		v.Op = OpAMD64VPHADDD256
		return true
	case OpPairwiseAddUint16x16:
		v.Op = OpAMD64VPHADDW256
		return true
	case OpPairwiseAddUint16x8:
		v.Op = OpAMD64VPHADDW128
		return true
	case OpPairwiseAddUint32x4:
		v.Op = OpAMD64VPHADDD128
		return true
	case OpPairwiseAddUint32x8:
		v.Op = OpAMD64VPHADDD256
		return true
	case OpPairwiseSubFloat32x4:
		v.Op = OpAMD64VHSUBPS128
		return true
	case OpPairwiseSubFloat32x8:
		v.Op = OpAMD64VHSUBPS256
		return true
	case OpPairwiseSubFloat64x2:
		v.Op = OpAMD64VHSUBPD128
		return true
	case OpPairwiseSubFloat64x4:
		v.Op = OpAMD64VHSUBPD256
		return true
	case OpPairwiseSubInt16x16:
		v.Op = OpAMD64VPHSUBW256
		return true
	case OpPairwiseSubInt16x8:
		v.Op = OpAMD64VPHSUBW128
		return true
	case OpPairwiseSubInt32x4:
		v.Op = OpAMD64VPHSUBD128
		return true
	case OpPairwiseSubInt32x8:
		v.Op = OpAMD64VPHSUBD256
		return true
	case OpPairwiseSubUint16x16:
		v.Op = OpAMD64VPHSUBW256
		return true
	case OpPairwiseSubUint16x8:
		v.Op = OpAMD64VPHSUBW128
		return true
	case OpPairwiseSubUint32x4:
		v.Op = OpAMD64VPHSUBD128
		return true
	case OpPairwiseSubUint32x8:
		v.Op = OpAMD64VPHSUBD256
		return true
	case OpPanicBounds:
		return rewriteValueAMD64_OpPanicBounds(v)
	case OpPopCount16:
		return rewriteValueAMD64_OpPopCount16(v)
	case OpPopCount32:
		v.Op = OpAMD64POPCNTL
		return true
	case OpPopCount64:
		v.Op = OpAMD64POPCNTQ
		return true
	case OpPopCount8:
		return rewriteValueAMD64_OpPopCount8(v)
	case OpPopCountInt16x16:
		v.Op = OpAMD64VPOPCNTW256
		return true
	case OpPopCountInt16x32:
		v.Op = OpAMD64VPOPCNTW512
		return true
	case OpPopCountInt16x8:
		v.Op = OpAMD64VPOPCNTW128
		return true
	case OpPopCountInt32x16:
		v.Op = OpAMD64VPOPCNTD512
		return true
	case OpPopCountInt32x4:
		v.Op = OpAMD64VPOPCNTD128
		return true
	case OpPopCountInt32x8:
		v.Op = OpAMD64VPOPCNTD256
		return true
	case OpPopCountInt64x2:
		v.Op = OpAMD64VPOPCNTQ128
		return true
	case OpPopCountInt64x4:
		v.Op = OpAMD64VPOPCNTQ256
		return true
	case OpPopCountInt64x8:
		v.Op = OpAMD64VPOPCNTQ512
		return true
	case OpPopCountInt8x16:
		v.Op = OpAMD64VPOPCNTB128
		return true
	case OpPopCountInt8x32:
		v.Op = OpAMD64VPOPCNTB256
		return true
	case OpPopCountInt8x64:
		v.Op = OpAMD64VPOPCNTB512
		return true
	case OpPopCountUint16x16:
		v.Op = OpAMD64VPOPCNTW256
		return true
	case OpPopCountUint16x32:
		v.Op = OpAMD64VPOPCNTW512
		return true
	case OpPopCountUint16x8:
		v.Op = OpAMD64VPOPCNTW128
		return true
	case OpPopCountUint32x16:
		v.Op = OpAMD64VPOPCNTD512
		return true
	case OpPopCountUint32x4:
		v.Op = OpAMD64VPOPCNTD128
		return true
	case OpPopCountUint32x8:
		v.Op = OpAMD64VPOPCNTD256
		return true
	case OpPopCountUint64x2:
		v.Op = OpAMD64VPOPCNTQ128
		return true
	case OpPopCountUint64x4:
		v.Op = OpAMD64VPOPCNTQ256
		return true
	case OpPopCountUint64x8:
		v.Op = OpAMD64VPOPCNTQ512
		return true
	case OpPopCountUint8x16:
		v.Op = OpAMD64VPOPCNTB128
		return true
	case OpPopCountUint8x32:
		v.Op = OpAMD64VPOPCNTB256
		return true
	case OpPopCountUint8x64:
		v.Op = OpAMD64VPOPCNTB512
		return true
	case OpPrefetchCache:
		v.Op = OpAMD64PrefetchT0
		return true
	case OpPrefetchCacheStreamed:
		v.Op = OpAMD64PrefetchNTA
		return true
	case OpRotateAllLeftInt32x16:
		return rewriteValueAMD64_OpRotateAllLeftInt32x16(v)
	case OpRotateAllLeftInt32x4:
		return rewriteValueAMD64_OpRotateAllLeftInt32x4(v)
	case OpRotateAllLeftInt32x8:
		return rewriteValueAMD64_OpRotateAllLeftInt32x8(v)
	case OpRotateAllLeftInt64x2:
		return rewriteValueAMD64_OpRotateAllLeftInt64x2(v)
	case OpRotateAllLeftInt64x4:
		return rewriteValueAMD64_OpRotateAllLeftInt64x4(v)
	case OpRotateAllLeftInt64x8:
		return rewriteValueAMD64_OpRotateAllLeftInt64x8(v)
	case OpRotateAllLeftUint32x16:
		return rewriteValueAMD64_OpRotateAllLeftUint32x16(v)
	case OpRotateAllLeftUint32x4:
		return rewriteValueAMD64_OpRotateAllLeftUint32x4(v)
	case OpRotateAllLeftUint32x8:
		return rewriteValueAMD64_OpRotateAllLeftUint32x8(v)
	case OpRotateAllLeftUint64x2:
		return rewriteValueAMD64_OpRotateAllLeftUint64x2(v)
	case OpRotateAllLeftUint64x4:
		return rewriteValueAMD64_OpRotateAllLeftUint64x4(v)
	case OpRotateAllLeftUint64x8:
		return rewriteValueAMD64_OpRotateAllLeftUint64x8(v)
	case OpRotateAllRightInt32x16:
		return rewriteValueAMD64_OpRotateAllRightInt32x16(v)
	case OpRotateAllRightInt32x4:
		return rewriteValueAMD64_OpRotateAllRightInt32x4(v)
	case OpRotateAllRightInt32x8:
		return rewriteValueAMD64_OpRotateAllRightInt32x8(v)
	case OpRotateAllRightInt64x2:
		return rewriteValueAMD64_OpRotateAllRightInt64x2(v)
	case OpRotateAllRightInt64x4:
		return rewriteValueAMD64_OpRotateAllRightInt64x4(v)
	case OpRotateAllRightInt64x8:
		return rewriteValueAMD64_OpRotateAllRightInt64x8(v)
	case OpRotateAllRightUint32x16:
		return rewriteValueAMD64_OpRotateAllRightUint32x16(v)
	case OpRotateAllRightUint32x4:
		return rewriteValueAMD64_OpRotateAllRightUint32x4(v)
	case OpRotateAllRightUint32x8:
		return rewriteValueAMD64_OpRotateAllRightUint32x8(v)
	case OpRotateAllRightUint64x2:
		return rewriteValueAMD64_OpRotateAllRightUint64x2(v)
	case OpRotateAllRightUint64x4:
		return rewriteValueAMD64_OpRotateAllRightUint64x4(v)
	case OpRotateAllRightUint64x8:
		return rewriteValueAMD64_OpRotateAllRightUint64x8(v)
	case OpRotateLeft16:
		v.Op = OpAMD64ROLW
		return true
	case OpRotateLeft32:
		v.Op = OpAMD64ROLL
		return true
	case OpRotateLeft64:
		v.Op = OpAMD64ROLQ
		return true
	case OpRotateLeft8:
		v.Op = OpAMD64ROLB
		return true
	case OpRotateLeftInt32x16:
		v.Op = OpAMD64VPROLVD512
		return true
	case OpRotateLeftInt32x4:
		v.Op = OpAMD64VPROLVD128
		return true
	case OpRotateLeftInt32x8:
		v.Op = OpAMD64VPROLVD256
		return true
	case OpRotateLeftInt64x2:
		v.Op = OpAMD64VPROLVQ128
		return true
	case OpRotateLeftInt64x4:
		v.Op = OpAMD64VPROLVQ256
		return true
	case OpRotateLeftInt64x8:
		v.Op = OpAMD64VPROLVQ512
		return true
	case OpRotateLeftUint32x16:
		v.Op = OpAMD64VPROLVD512
		return true
	case OpRotateLeftUint32x4:
		v.Op = OpAMD64VPROLVD128
		return true
	case OpRotateLeftUint32x8:
		v.Op = OpAMD64VPROLVD256
		return true
	case OpRotateLeftUint64x2:
		v.Op = OpAMD64VPROLVQ128
		return true
	case OpRotateLeftUint64x4:
		v.Op = OpAMD64VPROLVQ256
		return true
	case OpRotateLeftUint64x8:
		v.Op = OpAMD64VPROLVQ512
		return true
	case OpRotateRightInt32x16:
		v.Op = OpAMD64VPRORVD512
		return true
	case OpRotateRightInt32x4:
		v.Op = OpAMD64VPRORVD128
		return true
	case OpRotateRightInt32x8:
		v.Op = OpAMD64VPRORVD256
		return true
	case OpRotateRightInt64x2:
		v.Op = OpAMD64VPRORVQ128
		return true
	case OpRotateRightInt64x4:
		v.Op = OpAMD64VPRORVQ256
		return true
	case OpRotateRightInt64x8:
		v.Op = OpAMD64VPRORVQ512
		return true
	case OpRotateRightUint32x16:
		v.Op = OpAMD64VPRORVD512
		return true
	case OpRotateRightUint32x4:
		v.Op = OpAMD64VPRORVD128
		return true
	case OpRotateRightUint32x8:
		v.Op = OpAMD64VPRORVD256
		return true
	case OpRotateRightUint64x2:
		v.Op = OpAMD64VPRORVQ128
		return true
	case OpRotateRightUint64x4:
		v.Op = OpAMD64VPRORVQ256
		return true
	case OpRotateRightUint64x8:
		v.Op = OpAMD64VPRORVQ512
		return true
	case OpRound32F:
		v.Op = OpAMD64LoweredRound32F
		return true
	case OpRound64F:
		v.Op = OpAMD64LoweredRound64F
		return true
	case OpRoundFloat32x4:
		return rewriteValueAMD64_OpRoundFloat32x4(v)
	case OpRoundFloat32x8:
		return rewriteValueAMD64_OpRoundFloat32x8(v)
	case OpRoundFloat64x2:
		return rewriteValueAMD64_OpRoundFloat64x2(v)
	case OpRoundFloat64x4:
		return rewriteValueAMD64_OpRoundFloat64x4(v)
	case OpRoundSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat32x16(v)
	case OpRoundSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat32x4(v)
	case OpRoundSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat32x8(v)
	case OpRoundSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat64x2(v)
	case OpRoundSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat64x4(v)
	case OpRoundSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat64x8(v)
	case OpRoundToEven:
		return rewriteValueAMD64_OpRoundToEven(v)
	case OpRoundWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpRoundWithPrecisionFloat32x16(v)
	case OpRoundWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpRoundWithPrecisionFloat32x4(v)
	case OpRoundWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpRoundWithPrecisionFloat32x8(v)
	case OpRoundWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpRoundWithPrecisionFloat64x2(v)
	case OpRoundWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpRoundWithPrecisionFloat64x4(v)
	case OpRoundWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpRoundWithPrecisionFloat64x8(v)
	case OpRsh16Ux16:
		return rewriteValueAMD64_OpRsh16Ux16(v)
	case OpRsh16Ux32:
		return rewriteValueAMD64_OpRsh16Ux32(v)
	case OpRsh16Ux64:
		return rewriteValueAMD64_OpRsh16Ux64(v)
	case OpRsh16Ux8:
		return rewriteValueAMD64_OpRsh16Ux8(v)
	case OpRsh16x16:
		return rewriteValueAMD64_OpRsh16x16(v)
	case OpRsh16x32:
		return rewriteValueAMD64_OpRsh16x32(v)
	case OpRsh16x64:
		return rewriteValueAMD64_OpRsh16x64(v)
	case OpRsh16x8:
		return rewriteValueAMD64_OpRsh16x8(v)
	case OpRsh32Ux16:
		return rewriteValueAMD64_OpRsh32Ux16(v)
	case OpRsh32Ux32:
		return rewriteValueAMD64_OpRsh32Ux32(v)
	case OpRsh32Ux64:
		return rewriteValueAMD64_OpRsh32Ux64(v)
	case OpRsh32Ux8:
		return rewriteValueAMD64_OpRsh32Ux8(v)
	case OpRsh32x16:
		return rewriteValueAMD64_OpRsh32x16(v)
	case OpRsh32x32:
		return rewriteValueAMD64_OpRsh32x32(v)
	case OpRsh32x64:
		return rewriteValueAMD64_OpRsh32x64(v)
	case OpRsh32x8:
		return rewriteValueAMD64_OpRsh32x8(v)
	case OpRsh64Ux16:
		return rewriteValueAMD64_OpRsh64Ux16(v)
	case OpRsh64Ux32:
		return rewriteValueAMD64_OpRsh64Ux32(v)
	case OpRsh64Ux64:
		return rewriteValueAMD64_OpRsh64Ux64(v)
	case OpRsh64Ux8:
		return rewriteValueAMD64_OpRsh64Ux8(v)
	case OpRsh64x16:
		return rewriteValueAMD64_OpRsh64x16(v)
	case OpRsh64x32:
		return rewriteValueAMD64_OpRsh64x32(v)
	case OpRsh64x64:
		return rewriteValueAMD64_OpRsh64x64(v)
	case OpRsh64x8:
		return rewriteValueAMD64_OpRsh64x8(v)
	case OpRsh8Ux16:
		return rewriteValueAMD64_OpRsh8Ux16(v)
	case OpRsh8Ux32:
		return rewriteValueAMD64_OpRsh8Ux32(v)
	case OpRsh8Ux64:
		return rewriteValueAMD64_OpRsh8Ux64(v)
	case OpRsh8Ux8:
		return rewriteValueAMD64_OpRsh8Ux8(v)
	case OpRsh8x16:
		return rewriteValueAMD64_OpRsh8x16(v)
	case OpRsh8x32:
		return rewriteValueAMD64_OpRsh8x32(v)
	case OpRsh8x64:
		return rewriteValueAMD64_OpRsh8x64(v)
	case OpRsh8x8:
		return rewriteValueAMD64_OpRsh8x8(v)
	case OpSaturatedAddInt16x16:
		v.Op = OpAMD64VPADDSW256
		return true
	case OpSaturatedAddInt16x32:
		v.Op = OpAMD64VPADDSW512
		return true
	case OpSaturatedAddInt16x8:
		v.Op = OpAMD64VPADDSW128
		return true
	case OpSaturatedAddInt8x16:
		v.Op = OpAMD64VPADDSB128
		return true
	case OpSaturatedAddInt8x32:
		v.Op = OpAMD64VPADDSB256
		return true
	case OpSaturatedAddInt8x64:
		v.Op = OpAMD64VPADDSB512
		return true
	case OpSaturatedAddUint16x16:
		v.Op = OpAMD64VPADDSW256
		return true
	case OpSaturatedAddUint16x32:
		v.Op = OpAMD64VPADDSW512
		return true
	case OpSaturatedAddUint16x8:
		v.Op = OpAMD64VPADDSW128
		return true
	case OpSaturatedAddUint8x16:
		v.Op = OpAMD64VPADDSB128
		return true
	case OpSaturatedAddUint8x32:
		v.Op = OpAMD64VPADDSB256
		return true
	case OpSaturatedAddUint8x64:
		v.Op = OpAMD64VPADDSB512
		return true
	case OpSaturatedPairDotProdAccumulateInt32x16:
		v.Op = OpAMD64VPDPWSSDS512
		return true
	case OpSaturatedPairDotProdAccumulateInt32x4:
		v.Op = OpAMD64VPDPWSSDS128
		return true
	case OpSaturatedPairDotProdAccumulateInt32x8:
		v.Op = OpAMD64VPDPWSSDS256
		return true
	case OpSaturatedPairwiseAddInt16x16:
		v.Op = OpAMD64VPHADDSW256
		return true
	case OpSaturatedPairwiseAddInt16x8:
		v.Op = OpAMD64VPHADDSW128
		return true
	case OpSaturatedPairwiseSubInt16x16:
		v.Op = OpAMD64VPHSUBSW256
		return true
	case OpSaturatedPairwiseSubInt16x8:
		v.Op = OpAMD64VPHSUBSW128
		return true
	case OpSaturatedSubInt16x16:
		v.Op = OpAMD64VPSUBSW256
		return true
	case OpSaturatedSubInt16x32:
		v.Op = OpAMD64VPSUBSW512
		return true
	case OpSaturatedSubInt16x8:
		v.Op = OpAMD64VPSUBSW128
		return true
	case OpSaturatedSubInt8x16:
		v.Op = OpAMD64VPSUBSB128
		return true
	case OpSaturatedSubInt8x32:
		v.Op = OpAMD64VPSUBSB256
		return true
	case OpSaturatedSubInt8x64:
		v.Op = OpAMD64VPSUBSB512
		return true
	case OpSaturatedSubUint16x16:
		v.Op = OpAMD64VPSUBSW256
		return true
	case OpSaturatedSubUint16x32:
		v.Op = OpAMD64VPSUBSW512
		return true
	case OpSaturatedSubUint16x8:
		v.Op = OpAMD64VPSUBSW128
		return true
	case OpSaturatedSubUint8x16:
		v.Op = OpAMD64VPSUBSB128
		return true
	case OpSaturatedSubUint8x32:
		v.Op = OpAMD64VPSUBSB256
		return true
	case OpSaturatedSubUint8x64:
		v.Op = OpAMD64VPSUBSB512
		return true
	case OpSaturatedUnsignedSignedPairDotProdUint8x16:
		v.Op = OpAMD64VPMADDUBSW128
		return true
	case OpSaturatedUnsignedSignedPairDotProdUint8x32:
		v.Op = OpAMD64VPMADDUBSW256
		return true
	case OpSaturatedUnsignedSignedPairDotProdUint8x64:
		v.Op = OpAMD64VPMADDUBSW512
		return true
	case OpSaturatedUnsignedSignedQuadDotProdAccumulateInt32x16:
		v.Op = OpAMD64VPDPBUSDS512
		return true
	case OpSaturatedUnsignedSignedQuadDotProdAccumulateInt32x4:
		v.Op = OpAMD64VPDPBUSDS128
		return true
	case OpSaturatedUnsignedSignedQuadDotProdAccumulateInt32x8:
		v.Op = OpAMD64VPDPBUSDS256
		return true
	case OpSaturatedUnsignedSignedQuadDotProdAccumulateUint32x16:
		v.Op = OpAMD64VPDPBUSDS512
		return true
	case OpSaturatedUnsignedSignedQuadDotProdAccumulateUint32x4:
		v.Op = OpAMD64VPDPBUSDS128
		return true
	case OpSaturatedUnsignedSignedQuadDotProdAccumulateUint32x8:
		v.Op = OpAMD64VPDPBUSDS256
		return true
	case OpSelect0:
		return rewriteValueAMD64_OpSelect0(v)
	case OpSelect1:
		return rewriteValueAMD64_OpSelect1(v)
	case OpSelectN:
		return rewriteValueAMD64_OpSelectN(v)
	case OpSetElemInt16x8:
		return rewriteValueAMD64_OpSetElemInt16x8(v)
	case OpSetElemInt32x4:
		return rewriteValueAMD64_OpSetElemInt32x4(v)
	case OpSetElemInt64x2:
		return rewriteValueAMD64_OpSetElemInt64x2(v)
	case OpSetElemInt8x16:
		return rewriteValueAMD64_OpSetElemInt8x16(v)
	case OpSetElemUint16x8:
		return rewriteValueAMD64_OpSetElemUint16x8(v)
	case OpSetElemUint32x4:
		return rewriteValueAMD64_OpSetElemUint32x4(v)
	case OpSetElemUint64x2:
		return rewriteValueAMD64_OpSetElemUint64x2(v)
	case OpSetElemUint8x16:
		return rewriteValueAMD64_OpSetElemUint8x16(v)
	case OpShiftAllLeftAndFillUpperFromInt16x16:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt16x16(v)
	case OpShiftAllLeftAndFillUpperFromInt16x32:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt16x32(v)
	case OpShiftAllLeftAndFillUpperFromInt16x8:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt16x8(v)
	case OpShiftAllLeftAndFillUpperFromInt32x16:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt32x16(v)
	case OpShiftAllLeftAndFillUpperFromInt32x4:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt32x4(v)
	case OpShiftAllLeftAndFillUpperFromInt32x8:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt32x8(v)
	case OpShiftAllLeftAndFillUpperFromInt64x2:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt64x2(v)
	case OpShiftAllLeftAndFillUpperFromInt64x4:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt64x4(v)
	case OpShiftAllLeftAndFillUpperFromInt64x8:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt64x8(v)
	case OpShiftAllLeftAndFillUpperFromUint16x16:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint16x16(v)
	case OpShiftAllLeftAndFillUpperFromUint16x32:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint16x32(v)
	case OpShiftAllLeftAndFillUpperFromUint16x8:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint16x8(v)
	case OpShiftAllLeftAndFillUpperFromUint32x16:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint32x16(v)
	case OpShiftAllLeftAndFillUpperFromUint32x4:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint32x4(v)
	case OpShiftAllLeftAndFillUpperFromUint32x8:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint32x8(v)
	case OpShiftAllLeftAndFillUpperFromUint64x2:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint64x2(v)
	case OpShiftAllLeftAndFillUpperFromUint64x4:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint64x4(v)
	case OpShiftAllLeftAndFillUpperFromUint64x8:
		return rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint64x8(v)
	case OpShiftAllLeftInt16x16:
		v.Op = OpAMD64VPSLLW256
		return true
	case OpShiftAllLeftInt16x8:
		v.Op = OpAMD64VPSLLW128
		return true
	case OpShiftAllLeftInt32x4:
		v.Op = OpAMD64VPSLLD128
		return true
	case OpShiftAllLeftInt32x8:
		v.Op = OpAMD64VPSLLD256
		return true
	case OpShiftAllLeftInt64x2:
		v.Op = OpAMD64VPSLLQ128
		return true
	case OpShiftAllLeftInt64x4:
		v.Op = OpAMD64VPSLLQ256
		return true
	case OpShiftAllLeftInt64x8:
		v.Op = OpAMD64VPSLLQ512
		return true
	case OpShiftAllLeftUint16x16:
		v.Op = OpAMD64VPSLLW256
		return true
	case OpShiftAllLeftUint16x8:
		v.Op = OpAMD64VPSLLW128
		return true
	case OpShiftAllLeftUint32x4:
		v.Op = OpAMD64VPSLLD128
		return true
	case OpShiftAllLeftUint32x8:
		v.Op = OpAMD64VPSLLD256
		return true
	case OpShiftAllLeftUint64x2:
		v.Op = OpAMD64VPSLLQ128
		return true
	case OpShiftAllLeftUint64x4:
		v.Op = OpAMD64VPSLLQ256
		return true
	case OpShiftAllLeftUint64x8:
		v.Op = OpAMD64VPSLLQ512
		return true
	case OpShiftAllRightAndFillUpperFromInt16x16:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt16x16(v)
	case OpShiftAllRightAndFillUpperFromInt16x32:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt16x32(v)
	case OpShiftAllRightAndFillUpperFromInt16x8:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt16x8(v)
	case OpShiftAllRightAndFillUpperFromInt32x16:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt32x16(v)
	case OpShiftAllRightAndFillUpperFromInt32x4:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt32x4(v)
	case OpShiftAllRightAndFillUpperFromInt32x8:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt32x8(v)
	case OpShiftAllRightAndFillUpperFromInt64x2:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt64x2(v)
	case OpShiftAllRightAndFillUpperFromInt64x4:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt64x4(v)
	case OpShiftAllRightAndFillUpperFromInt64x8:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt64x8(v)
	case OpShiftAllRightAndFillUpperFromUint16x16:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint16x16(v)
	case OpShiftAllRightAndFillUpperFromUint16x32:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint16x32(v)
	case OpShiftAllRightAndFillUpperFromUint16x8:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint16x8(v)
	case OpShiftAllRightAndFillUpperFromUint32x16:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint32x16(v)
	case OpShiftAllRightAndFillUpperFromUint32x4:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint32x4(v)
	case OpShiftAllRightAndFillUpperFromUint32x8:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint32x8(v)
	case OpShiftAllRightAndFillUpperFromUint64x2:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint64x2(v)
	case OpShiftAllRightAndFillUpperFromUint64x4:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint64x4(v)
	case OpShiftAllRightAndFillUpperFromUint64x8:
		return rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint64x8(v)
	case OpShiftAllRightInt16x16:
		v.Op = OpAMD64VPSRLW256
		return true
	case OpShiftAllRightInt16x8:
		v.Op = OpAMD64VPSRLW128
		return true
	case OpShiftAllRightInt32x4:
		v.Op = OpAMD64VPSRLD128
		return true
	case OpShiftAllRightInt32x8:
		v.Op = OpAMD64VPSRLD256
		return true
	case OpShiftAllRightInt64x2:
		v.Op = OpAMD64VPSRLQ128
		return true
	case OpShiftAllRightInt64x4:
		v.Op = OpAMD64VPSRLQ256
		return true
	case OpShiftAllRightInt64x8:
		v.Op = OpAMD64VPSRLQ512
		return true
	case OpShiftAllRightSignExtendedInt16x16:
		v.Op = OpAMD64VPSRAW256
		return true
	case OpShiftAllRightSignExtendedInt16x8:
		v.Op = OpAMD64VPSRAW128
		return true
	case OpShiftAllRightSignExtendedInt32x4:
		v.Op = OpAMD64VPSRAD128
		return true
	case OpShiftAllRightSignExtendedInt32x8:
		v.Op = OpAMD64VPSRAD256
		return true
	case OpShiftAllRightSignExtendedInt64x2:
		v.Op = OpAMD64VPSRAQ128
		return true
	case OpShiftAllRightSignExtendedInt64x4:
		v.Op = OpAMD64VPSRAQ256
		return true
	case OpShiftAllRightSignExtendedInt64x8:
		v.Op = OpAMD64VPSRAQ512
		return true
	case OpShiftAllRightUint16x16:
		v.Op = OpAMD64VPSRLW256
		return true
	case OpShiftAllRightUint16x8:
		v.Op = OpAMD64VPSRLW128
		return true
	case OpShiftAllRightUint32x4:
		v.Op = OpAMD64VPSRLD128
		return true
	case OpShiftAllRightUint32x8:
		v.Op = OpAMD64VPSRLD256
		return true
	case OpShiftAllRightUint64x2:
		v.Op = OpAMD64VPSRLQ128
		return true
	case OpShiftAllRightUint64x4:
		v.Op = OpAMD64VPSRLQ256
		return true
	case OpShiftAllRightUint64x8:
		v.Op = OpAMD64VPSRLQ512
		return true
	case OpShiftLeftAndFillUpperFromInt16x16:
		v.Op = OpAMD64VPSHLDVW256
		return true
	case OpShiftLeftAndFillUpperFromInt16x32:
		v.Op = OpAMD64VPSHLDVW512
		return true
	case OpShiftLeftAndFillUpperFromInt16x8:
		v.Op = OpAMD64VPSHLDVW128
		return true
	case OpShiftLeftAndFillUpperFromInt32x16:
		v.Op = OpAMD64VPSHLDVD512
		return true
	case OpShiftLeftAndFillUpperFromInt32x4:
		v.Op = OpAMD64VPSHLDVD128
		return true
	case OpShiftLeftAndFillUpperFromInt32x8:
		v.Op = OpAMD64VPSHLDVD256
		return true
	case OpShiftLeftAndFillUpperFromInt64x2:
		v.Op = OpAMD64VPSHLDVQ128
		return true
	case OpShiftLeftAndFillUpperFromInt64x4:
		v.Op = OpAMD64VPSHLDVQ256
		return true
	case OpShiftLeftAndFillUpperFromInt64x8:
		v.Op = OpAMD64VPSHLDVQ512
		return true
	case OpShiftLeftAndFillUpperFromUint16x16:
		v.Op = OpAMD64VPSHLDVW256
		return true
	case OpShiftLeftAndFillUpperFromUint16x32:
		v.Op = OpAMD64VPSHLDVW512
		return true
	case OpShiftLeftAndFillUpperFromUint16x8:
		v.Op = OpAMD64VPSHLDVW128
		return true
	case OpShiftLeftAndFillUpperFromUint32x16:
		v.Op = OpAMD64VPSHLDVD512
		return true
	case OpShiftLeftAndFillUpperFromUint32x4:
		v.Op = OpAMD64VPSHLDVD128
		return true
	case OpShiftLeftAndFillUpperFromUint32x8:
		v.Op = OpAMD64VPSHLDVD256
		return true
	case OpShiftLeftAndFillUpperFromUint64x2:
		v.Op = OpAMD64VPSHLDVQ128
		return true
	case OpShiftLeftAndFillUpperFromUint64x4:
		v.Op = OpAMD64VPSHLDVQ256
		return true
	case OpShiftLeftAndFillUpperFromUint64x8:
		v.Op = OpAMD64VPSHLDVQ512
		return true
	case OpShiftLeftInt16x16:
		v.Op = OpAMD64VPSLLVW256
		return true
	case OpShiftLeftInt16x32:
		v.Op = OpAMD64VPSLLVW512
		return true
	case OpShiftLeftInt16x8:
		v.Op = OpAMD64VPSLLVW128
		return true
	case OpShiftLeftInt32x16:
		v.Op = OpAMD64VPSLLVD512
		return true
	case OpShiftLeftInt32x4:
		v.Op = OpAMD64VPSLLVD128
		return true
	case OpShiftLeftInt32x8:
		v.Op = OpAMD64VPSLLVD256
		return true
	case OpShiftLeftInt64x2:
		v.Op = OpAMD64VPSLLVQ128
		return true
	case OpShiftLeftInt64x4:
		v.Op = OpAMD64VPSLLVQ256
		return true
	case OpShiftLeftInt64x8:
		v.Op = OpAMD64VPSLLVQ512
		return true
	case OpShiftLeftUint16x16:
		v.Op = OpAMD64VPSLLVW256
		return true
	case OpShiftLeftUint16x32:
		v.Op = OpAMD64VPSLLVW512
		return true
	case OpShiftLeftUint16x8:
		v.Op = OpAMD64VPSLLVW128
		return true
	case OpShiftLeftUint32x16:
		v.Op = OpAMD64VPSLLVD512
		return true
	case OpShiftLeftUint32x4:
		v.Op = OpAMD64VPSLLVD128
		return true
	case OpShiftLeftUint32x8:
		v.Op = OpAMD64VPSLLVD256
		return true
	case OpShiftLeftUint64x2:
		v.Op = OpAMD64VPSLLVQ128
		return true
	case OpShiftLeftUint64x4:
		v.Op = OpAMD64VPSLLVQ256
		return true
	case OpShiftLeftUint64x8:
		v.Op = OpAMD64VPSLLVQ512
		return true
	case OpShiftRightAndFillUpperFromInt16x16:
		v.Op = OpAMD64VPSHRDVW256
		return true
	case OpShiftRightAndFillUpperFromInt16x32:
		v.Op = OpAMD64VPSHRDVW512
		return true
	case OpShiftRightAndFillUpperFromInt16x8:
		v.Op = OpAMD64VPSHRDVW128
		return true
	case OpShiftRightAndFillUpperFromInt32x16:
		v.Op = OpAMD64VPSHRDVD512
		return true
	case OpShiftRightAndFillUpperFromInt32x4:
		v.Op = OpAMD64VPSHRDVD128
		return true
	case OpShiftRightAndFillUpperFromInt32x8:
		v.Op = OpAMD64VPSHRDVD256
		return true
	case OpShiftRightAndFillUpperFromInt64x2:
		v.Op = OpAMD64VPSHRDVQ128
		return true
	case OpShiftRightAndFillUpperFromInt64x4:
		v.Op = OpAMD64VPSHRDVQ256
		return true
	case OpShiftRightAndFillUpperFromInt64x8:
		v.Op = OpAMD64VPSHRDVQ512
		return true
	case OpShiftRightAndFillUpperFromUint16x16:
		v.Op = OpAMD64VPSHRDVW256
		return true
	case OpShiftRightAndFillUpperFromUint16x32:
		v.Op = OpAMD64VPSHRDVW512
		return true
	case OpShiftRightAndFillUpperFromUint16x8:
		v.Op = OpAMD64VPSHRDVW128
		return true
	case OpShiftRightAndFillUpperFromUint32x16:
		v.Op = OpAMD64VPSHRDVD512
		return true
	case OpShiftRightAndFillUpperFromUint32x4:
		v.Op = OpAMD64VPSHRDVD128
		return true
	case OpShiftRightAndFillUpperFromUint32x8:
		v.Op = OpAMD64VPSHRDVD256
		return true
	case OpShiftRightAndFillUpperFromUint64x2:
		v.Op = OpAMD64VPSHRDVQ128
		return true
	case OpShiftRightAndFillUpperFromUint64x4:
		v.Op = OpAMD64VPSHRDVQ256
		return true
	case OpShiftRightAndFillUpperFromUint64x8:
		v.Op = OpAMD64VPSHRDVQ512
		return true
	case OpShiftRightInt16x16:
		v.Op = OpAMD64VPSRLVW256
		return true
	case OpShiftRightInt16x32:
		v.Op = OpAMD64VPSRLVW512
		return true
	case OpShiftRightInt16x8:
		v.Op = OpAMD64VPSRLVW128
		return true
	case OpShiftRightInt32x16:
		v.Op = OpAMD64VPSRLVD512
		return true
	case OpShiftRightInt32x4:
		v.Op = OpAMD64VPSRLVD128
		return true
	case OpShiftRightInt32x8:
		v.Op = OpAMD64VPSRLVD256
		return true
	case OpShiftRightInt64x2:
		v.Op = OpAMD64VPSRLVQ128
		return true
	case OpShiftRightInt64x4:
		v.Op = OpAMD64VPSRLVQ256
		return true
	case OpShiftRightInt64x8:
		v.Op = OpAMD64VPSRLVQ512
		return true
	case OpShiftRightSignExtendedInt16x16:
		v.Op = OpAMD64VPSRAVW256
		return true
	case OpShiftRightSignExtendedInt16x32:
		v.Op = OpAMD64VPSRAVW512
		return true
	case OpShiftRightSignExtendedInt16x8:
		v.Op = OpAMD64VPSRAVW128
		return true
	case OpShiftRightSignExtendedInt32x16:
		v.Op = OpAMD64VPSRAVD512
		return true
	case OpShiftRightSignExtendedInt32x4:
		v.Op = OpAMD64VPSRAVD128
		return true
	case OpShiftRightSignExtendedInt32x8:
		v.Op = OpAMD64VPSRAVD256
		return true
	case OpShiftRightSignExtendedInt64x2:
		v.Op = OpAMD64VPSRAVQ128
		return true
	case OpShiftRightSignExtendedInt64x4:
		v.Op = OpAMD64VPSRAVQ256
		return true
	case OpShiftRightSignExtendedInt64x8:
		v.Op = OpAMD64VPSRAVQ512
		return true
	case OpShiftRightSignExtendedUint16x16:
		v.Op = OpAMD64VPSRAVW256
		return true
	case OpShiftRightSignExtendedUint16x32:
		v.Op = OpAMD64VPSRAVW512
		return true
	case OpShiftRightSignExtendedUint16x8:
		v.Op = OpAMD64VPSRAVW128
		return true
	case OpShiftRightSignExtendedUint32x16:
		v.Op = OpAMD64VPSRAVD512
		return true
	case OpShiftRightSignExtendedUint32x4:
		v.Op = OpAMD64VPSRAVD128
		return true
	case OpShiftRightSignExtendedUint32x8:
		v.Op = OpAMD64VPSRAVD256
		return true
	case OpShiftRightSignExtendedUint64x2:
		v.Op = OpAMD64VPSRAVQ128
		return true
	case OpShiftRightSignExtendedUint64x4:
		v.Op = OpAMD64VPSRAVQ256
		return true
	case OpShiftRightSignExtendedUint64x8:
		v.Op = OpAMD64VPSRAVQ512
		return true
	case OpShiftRightUint16x16:
		v.Op = OpAMD64VPSRLVW256
		return true
	case OpShiftRightUint16x32:
		v.Op = OpAMD64VPSRLVW512
		return true
	case OpShiftRightUint16x8:
		v.Op = OpAMD64VPSRLVW128
		return true
	case OpShiftRightUint32x16:
		v.Op = OpAMD64VPSRLVD512
		return true
	case OpShiftRightUint32x4:
		v.Op = OpAMD64VPSRLVD128
		return true
	case OpShiftRightUint32x8:
		v.Op = OpAMD64VPSRLVD256
		return true
	case OpShiftRightUint64x2:
		v.Op = OpAMD64VPSRLVQ128
		return true
	case OpShiftRightUint64x4:
		v.Op = OpAMD64VPSRLVQ256
		return true
	case OpShiftRightUint64x8:
		v.Op = OpAMD64VPSRLVQ512
		return true
	case OpSignExt16to32:
		v.Op = OpAMD64MOVWQSX
		return true
	case OpSignExt16to64:
		v.Op = OpAMD64MOVWQSX
		return true
	case OpSignExt32to64:
		v.Op = OpAMD64MOVLQSX
		return true
	case OpSignExt8to16:
		v.Op = OpAMD64MOVBQSX
		return true
	case OpSignExt8to32:
		v.Op = OpAMD64MOVBQSX
		return true
	case OpSignExt8to64:
		v.Op = OpAMD64MOVBQSX
		return true
	case OpSignInt16x16:
		v.Op = OpAMD64VPSIGNW256
		return true
	case OpSignInt16x8:
		v.Op = OpAMD64VPSIGNW128
		return true
	case OpSignInt32x4:
		v.Op = OpAMD64VPSIGND128
		return true
	case OpSignInt32x8:
		v.Op = OpAMD64VPSIGND256
		return true
	case OpSignInt8x16:
		v.Op = OpAMD64VPSIGNB128
		return true
	case OpSignInt8x32:
		v.Op = OpAMD64VPSIGNB256
		return true
	case OpSlicemask:
		return rewriteValueAMD64_OpSlicemask(v)
	case OpSpectreIndex:
		return rewriteValueAMD64_OpSpectreIndex(v)
	case OpSpectreSliceIndex:
		return rewriteValueAMD64_OpSpectreSliceIndex(v)
	case OpSqrt:
		v.Op = OpAMD64SQRTSD
		return true
	case OpSqrt32:
		v.Op = OpAMD64SQRTSS
		return true
	case OpSqrtFloat32x16:
		v.Op = OpAMD64VSQRTPS512
		return true
	case OpSqrtFloat32x4:
		v.Op = OpAMD64VSQRTPS128
		return true
	case OpSqrtFloat32x8:
		v.Op = OpAMD64VSQRTPS256
		return true
	case OpSqrtFloat64x2:
		v.Op = OpAMD64VSQRTPD128
		return true
	case OpSqrtFloat64x4:
		v.Op = OpAMD64VSQRTPD256
		return true
	case OpSqrtFloat64x8:
		v.Op = OpAMD64VSQRTPD512
		return true
	case OpStaticCall:
		v.Op = OpAMD64CALLstatic
		return true
	case OpStore:
		return rewriteValueAMD64_OpStore(v)
	case OpSub16:
		v.Op = OpAMD64SUBL
		return true
	case OpSub32:
		v.Op = OpAMD64SUBL
		return true
	case OpSub32F:
		v.Op = OpAMD64SUBSS
		return true
	case OpSub64:
		v.Op = OpAMD64SUBQ
		return true
	case OpSub64F:
		v.Op = OpAMD64SUBSD
		return true
	case OpSub8:
		v.Op = OpAMD64SUBL
		return true
	case OpSubFloat32x16:
		v.Op = OpAMD64VSUBPS512
		return true
	case OpSubFloat32x4:
		v.Op = OpAMD64VSUBPS128
		return true
	case OpSubFloat32x8:
		v.Op = OpAMD64VSUBPS256
		return true
	case OpSubFloat64x2:
		v.Op = OpAMD64VSUBPD128
		return true
	case OpSubFloat64x4:
		v.Op = OpAMD64VSUBPD256
		return true
	case OpSubFloat64x8:
		v.Op = OpAMD64VSUBPD512
		return true
	case OpSubInt16x16:
		v.Op = OpAMD64VPSUBW256
		return true
	case OpSubInt16x32:
		v.Op = OpAMD64VPSUBW512
		return true
	case OpSubInt16x8:
		v.Op = OpAMD64VPSUBW128
		return true
	case OpSubInt32x16:
		v.Op = OpAMD64VPSUBD512
		return true
	case OpSubInt32x4:
		v.Op = OpAMD64VPSUBD128
		return true
	case OpSubInt32x8:
		v.Op = OpAMD64VPSUBD256
		return true
	case OpSubInt64x2:
		v.Op = OpAMD64VPSUBQ128
		return true
	case OpSubInt64x4:
		v.Op = OpAMD64VPSUBQ256
		return true
	case OpSubInt64x8:
		v.Op = OpAMD64VPSUBQ512
		return true
	case OpSubInt8x16:
		v.Op = OpAMD64VPSUBB128
		return true
	case OpSubInt8x32:
		v.Op = OpAMD64VPSUBB256
		return true
	case OpSubInt8x64:
		v.Op = OpAMD64VPSUBB512
		return true
	case OpSubPtr:
		v.Op = OpAMD64SUBQ
		return true
	case OpSubUint16x16:
		v.Op = OpAMD64VPSUBW256
		return true
	case OpSubUint16x32:
		v.Op = OpAMD64VPSUBW512
		return true
	case OpSubUint16x8:
		v.Op = OpAMD64VPSUBW128
		return true
	case OpSubUint32x16:
		v.Op = OpAMD64VPSUBD512
		return true
	case OpSubUint32x4:
		v.Op = OpAMD64VPSUBD128
		return true
	case OpSubUint32x8:
		v.Op = OpAMD64VPSUBD256
		return true
	case OpSubUint64x2:
		v.Op = OpAMD64VPSUBQ128
		return true
	case OpSubUint64x4:
		v.Op = OpAMD64VPSUBQ256
		return true
	case OpSubUint64x8:
		v.Op = OpAMD64VPSUBQ512
		return true
	case OpSubUint8x16:
		v.Op = OpAMD64VPSUBB128
		return true
	case OpSubUint8x32:
		v.Op = OpAMD64VPSUBB256
		return true
	case OpSubUint8x64:
		v.Op = OpAMD64VPSUBB512
		return true
	case OpTailCall:
		v.Op = OpAMD64CALLtail
		return true
	case OpTrunc:
		return rewriteValueAMD64_OpTrunc(v)
	case OpTrunc16to8:
		v.Op = OpCopy
		return true
	case OpTrunc32to16:
		v.Op = OpCopy
		return true
	case OpTrunc32to8:
		v.Op = OpCopy
		return true
	case OpTrunc64to16:
		v.Op = OpCopy
		return true
	case OpTrunc64to32:
		v.Op = OpCopy
		return true
	case OpTrunc64to8:
		v.Op = OpCopy
		return true
	case OpTruncFloat32x4:
		return rewriteValueAMD64_OpTruncFloat32x4(v)
	case OpTruncFloat32x8:
		return rewriteValueAMD64_OpTruncFloat32x8(v)
	case OpTruncFloat64x2:
		return rewriteValueAMD64_OpTruncFloat64x2(v)
	case OpTruncFloat64x4:
		return rewriteValueAMD64_OpTruncFloat64x4(v)
	case OpTruncSuppressExceptionWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat32x16(v)
	case OpTruncSuppressExceptionWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat32x4(v)
	case OpTruncSuppressExceptionWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat32x8(v)
	case OpTruncSuppressExceptionWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat64x2(v)
	case OpTruncSuppressExceptionWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat64x4(v)
	case OpTruncSuppressExceptionWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat64x8(v)
	case OpTruncWithPrecisionFloat32x16:
		return rewriteValueAMD64_OpTruncWithPrecisionFloat32x16(v)
	case OpTruncWithPrecisionFloat32x4:
		return rewriteValueAMD64_OpTruncWithPrecisionFloat32x4(v)
	case OpTruncWithPrecisionFloat32x8:
		return rewriteValueAMD64_OpTruncWithPrecisionFloat32x8(v)
	case OpTruncWithPrecisionFloat64x2:
		return rewriteValueAMD64_OpTruncWithPrecisionFloat64x2(v)
	case OpTruncWithPrecisionFloat64x4:
		return rewriteValueAMD64_OpTruncWithPrecisionFloat64x4(v)
	case OpTruncWithPrecisionFloat64x8:
		return rewriteValueAMD64_OpTruncWithPrecisionFloat64x8(v)
	case OpUnsignedSignedQuadDotProdAccumulateInt32x16:
		v.Op = OpAMD64VPDPBUSD512
		return true
	case OpUnsignedSignedQuadDotProdAccumulateInt32x4:
		v.Op = OpAMD64VPDPBUSD128
		return true
	case OpUnsignedSignedQuadDotProdAccumulateInt32x8:
		v.Op = OpAMD64VPDPBUSD256
		return true
	case OpUnsignedSignedQuadDotProdAccumulateUint32x16:
		v.Op = OpAMD64VPDPBUSD512
		return true
	case OpUnsignedSignedQuadDotProdAccumulateUint32x4:
		v.Op = OpAMD64VPDPBUSD128
		return true
	case OpUnsignedSignedQuadDotProdAccumulateUint32x8:
		v.Op = OpAMD64VPDPBUSD256
		return true
	case OpWB:
		v.Op = OpAMD64LoweredWB
		return true
	case OpXor16:
		v.Op = OpAMD64XORL
		return true
	case OpXor32:
		v.Op = OpAMD64XORL
		return true
	case OpXor64:
		v.Op = OpAMD64XORQ
		return true
	case OpXor8:
		v.Op = OpAMD64XORL
		return true
	case OpXorFloat32x16:
		v.Op = OpAMD64VXORPS512
		return true
	case OpXorFloat32x4:
		v.Op = OpAMD64VXORPS128
		return true
	case OpXorFloat32x8:
		v.Op = OpAMD64VXORPS256
		return true
	case OpXorFloat64x2:
		v.Op = OpAMD64VXORPD128
		return true
	case OpXorFloat64x4:
		v.Op = OpAMD64VXORPD256
		return true
	case OpXorFloat64x8:
		v.Op = OpAMD64VXORPD512
		return true
	case OpXorInt16x16:
		v.Op = OpAMD64VPXOR256
		return true
	case OpXorInt16x8:
		v.Op = OpAMD64VPXOR128
		return true
	case OpXorInt32x16:
		v.Op = OpAMD64VPXORD512
		return true
	case OpXorInt32x4:
		v.Op = OpAMD64VPXOR128
		return true
	case OpXorInt32x8:
		v.Op = OpAMD64VPXOR256
		return true
	case OpXorInt64x2:
		v.Op = OpAMD64VPXOR128
		return true
	case OpXorInt64x4:
		v.Op = OpAMD64VPXOR256
		return true
	case OpXorInt64x8:
		v.Op = OpAMD64VPXORQ512
		return true
	case OpXorInt8x16:
		v.Op = OpAMD64VPXOR128
		return true
	case OpXorInt8x32:
		v.Op = OpAMD64VPXOR256
		return true
	case OpXorUint16x16:
		v.Op = OpAMD64VPXOR256
		return true
	case OpXorUint16x8:
		v.Op = OpAMD64VPXOR128
		return true
	case OpXorUint32x16:
		v.Op = OpAMD64VPXORD512
		return true
	case OpXorUint32x4:
		v.Op = OpAMD64VPXOR128
		return true
	case OpXorUint32x8:
		v.Op = OpAMD64VPXOR256
		return true
	case OpXorUint64x2:
		v.Op = OpAMD64VPXOR128
		return true
	case OpXorUint64x4:
		v.Op = OpAMD64VPXOR256
		return true
	case OpXorUint64x8:
		v.Op = OpAMD64VPXORQ512
		return true
	case OpXorUint8x16:
		v.Op = OpAMD64VPXOR128
		return true
	case OpXorUint8x32:
		v.Op = OpAMD64VPXOR256
		return true
	case OpZero:
		return rewriteValueAMD64_OpZero(v)
	case OpZeroExt16to32:
		v.Op = OpAMD64MOVWQZX
		return true
	case OpZeroExt16to64:
		v.Op = OpAMD64MOVWQZX
		return true
	case OpZeroExt32to64:
		v.Op = OpAMD64MOVLQZX
		return true
	case OpZeroExt8to16:
		v.Op = OpAMD64MOVBQZX
		return true
	case OpZeroExt8to32:
		v.Op = OpAMD64MOVBQZX
		return true
	case OpZeroExt8to64:
		v.Op = OpAMD64MOVBQZX
		return true
	case OpZeroSIMD:
		return rewriteValueAMD64_OpZeroSIMD(v)
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADCQ(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ADCQ x (MOVQconst [c]) carry)
	// cond: is32Bit(c)
	// result: (ADCQconst x [int32(c)] carry)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_1.AuxInt)
			carry := v_2
			if !(is32Bit(c)) {
				continue
			}
			v.reset(OpAMD64ADCQconst)
			v.AuxInt = int32ToAuxInt(int32(c))
			v.AddArg2(x, carry)
			return true
		}
		break
	}
	// match: (ADCQ x y (FlagEQ))
	// result: (ADDQcarry x y)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64ADDQcarry)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADCQconst(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ADCQconst x [c] (FlagEQ))
	// result: (ADDQconstcarry x [c])
	for {
		c := auxIntToInt32(v.AuxInt)
		x := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64ADDQconstcarry)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ADDL (SHRLconst [1] x) (SHRLconst [1] x))
	// result: (ANDLconst [-2] x)
	for {
		if v_0.Op != OpAMD64SHRLconst || auxIntToInt8(v_0.AuxInt) != 1 {
			break
		}
		x := v_0.Args[0]
		if v_1.Op != OpAMD64SHRLconst || auxIntToInt8(v_1.AuxInt) != 1 || x != v_1.Args[0] {
			break
		}
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(-2)
		v.AddArg(x)
		return true
	}
	// match: (ADDL x (MOVLconst [c]))
	// result: (ADDLconst [c] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVLconst {
				continue
			}
			c := auxIntToInt32(v_1.AuxInt)
			v.reset(OpAMD64ADDLconst)
			v.AuxInt = int32ToAuxInt(c)
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ADDL x (SHLLconst [3] y))
	// result: (LEAL8 x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64SHLLconst || auxIntToInt8(v_1.AuxInt) != 3 {
				continue
			}
			y := v_1.Args[0]
			v.reset(OpAMD64LEAL8)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDL x (SHLLconst [2] y))
	// result: (LEAL4 x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64SHLLconst || auxIntToInt8(v_1.AuxInt) != 2 {
				continue
			}
			y := v_1.Args[0]
			v.reset(OpAMD64LEAL4)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDL x (ADDL y y))
	// result: (LEAL2 x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64ADDL {
				continue
			}
			y := v_1.Args[1]
			if y != v_1.Args[0] {
				continue
			}
			v.reset(OpAMD64LEAL2)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDL x (ADDL x y))
	// result: (LEAL2 y x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64ADDL {
				continue
			}
			_ = v_1.Args[1]
			v_1_0 := v_1.Args[0]
			v_1_1 := v_1.Args[1]
			for _i1 := 0; _i1 <= 1; _i1, v_1_0, v_1_1 = _i1+1, v_1_1, v_1_0 {
				if x != v_1_0 {
					continue
				}
				y := v_1_1
				v.reset(OpAMD64LEAL2)
				v.AddArg2(y, x)
				return true
			}
		}
		break
	}
	// match: (ADDL (ADDLconst [c] x) y)
	// result: (LEAL1 [c] x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64ADDLconst {
				continue
			}
			c := auxIntToInt32(v_0.AuxInt)
			x := v_0.Args[0]
			y := v_1
			v.reset(OpAMD64LEAL1)
			v.AuxInt = int32ToAuxInt(c)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDL x (LEAL [c] {s} y))
	// cond: x.Op != OpSB && y.Op != OpSB
	// result: (LEAL1 [c] {s} x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64LEAL {
				continue
			}
			c := auxIntToInt32(v_1.AuxInt)
			s := auxToSym(v_1.Aux)
			y := v_1.Args[0]
			if !(x.Op != OpSB && y.Op != OpSB) {
				continue
			}
			v.reset(OpAMD64LEAL1)
			v.AuxInt = int32ToAuxInt(c)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDL x (NEGL y))
	// result: (SUBL x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64NEGL {
				continue
			}
			y := v_1.Args[0]
			v.reset(OpAMD64SUBL)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDL x l:(MOVLload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (ADDLload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVLload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64ADDLload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDLconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (ADDLconst [c] (ADDL x y))
	// result: (LEAL1 [c] x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ADDL {
			break
		}
		y := v_0.Args[1]
		x := v_0.Args[0]
		v.reset(OpAMD64LEAL1)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg2(x, y)
		return true
	}
	// match: (ADDLconst [c] (ADDL x x))
	// result: (LEAL1 [c] x x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ADDL {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpAMD64LEAL1)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg2(x, x)
		return true
	}
	// match: (ADDLconst [c] (LEAL [d] {s} x))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAL [c+d] {s} x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64LEAL {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		s := auxToSym(v_0.Aux)
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAL)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg(x)
		return true
	}
	// match: (ADDLconst [c] (LEAL1 [d] {s} x y))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAL1 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64LEAL1 {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		s := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAL1)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (ADDLconst [c] (LEAL2 [d] {s} x y))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAL2 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64LEAL2 {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		s := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAL2)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (ADDLconst [c] (LEAL4 [d] {s} x y))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAL4 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64LEAL4 {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		s := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAL4)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (ADDLconst [c] (LEAL8 [d] {s} x y))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAL8 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64LEAL8 {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		s := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAL8)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (ADDLconst [c] x)
	// cond: c==0
	// result: x
	for {
		c := auxIntToInt32(v.AuxInt)
		x := v_0
		if !(c == 0) {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (ADDLconst [c] (MOVLconst [d]))
	// result: (MOVLconst [c+d])
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(c + d)
		return true
	}
	// match: (ADDLconst [c] (ADDLconst [d] x))
	// result: (ADDLconst [c+d] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ADDLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		v.reset(OpAMD64ADDLconst)
		v.AuxInt = int32ToAuxInt(c + d)
		v.AddArg(x)
		return true
	}
	// match: (ADDLconst [off] x:(SP))
	// result: (LEAL [off] x)
	for {
		off := auxIntToInt32(v.AuxInt)
		x := v_0
		if x.Op != OpSP {
			break
		}
		v.reset(OpAMD64LEAL)
		v.AuxInt = int32ToAuxInt(off)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDLconstmodify(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ADDLconstmodify [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (ADDLconstmodify [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64ADDLconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (ADDLconstmodify [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (ADDLconstmodify [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ADDLconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDLload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (ADDLload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ADDLload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ADDLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ADDLload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ADDLload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ADDLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ADDLload x [off] {sym} ptr (MOVSSstore [off] {sym} ptr y _))
	// result: (ADDL x (MOVLf2i y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVSSstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64ADDL)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVLf2i, typ.UInt32)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDLmodify(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ADDLmodify [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ADDLmodify [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ADDLmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (ADDLmodify [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ADDLmodify [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ADDLmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ADDQ (SHRQconst [1] x) (SHRQconst [1] x))
	// result: (ANDQconst [-2] x)
	for {
		if v_0.Op != OpAMD64SHRQconst || auxIntToInt8(v_0.AuxInt) != 1 {
			break
		}
		x := v_0.Args[0]
		if v_1.Op != OpAMD64SHRQconst || auxIntToInt8(v_1.AuxInt) != 1 || x != v_1.Args[0] {
			break
		}
		v.reset(OpAMD64ANDQconst)
		v.AuxInt = int32ToAuxInt(-2)
		v.AddArg(x)
		return true
	}
	// match: (ADDQ x (MOVQconst <t> [c]))
	// cond: is32Bit(c) && !t.IsPtr()
	// result: (ADDQconst [int32(c)] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVQconst {
				continue
			}
			t := v_1.Type
			c := auxIntToInt64(v_1.AuxInt)
			if !(is32Bit(c) && !t.IsPtr()) {
				continue
			}
			v.reset(OpAMD64ADDQconst)
			v.AuxInt = int32ToAuxInt(int32(c))
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ADDQ x (MOVLconst [c]))
	// result: (ADDQconst [c] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVLconst {
				continue
			}
			c := auxIntToInt32(v_1.AuxInt)
			v.reset(OpAMD64ADDQconst)
			v.AuxInt = int32ToAuxInt(c)
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ADDQ x (SHLQconst [3] y))
	// result: (LEAQ8 x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64SHLQconst || auxIntToInt8(v_1.AuxInt) != 3 {
				continue
			}
			y := v_1.Args[0]
			v.reset(OpAMD64LEAQ8)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDQ x (SHLQconst [2] y))
	// result: (LEAQ4 x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64SHLQconst || auxIntToInt8(v_1.AuxInt) != 2 {
				continue
			}
			y := v_1.Args[0]
			v.reset(OpAMD64LEAQ4)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDQ x (ADDQ y y))
	// result: (LEAQ2 x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64ADDQ {
				continue
			}
			y := v_1.Args[1]
			if y != v_1.Args[0] {
				continue
			}
			v.reset(OpAMD64LEAQ2)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDQ x (ADDQ x y))
	// result: (LEAQ2 y x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64ADDQ {
				continue
			}
			_ = v_1.Args[1]
			v_1_0 := v_1.Args[0]
			v_1_1 := v_1.Args[1]
			for _i1 := 0; _i1 <= 1; _i1, v_1_0, v_1_1 = _i1+1, v_1_1, v_1_0 {
				if x != v_1_0 {
					continue
				}
				y := v_1_1
				v.reset(OpAMD64LEAQ2)
				v.AddArg2(y, x)
				return true
			}
		}
		break
	}
	// match: (ADDQ (ADDQconst [c] x) y)
	// result: (LEAQ1 [c] x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64ADDQconst {
				continue
			}
			c := auxIntToInt32(v_0.AuxInt)
			x := v_0.Args[0]
			y := v_1
			v.reset(OpAMD64LEAQ1)
			v.AuxInt = int32ToAuxInt(c)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDQ x (LEAQ [c] {s} y))
	// cond: x.Op != OpSB && y.Op != OpSB
	// result: (LEAQ1 [c] {s} x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64LEAQ {
				continue
			}
			c := auxIntToInt32(v_1.AuxInt)
			s := auxToSym(v_1.Aux)
			y := v_1.Args[0]
			if !(x.Op != OpSB && y.Op != OpSB) {
				continue
			}
			v.reset(OpAMD64LEAQ1)
			v.AuxInt = int32ToAuxInt(c)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDQ x (NEGQ y))
	// result: (SUBQ x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64NEGQ {
				continue
			}
			y := v_1.Args[0]
			v.reset(OpAMD64SUBQ)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ADDQ x l:(MOVQload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (ADDQload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVQload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64ADDQload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDQcarry(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ADDQcarry x (MOVQconst [c]))
	// cond: is32Bit(c)
	// result: (ADDQconstcarry x [int32(c)])
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_1.AuxInt)
			if !(is32Bit(c)) {
				continue
			}
			v.reset(OpAMD64ADDQconstcarry)
			v.AuxInt = int32ToAuxInt(int32(c))
			v.AddArg(x)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (ADDQconst [c] (ADDQ x y))
	// result: (LEAQ1 [c] x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ADDQ {
			break
		}
		y := v_0.Args[1]
		x := v_0.Args[0]
		v.reset(OpAMD64LEAQ1)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg2(x, y)
		return true
	}
	// match: (ADDQconst [c] (ADDQ x x))
	// result: (LEAQ1 [c] x x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ADDQ {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpAMD64LEAQ1)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg2(x, x)
		return true
	}
	// match: (ADDQconst [c] (LEAQ [d] {s} x))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAQ [c+d] {s} x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		s := auxToSym(v_0.Aux)
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg(x)
		return true
	}
	// match: (ADDQconst [c] (LEAQ1 [d] {s} x y))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAQ1 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64LEAQ1 {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		s := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAQ1)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (ADDQconst [c] (LEAQ2 [d] {s} x y))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAQ2 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64LEAQ2 {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		s := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAQ2)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (ADDQconst [c] (LEAQ4 [d] {s} x y))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAQ4 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64LEAQ4 {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		s := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAQ4)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (ADDQconst [c] (LEAQ8 [d] {s} x y))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAQ8 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64LEAQ8 {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		s := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAQ8)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (ADDQconst [0] x)
	// result: x
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (ADDQconst [c] (MOVQconst [d]))
	// result: (MOVQconst [int64(c)+d])
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(c) + d)
		return true
	}
	// match: (ADDQconst [c] (ADDQconst [d] x))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (ADDQconst [c+d] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64ADDQconst)
		v.AuxInt = int32ToAuxInt(c + d)
		v.AddArg(x)
		return true
	}
	// match: (ADDQconst [off] x:(SP))
	// result: (LEAQ [off] x)
	for {
		off := auxIntToInt32(v.AuxInt)
		x := v_0
		if x.Op != OpSP {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.AuxInt = int32ToAuxInt(off)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDQconstmodify(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ADDQconstmodify [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (ADDQconstmodify [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64ADDQconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (ADDQconstmodify [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (ADDQconstmodify [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ADDQconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDQload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (ADDQload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ADDQload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ADDQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ADDQload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ADDQload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ADDQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ADDQload x [off] {sym} ptr (MOVSDstore [off] {sym} ptr y _))
	// result: (ADDQ x (MOVQf2i y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVSDstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64ADDQ)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVQf2i, typ.UInt64)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDQmodify(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ADDQmodify [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ADDQmodify [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ADDQmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (ADDQmodify [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ADDQmodify [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ADDQmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDSD(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ADDSD x l:(MOVSDload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (ADDSDload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVSDload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64ADDSDload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	// match: (ADDSD (MULSD x y) z)
	// cond: buildcfg.GOAMD64 >= 3 && z.Block.Func.useFMA(v)
	// result: (VFMADD231SD z x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64MULSD {
				continue
			}
			y := v_0.Args[1]
			x := v_0.Args[0]
			z := v_1
			if !(buildcfg.GOAMD64 >= 3 && z.Block.Func.useFMA(v)) {
				continue
			}
			v.reset(OpAMD64VFMADD231SD)
			v.AddArg3(z, x, y)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDSDload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (ADDSDload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ADDSDload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ADDSDload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ADDSDload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ADDSDload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ADDSDload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ADDSDload x [off] {sym} ptr (MOVQstore [off] {sym} ptr y _))
	// result: (ADDSD x (MOVQi2f y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVQstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64ADDSD)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVQi2f, typ.Float64)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDSS(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ADDSS x l:(MOVSSload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (ADDSSload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVSSload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64ADDSSload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	// match: (ADDSS (MULSS x y) z)
	// cond: buildcfg.GOAMD64 >= 3 && z.Block.Func.useFMA(v)
	// result: (VFMADD231SS z x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64MULSS {
				continue
			}
			y := v_0.Args[1]
			x := v_0.Args[0]
			z := v_1
			if !(buildcfg.GOAMD64 >= 3 && z.Block.Func.useFMA(v)) {
				continue
			}
			v.reset(OpAMD64VFMADD231SS)
			v.AddArg3(z, x, y)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64ADDSSload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (ADDSSload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ADDSSload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ADDSSload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ADDSSload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ADDSSload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ADDSSload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ADDSSload x [off] {sym} ptr (MOVLstore [off] {sym} ptr y _))
	// result: (ADDSS x (MOVLi2f y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVLstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64ADDSS)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVLi2f, typ.Float32)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (ANDL (NOTL (SHLL (MOVLconst [1]) y)) x)
	// result: (BTRL x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64NOTL {
				continue
			}
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SHLL {
				continue
			}
			y := v_0_0.Args[1]
			v_0_0_0 := v_0_0.Args[0]
			if v_0_0_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_0_0_0.AuxInt) != 1 {
				continue
			}
			x := v_1
			v.reset(OpAMD64BTRL)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ANDL x (MOVLconst [c]))
	// result: (ANDLconst [c] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVLconst {
				continue
			}
			c := auxIntToInt32(v_1.AuxInt)
			v.reset(OpAMD64ANDLconst)
			v.AuxInt = int32ToAuxInt(c)
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ANDL x x)
	// result: x
	for {
		x := v_0
		if x != v_1 {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (ANDL x l:(MOVLload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (ANDLload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVLload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64ANDLload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	// match: (ANDL x (NOTL y))
	// cond: buildcfg.GOAMD64 >= 3
	// result: (ANDNL x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64NOTL {
				continue
			}
			y := v_1.Args[0]
			if !(buildcfg.GOAMD64 >= 3) {
				continue
			}
			v.reset(OpAMD64ANDNL)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ANDL x (NEGL x))
	// cond: buildcfg.GOAMD64 >= 3
	// result: (BLSIL x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64NEGL || x != v_1.Args[0] || !(buildcfg.GOAMD64 >= 3) {
				continue
			}
			v.reset(OpAMD64BLSIL)
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ANDL <t> x (ADDLconst [-1] x))
	// cond: buildcfg.GOAMD64 >= 3
	// result: (Select0 <t> (BLSRL x))
	for {
		t := v.Type
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64ADDLconst || auxIntToInt32(v_1.AuxInt) != -1 || x != v_1.Args[0] || !(buildcfg.GOAMD64 >= 3) {
				continue
			}
			v.reset(OpSelect0)
			v.Type = t
			v0 := b.NewValue0(v.Pos, OpAMD64BLSRL, types.NewTuple(typ.UInt32, types.TypeFlags))
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDLconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (ANDLconst [c] (ANDLconst [d] x))
	// result: (ANDLconst [c & d] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ANDLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(c & d)
		v.AddArg(x)
		return true
	}
	// match: (ANDLconst [ 0xFF] x)
	// result: (MOVBQZX x)
	for {
		if auxIntToInt32(v.AuxInt) != 0xFF {
			break
		}
		x := v_0
		v.reset(OpAMD64MOVBQZX)
		v.AddArg(x)
		return true
	}
	// match: (ANDLconst [0xFFFF] x)
	// result: (MOVWQZX x)
	for {
		if auxIntToInt32(v.AuxInt) != 0xFFFF {
			break
		}
		x := v_0
		v.reset(OpAMD64MOVWQZX)
		v.AddArg(x)
		return true
	}
	// match: (ANDLconst [c] _)
	// cond: c==0
	// result: (MOVLconst [0])
	for {
		c := auxIntToInt32(v.AuxInt)
		if !(c == 0) {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (ANDLconst [c] x)
	// cond: c==-1
	// result: x
	for {
		c := auxIntToInt32(v.AuxInt)
		x := v_0
		if !(c == -1) {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (ANDLconst [c] (MOVLconst [d]))
	// result: (MOVLconst [c&d])
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(c & d)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDLconstmodify(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ANDLconstmodify [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (ANDLconstmodify [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64ANDLconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (ANDLconstmodify [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (ANDLconstmodify [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ANDLconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDLload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (ANDLload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ANDLload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ANDLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ANDLload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ANDLload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ANDLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ANDLload x [off] {sym} ptr (MOVSSstore [off] {sym} ptr y _))
	// result: (ANDL x (MOVLf2i y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVSSstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVLf2i, typ.UInt32)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDLmodify(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ANDLmodify [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ANDLmodify [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ANDLmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (ANDLmodify [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ANDLmodify [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ANDLmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDNL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ANDNL x (SHLL (MOVLconst [1]) y))
	// result: (BTRL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64SHLL {
			break
		}
		y := v_1.Args[1]
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_1_0.AuxInt) != 1 {
			break
		}
		v.reset(OpAMD64BTRL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDNQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ANDNQ x (SHLQ (MOVQconst [1]) y))
	// result: (BTRQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64SHLQ {
			break
		}
		y := v_1.Args[1]
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64MOVQconst || auxIntToInt64(v_1_0.AuxInt) != 1 {
			break
		}
		v.reset(OpAMD64BTRQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (ANDQ (NOTQ (SHLQ (MOVQconst [1]) y)) x)
	// result: (BTRQ x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64NOTQ {
				continue
			}
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SHLQ {
				continue
			}
			y := v_0_0.Args[1]
			v_0_0_0 := v_0_0.Args[0]
			if v_0_0_0.Op != OpAMD64MOVQconst || auxIntToInt64(v_0_0_0.AuxInt) != 1 {
				continue
			}
			x := v_1
			v.reset(OpAMD64BTRQ)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ANDQ (MOVQconst [c]) x)
	// cond: isUint64PowerOfTwo(^c) && uint64(^c) >= 1<<31
	// result: (BTRQconst [int8(log64(^c))] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_0.AuxInt)
			x := v_1
			if !(isUint64PowerOfTwo(^c) && uint64(^c) >= 1<<31) {
				continue
			}
			v.reset(OpAMD64BTRQconst)
			v.AuxInt = int8ToAuxInt(int8(log64(^c)))
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ANDQ x (MOVQconst [c]))
	// cond: is32Bit(c)
	// result: (ANDQconst [int32(c)] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_1.AuxInt)
			if !(is32Bit(c)) {
				continue
			}
			v.reset(OpAMD64ANDQconst)
			v.AuxInt = int32ToAuxInt(int32(c))
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ANDQ x x)
	// result: x
	for {
		x := v_0
		if x != v_1 {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (ANDQ x l:(MOVQload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (ANDQload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVQload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64ANDQload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	// match: (ANDQ x (NOTQ y))
	// cond: buildcfg.GOAMD64 >= 3
	// result: (ANDNQ x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64NOTQ {
				continue
			}
			y := v_1.Args[0]
			if !(buildcfg.GOAMD64 >= 3) {
				continue
			}
			v.reset(OpAMD64ANDNQ)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ANDQ x (NEGQ x))
	// cond: buildcfg.GOAMD64 >= 3
	// result: (BLSIQ x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64NEGQ || x != v_1.Args[0] || !(buildcfg.GOAMD64 >= 3) {
				continue
			}
			v.reset(OpAMD64BLSIQ)
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ANDQ <t> x (ADDQconst [-1] x))
	// cond: buildcfg.GOAMD64 >= 3
	// result: (Select0 <t> (BLSRQ x))
	for {
		t := v.Type
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64ADDQconst || auxIntToInt32(v_1.AuxInt) != -1 || x != v_1.Args[0] || !(buildcfg.GOAMD64 >= 3) {
				continue
			}
			v.reset(OpSelect0)
			v.Type = t
			v0 := b.NewValue0(v.Pos, OpAMD64BLSRQ, types.NewTuple(typ.UInt64, types.TypeFlags))
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (ANDQconst [c] (ANDQconst [d] x))
	// result: (ANDQconst [c & d] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ANDQconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		v.reset(OpAMD64ANDQconst)
		v.AuxInt = int32ToAuxInt(c & d)
		v.AddArg(x)
		return true
	}
	// match: (ANDQconst [ 0xFF] x)
	// result: (MOVBQZX x)
	for {
		if auxIntToInt32(v.AuxInt) != 0xFF {
			break
		}
		x := v_0
		v.reset(OpAMD64MOVBQZX)
		v.AddArg(x)
		return true
	}
	// match: (ANDQconst [0xFFFF] x)
	// result: (MOVWQZX x)
	for {
		if auxIntToInt32(v.AuxInt) != 0xFFFF {
			break
		}
		x := v_0
		v.reset(OpAMD64MOVWQZX)
		v.AddArg(x)
		return true
	}
	// match: (ANDQconst [0] _)
	// result: (MOVQconst [0])
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(0)
		return true
	}
	// match: (ANDQconst [-1] x)
	// result: x
	for {
		if auxIntToInt32(v.AuxInt) != -1 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (ANDQconst [c] (MOVQconst [d]))
	// result: (MOVQconst [int64(c)&d])
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(c) & d)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDQconstmodify(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ANDQconstmodify [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (ANDQconstmodify [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64ANDQconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (ANDQconstmodify [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (ANDQconstmodify [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ANDQconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDQload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (ANDQload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ANDQload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ANDQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ANDQload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ANDQload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ANDQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ANDQload x [off] {sym} ptr (MOVSDstore [off] {sym} ptr y _))
	// result: (ANDQ x (MOVQf2i y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVSDstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64ANDQ)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVQf2i, typ.UInt64)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ANDQmodify(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ANDQmodify [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ANDQmodify [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ANDQmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (ANDQmodify [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ANDQmodify [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ANDQmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64BSFQ(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (BSFQ (ORQconst <t> [1<<8] (MOVBQZX x)))
	// result: (BSFQ (ORQconst <t> [1<<8] x))
	for {
		if v_0.Op != OpAMD64ORQconst {
			break
		}
		t := v_0.Type
		if auxIntToInt32(v_0.AuxInt) != 1<<8 {
			break
		}
		v_0_0 := v_0.Args[0]
		if v_0_0.Op != OpAMD64MOVBQZX {
			break
		}
		x := v_0_0.Args[0]
		v.reset(OpAMD64BSFQ)
		v0 := b.NewValue0(v.Pos, OpAMD64ORQconst, t)
		v0.AuxInt = int32ToAuxInt(1 << 8)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (BSFQ (ORQconst <t> [1<<16] (MOVWQZX x)))
	// result: (BSFQ (ORQconst <t> [1<<16] x))
	for {
		if v_0.Op != OpAMD64ORQconst {
			break
		}
		t := v_0.Type
		if auxIntToInt32(v_0.AuxInt) != 1<<16 {
			break
		}
		v_0_0 := v_0.Args[0]
		if v_0_0.Op != OpAMD64MOVWQZX {
			break
		}
		x := v_0_0.Args[0]
		v.reset(OpAMD64BSFQ)
		v0 := b.NewValue0(v.Pos, OpAMD64ORQconst, t)
		v0.AuxInt = int32ToAuxInt(1 << 16)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64BSWAPL(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (BSWAPL (BSWAPL p))
	// result: p
	for {
		if v_0.Op != OpAMD64BSWAPL {
			break
		}
		p := v_0.Args[0]
		v.copyOf(p)
		return true
	}
	// match: (BSWAPL x:(MOVLload [i] {s} p mem))
	// cond: x.Uses == 1 && buildcfg.GOAMD64 >= 3
	// result: @x.Block (MOVBELload [i] {s} p mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVLload {
			break
		}
		i := auxIntToInt32(x.AuxInt)
		s := auxToSym(x.Aux)
		mem := x.Args[1]
		p := x.Args[0]
		if !(x.Uses == 1 && buildcfg.GOAMD64 >= 3) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVBELload, typ.UInt32)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(i)
		v0.Aux = symToAux(s)
		v0.AddArg2(p, mem)
		return true
	}
	// match: (BSWAPL x:(MOVBELload [i] {s} p mem))
	// cond: x.Uses == 1
	// result: @x.Block (MOVLload [i] {s} p mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVBELload {
			break
		}
		i := auxIntToInt32(x.AuxInt)
		s := auxToSym(x.Aux)
		mem := x.Args[1]
		p := x.Args[0]
		if !(x.Uses == 1) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVLload, typ.UInt32)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(i)
		v0.Aux = symToAux(s)
		v0.AddArg2(p, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64BSWAPQ(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (BSWAPQ (BSWAPQ p))
	// result: p
	for {
		if v_0.Op != OpAMD64BSWAPQ {
			break
		}
		p := v_0.Args[0]
		v.copyOf(p)
		return true
	}
	// match: (BSWAPQ x:(MOVQload [i] {s} p mem))
	// cond: x.Uses == 1 && buildcfg.GOAMD64 >= 3
	// result: @x.Block (MOVBEQload [i] {s} p mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVQload {
			break
		}
		i := auxIntToInt32(x.AuxInt)
		s := auxToSym(x.Aux)
		mem := x.Args[1]
		p := x.Args[0]
		if !(x.Uses == 1 && buildcfg.GOAMD64 >= 3) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVBEQload, typ.UInt64)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(i)
		v0.Aux = symToAux(s)
		v0.AddArg2(p, mem)
		return true
	}
	// match: (BSWAPQ x:(MOVBEQload [i] {s} p mem))
	// cond: x.Uses == 1
	// result: @x.Block (MOVQload [i] {s} p mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVBEQload {
			break
		}
		i := auxIntToInt32(x.AuxInt)
		s := auxToSym(x.Aux)
		mem := x.Args[1]
		p := x.Args[0]
		if !(x.Uses == 1) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVQload, typ.UInt64)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(i)
		v0.Aux = symToAux(s)
		v0.AddArg2(p, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64BTCQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (BTCQconst [c] (MOVQconst [d]))
	// result: (MOVQconst [d^(1<<uint32(c))])
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(d ^ (1 << uint32(c)))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64BTLconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (BTLconst [c] (SHRQconst [d] x))
	// cond: (c+d)<64
	// result: (BTQconst [c+d] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64SHRQconst {
			break
		}
		d := auxIntToInt8(v_0.AuxInt)
		x := v_0.Args[0]
		if !((c + d) < 64) {
			break
		}
		v.reset(OpAMD64BTQconst)
		v.AuxInt = int8ToAuxInt(c + d)
		v.AddArg(x)
		return true
	}
	// match: (BTLconst [c] (ADDQ x x))
	// cond: c>1
	// result: (BTLconst [c-1] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64ADDQ {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] || !(c > 1) {
			break
		}
		v.reset(OpAMD64BTLconst)
		v.AuxInt = int8ToAuxInt(c - 1)
		v.AddArg(x)
		return true
	}
	// match: (BTLconst [c] (SHLQconst [d] x))
	// cond: c>d
	// result: (BTLconst [c-d] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64SHLQconst {
			break
		}
		d := auxIntToInt8(v_0.AuxInt)
		x := v_0.Args[0]
		if !(c > d) {
			break
		}
		v.reset(OpAMD64BTLconst)
		v.AuxInt = int8ToAuxInt(c - d)
		v.AddArg(x)
		return true
	}
	// match: (BTLconst [0] s:(SHRQ x y))
	// result: (BTQ y x)
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		s := v_0
		if s.Op != OpAMD64SHRQ {
			break
		}
		y := s.Args[1]
		x := s.Args[0]
		v.reset(OpAMD64BTQ)
		v.AddArg2(y, x)
		return true
	}
	// match: (BTLconst [c] (SHRLconst [d] x))
	// cond: (c+d)<32
	// result: (BTLconst [c+d] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64SHRLconst {
			break
		}
		d := auxIntToInt8(v_0.AuxInt)
		x := v_0.Args[0]
		if !((c + d) < 32) {
			break
		}
		v.reset(OpAMD64BTLconst)
		v.AuxInt = int8ToAuxInt(c + d)
		v.AddArg(x)
		return true
	}
	// match: (BTLconst [c] (ADDL x x))
	// cond: c>1
	// result: (BTLconst [c-1] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64ADDL {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] || !(c > 1) {
			break
		}
		v.reset(OpAMD64BTLconst)
		v.AuxInt = int8ToAuxInt(c - 1)
		v.AddArg(x)
		return true
	}
	// match: (BTLconst [c] (SHLLconst [d] x))
	// cond: c>d
	// result: (BTLconst [c-d] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64SHLLconst {
			break
		}
		d := auxIntToInt8(v_0.AuxInt)
		x := v_0.Args[0]
		if !(c > d) {
			break
		}
		v.reset(OpAMD64BTLconst)
		v.AuxInt = int8ToAuxInt(c - d)
		v.AddArg(x)
		return true
	}
	// match: (BTLconst [0] s:(SHRL x y))
	// result: (BTL y x)
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		s := v_0
		if s.Op != OpAMD64SHRL {
			break
		}
		y := s.Args[1]
		x := s.Args[0]
		v.reset(OpAMD64BTL)
		v.AddArg2(y, x)
		return true
	}
	// match: (BTLconst [0] s:(SHRXL x y))
	// result: (BTL y x)
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		s := v_0
		if s.Op != OpAMD64SHRXL {
			break
		}
		y := s.Args[1]
		x := s.Args[0]
		v.reset(OpAMD64BTL)
		v.AddArg2(y, x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64BTQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (BTQconst [c] (SHRQconst [d] x))
	// cond: (c+d)<64
	// result: (BTQconst [c+d] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64SHRQconst {
			break
		}
		d := auxIntToInt8(v_0.AuxInt)
		x := v_0.Args[0]
		if !((c + d) < 64) {
			break
		}
		v.reset(OpAMD64BTQconst)
		v.AuxInt = int8ToAuxInt(c + d)
		v.AddArg(x)
		return true
	}
	// match: (BTQconst [c] (ADDQ x x))
	// cond: c>1
	// result: (BTQconst [c-1] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64ADDQ {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] || !(c > 1) {
			break
		}
		v.reset(OpAMD64BTQconst)
		v.AuxInt = int8ToAuxInt(c - 1)
		v.AddArg(x)
		return true
	}
	// match: (BTQconst [c] (SHLQconst [d] x))
	// cond: c>d
	// result: (BTQconst [c-d] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64SHLQconst {
			break
		}
		d := auxIntToInt8(v_0.AuxInt)
		x := v_0.Args[0]
		if !(c > d) {
			break
		}
		v.reset(OpAMD64BTQconst)
		v.AuxInt = int8ToAuxInt(c - d)
		v.AddArg(x)
		return true
	}
	// match: (BTQconst [0] s:(SHRQ x y))
	// result: (BTQ y x)
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		s := v_0
		if s.Op != OpAMD64SHRQ {
			break
		}
		y := s.Args[1]
		x := s.Args[0]
		v.reset(OpAMD64BTQ)
		v.AddArg2(y, x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64BTRQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (BTRQconst [c] (BTSQconst [c] x))
	// result: (BTRQconst [c] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64BTSQconst || auxIntToInt8(v_0.AuxInt) != c {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64BTRQconst)
		v.AuxInt = int8ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	// match: (BTRQconst [c] (BTCQconst [c] x))
	// result: (BTRQconst [c] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64BTCQconst || auxIntToInt8(v_0.AuxInt) != c {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64BTRQconst)
		v.AuxInt = int8ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	// match: (BTRQconst [c] (MOVQconst [d]))
	// result: (MOVQconst [d&^(1<<uint32(c))])
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(d &^ (1 << uint32(c)))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64BTSQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (BTSQconst [c] (BTRQconst [c] x))
	// result: (BTSQconst [c] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64BTRQconst || auxIntToInt8(v_0.AuxInt) != c {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64BTSQconst)
		v.AuxInt = int8ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	// match: (BTSQconst [c] (BTCQconst [c] x))
	// result: (BTSQconst [c] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64BTCQconst || auxIntToInt8(v_0.AuxInt) != c {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64BTSQconst)
		v.AuxInt = int8ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	// match: (BTSQconst [c] (MOVQconst [d]))
	// result: (MOVQconst [d|(1<<uint32(c))])
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(d | (1 << uint32(c)))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVLCC(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVLCC x y (InvertFlags cond))
	// result: (CMOVLLS x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVLLS)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVLCC _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLCC _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLCC y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLCC y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLCC _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVLCS(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVLCS x y (InvertFlags cond))
	// result: (CMOVLHI x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVLHI)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVLCS y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLCS y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLCS _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLCS _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLCS y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVLEQ(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMOVLEQ x y (InvertFlags cond))
	// result: (CMOVLEQ x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVLEQ)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVLEQ _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLEQ y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLEQ y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLEQ y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLEQ y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLEQ x y (TESTQ s:(Select0 blsr:(BLSRQ _)) s))
	// result: (CMOVLEQ x y (Select1 <types.TypeFlags> blsr))
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64TESTQ {
			break
		}
		_ = v_2.Args[1]
		v_2_0 := v_2.Args[0]
		v_2_1 := v_2.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_2_0, v_2_1 = _i0+1, v_2_1, v_2_0 {
			s := v_2_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRQ || s != v_2_1 {
				continue
			}
			v.reset(OpAMD64CMOVLEQ)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg3(x, y, v0)
			return true
		}
		break
	}
	// match: (CMOVLEQ x y (TESTL s:(Select0 blsr:(BLSRL _)) s))
	// result: (CMOVLEQ x y (Select1 <types.TypeFlags> blsr))
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64TESTL {
			break
		}
		_ = v_2.Args[1]
		v_2_0 := v_2.Args[0]
		v_2_1 := v_2.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_2_0, v_2_1 = _i0+1, v_2_1, v_2_0 {
			s := v_2_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRL || s != v_2_1 {
				continue
			}
			v.reset(OpAMD64CMOVLEQ)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg3(x, y, v0)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVLGE(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMOVLGE x y (InvertFlags cond))
	// result: (CMOVLLE x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVLLE)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVLGE _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLGE _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLGE _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLGE y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLGE y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLGE x y c:(CMPQconst [128] z))
	// cond: c.Uses == 1
	// result: (CMOVLGT x y (CMPQconst [127] z))
	for {
		x := v_0
		y := v_1
		c := v_2
		if c.Op != OpAMD64CMPQconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		z := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64CMOVLGT)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(z)
		v.AddArg3(x, y, v0)
		return true
	}
	// match: (CMOVLGE x y c:(CMPLconst [128] z))
	// cond: c.Uses == 1
	// result: (CMOVLGT x y (CMPLconst [127] z))
	for {
		x := v_0
		y := v_1
		c := v_2
		if c.Op != OpAMD64CMPLconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		z := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64CMOVLGT)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(z)
		v.AddArg3(x, y, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVLGT(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVLGT x y (InvertFlags cond))
	// result: (CMOVLLT x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVLLT)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVLGT y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLGT _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLGT _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLGT y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLGT y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVLHI(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVLHI x y (InvertFlags cond))
	// result: (CMOVLCS x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVLCS)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVLHI y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLHI _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLHI y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLHI y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLHI _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVLLE(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVLLE x y (InvertFlags cond))
	// result: (CMOVLGE x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVLGE)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVLLE _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLLE y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLLE y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLLE _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLLE _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVLLS(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVLLS x y (InvertFlags cond))
	// result: (CMOVLCC x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVLCC)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVLLS _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLLS y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLLS _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLLS _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLLS y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVLLT(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMOVLLT x y (InvertFlags cond))
	// result: (CMOVLGT x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVLGT)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVLLT y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLLT y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLLT y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLLT _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLLT _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLLT x y c:(CMPQconst [128] z))
	// cond: c.Uses == 1
	// result: (CMOVLLE x y (CMPQconst [127] z))
	for {
		x := v_0
		y := v_1
		c := v_2
		if c.Op != OpAMD64CMPQconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		z := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64CMOVLLE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(z)
		v.AddArg3(x, y, v0)
		return true
	}
	// match: (CMOVLLT x y c:(CMPLconst [128] z))
	// cond: c.Uses == 1
	// result: (CMOVLLE x y (CMPLconst [127] z))
	for {
		x := v_0
		y := v_1
		c := v_2
		if c.Op != OpAMD64CMPLconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		z := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64CMOVLLE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(z)
		v.AddArg3(x, y, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVLNE(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMOVLNE x y (InvertFlags cond))
	// result: (CMOVLNE x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVLNE)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVLNE y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVLNE _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLNE _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLNE _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLNE _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVLNE x y (TESTQ s:(Select0 blsr:(BLSRQ _)) s))
	// result: (CMOVLNE x y (Select1 <types.TypeFlags> blsr))
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64TESTQ {
			break
		}
		_ = v_2.Args[1]
		v_2_0 := v_2.Args[0]
		v_2_1 := v_2.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_2_0, v_2_1 = _i0+1, v_2_1, v_2_0 {
			s := v_2_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRQ || s != v_2_1 {
				continue
			}
			v.reset(OpAMD64CMOVLNE)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg3(x, y, v0)
			return true
		}
		break
	}
	// match: (CMOVLNE x y (TESTL s:(Select0 blsr:(BLSRL _)) s))
	// result: (CMOVLNE x y (Select1 <types.TypeFlags> blsr))
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64TESTL {
			break
		}
		_ = v_2.Args[1]
		v_2_0 := v_2.Args[0]
		v_2_1 := v_2.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_2_0, v_2_1 = _i0+1, v_2_1, v_2_0 {
			s := v_2_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRL || s != v_2_1 {
				continue
			}
			v.reset(OpAMD64CMOVLNE)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg3(x, y, v0)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVQCC(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVQCC x y (InvertFlags cond))
	// result: (CMOVQLS x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVQLS)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVQCC _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQCC _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQCC y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQCC y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQCC _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVQCS(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVQCS x y (InvertFlags cond))
	// result: (CMOVQHI x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVQHI)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVQCS y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQCS y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQCS _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQCS _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQCS y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVQEQ(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMOVQEQ x y (InvertFlags cond))
	// result: (CMOVQEQ x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVQEQ)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVQEQ _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQEQ y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQEQ y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQEQ y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQEQ y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQEQ x _ (Select1 (BSFQ (ORQconst [c] _))))
	// cond: c != 0
	// result: x
	for {
		x := v_0
		if v_2.Op != OpSelect1 {
			break
		}
		v_2_0 := v_2.Args[0]
		if v_2_0.Op != OpAMD64BSFQ {
			break
		}
		v_2_0_0 := v_2_0.Args[0]
		if v_2_0_0.Op != OpAMD64ORQconst {
			break
		}
		c := auxIntToInt32(v_2_0_0.AuxInt)
		if !(c != 0) {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQEQ x _ (Select1 (BSRQ (ORQconst [c] _))))
	// cond: c != 0
	// result: x
	for {
		x := v_0
		if v_2.Op != OpSelect1 {
			break
		}
		v_2_0 := v_2.Args[0]
		if v_2_0.Op != OpAMD64BSRQ {
			break
		}
		v_2_0_0 := v_2_0.Args[0]
		if v_2_0_0.Op != OpAMD64ORQconst {
			break
		}
		c := auxIntToInt32(v_2_0_0.AuxInt)
		if !(c != 0) {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQEQ x y (TESTQ s:(Select0 blsr:(BLSRQ _)) s))
	// result: (CMOVQEQ x y (Select1 <types.TypeFlags> blsr))
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64TESTQ {
			break
		}
		_ = v_2.Args[1]
		v_2_0 := v_2.Args[0]
		v_2_1 := v_2.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_2_0, v_2_1 = _i0+1, v_2_1, v_2_0 {
			s := v_2_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRQ || s != v_2_1 {
				continue
			}
			v.reset(OpAMD64CMOVQEQ)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg3(x, y, v0)
			return true
		}
		break
	}
	// match: (CMOVQEQ x y (TESTL s:(Select0 blsr:(BLSRL _)) s))
	// result: (CMOVQEQ x y (Select1 <types.TypeFlags> blsr))
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64TESTL {
			break
		}
		_ = v_2.Args[1]
		v_2_0 := v_2.Args[0]
		v_2_1 := v_2.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_2_0, v_2_1 = _i0+1, v_2_1, v_2_0 {
			s := v_2_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRL || s != v_2_1 {
				continue
			}
			v.reset(OpAMD64CMOVQEQ)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg3(x, y, v0)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVQGE(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMOVQGE x y (InvertFlags cond))
	// result: (CMOVQLE x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVQLE)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVQGE _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQGE _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQGE _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQGE y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQGE y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQGE x y c:(CMPQconst [128] z))
	// cond: c.Uses == 1
	// result: (CMOVQGT x y (CMPQconst [127] z))
	for {
		x := v_0
		y := v_1
		c := v_2
		if c.Op != OpAMD64CMPQconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		z := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64CMOVQGT)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(z)
		v.AddArg3(x, y, v0)
		return true
	}
	// match: (CMOVQGE x y c:(CMPLconst [128] z))
	// cond: c.Uses == 1
	// result: (CMOVQGT x y (CMPLconst [127] z))
	for {
		x := v_0
		y := v_1
		c := v_2
		if c.Op != OpAMD64CMPLconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		z := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64CMOVQGT)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(z)
		v.AddArg3(x, y, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVQGT(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVQGT x y (InvertFlags cond))
	// result: (CMOVQLT x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVQLT)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVQGT y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQGT _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQGT _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQGT y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQGT y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVQHI(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVQHI x y (InvertFlags cond))
	// result: (CMOVQCS x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVQCS)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVQHI y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQHI _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQHI y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQHI y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQHI _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVQLE(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVQLE x y (InvertFlags cond))
	// result: (CMOVQGE x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVQGE)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVQLE _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQLE y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQLE y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQLE _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQLE _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVQLS(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVQLS x y (InvertFlags cond))
	// result: (CMOVQCC x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVQCC)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVQLS _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQLS y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQLS _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQLS _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQLS y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVQLT(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMOVQLT x y (InvertFlags cond))
	// result: (CMOVQGT x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVQGT)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVQLT y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQLT y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQLT y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQLT _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQLT _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQLT x y c:(CMPQconst [128] z))
	// cond: c.Uses == 1
	// result: (CMOVQLE x y (CMPQconst [127] z))
	for {
		x := v_0
		y := v_1
		c := v_2
		if c.Op != OpAMD64CMPQconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		z := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64CMOVQLE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(z)
		v.AddArg3(x, y, v0)
		return true
	}
	// match: (CMOVQLT x y c:(CMPLconst [128] z))
	// cond: c.Uses == 1
	// result: (CMOVQLE x y (CMPLconst [127] z))
	for {
		x := v_0
		y := v_1
		c := v_2
		if c.Op != OpAMD64CMPLconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		z := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64CMOVQLE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(z)
		v.AddArg3(x, y, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVQNE(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMOVQNE x y (InvertFlags cond))
	// result: (CMOVQNE x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVQNE)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVQNE y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVQNE _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQNE _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQNE _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQNE _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVQNE x y (TESTQ s:(Select0 blsr:(BLSRQ _)) s))
	// result: (CMOVQNE x y (Select1 <types.TypeFlags> blsr))
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64TESTQ {
			break
		}
		_ = v_2.Args[1]
		v_2_0 := v_2.Args[0]
		v_2_1 := v_2.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_2_0, v_2_1 = _i0+1, v_2_1, v_2_0 {
			s := v_2_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRQ || s != v_2_1 {
				continue
			}
			v.reset(OpAMD64CMOVQNE)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg3(x, y, v0)
			return true
		}
		break
	}
	// match: (CMOVQNE x y (TESTL s:(Select0 blsr:(BLSRL _)) s))
	// result: (CMOVQNE x y (Select1 <types.TypeFlags> blsr))
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64TESTL {
			break
		}
		_ = v_2.Args[1]
		v_2_0 := v_2.Args[0]
		v_2_1 := v_2.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_2_0, v_2_1 = _i0+1, v_2_1, v_2_0 {
			s := v_2_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRL || s != v_2_1 {
				continue
			}
			v.reset(OpAMD64CMOVQNE)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg3(x, y, v0)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVWCC(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVWCC x y (InvertFlags cond))
	// result: (CMOVWLS x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVWLS)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVWCC _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWCC _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWCC y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWCC y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWCC _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVWCS(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVWCS x y (InvertFlags cond))
	// result: (CMOVWHI x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVWHI)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVWCS y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWCS y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWCS _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWCS _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWCS y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVWEQ(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVWEQ x y (InvertFlags cond))
	// result: (CMOVWEQ x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVWEQ)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVWEQ _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWEQ y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWEQ y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWEQ y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWEQ y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVWGE(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVWGE x y (InvertFlags cond))
	// result: (CMOVWLE x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVWLE)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVWGE _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWGE _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWGE _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWGE y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWGE y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVWGT(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVWGT x y (InvertFlags cond))
	// result: (CMOVWLT x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVWLT)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVWGT y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWGT _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWGT _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWGT y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWGT y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVWHI(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVWHI x y (InvertFlags cond))
	// result: (CMOVWCS x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVWCS)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVWHI y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWHI _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWHI y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWHI y _ (FlagLT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWHI _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVWLE(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVWLE x y (InvertFlags cond))
	// result: (CMOVWGE x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVWGE)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVWLE _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWLE y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWLE y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWLE _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWLE _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVWLS(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVWLS x y (InvertFlags cond))
	// result: (CMOVWCC x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVWCC)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVWLS _ x (FlagEQ))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWLS y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWLS _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWLS _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWLS y _ (FlagLT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVWLT(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVWLT x y (InvertFlags cond))
	// result: (CMOVWGT x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVWGT)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVWLT y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWLT y _ (FlagGT_UGT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWLT y _ (FlagGT_ULT))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWLT _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWLT _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMOVWNE(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMOVWNE x y (InvertFlags cond))
	// result: (CMOVWNE x y cond)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64InvertFlags {
			break
		}
		cond := v_2.Args[0]
		v.reset(OpAMD64CMOVWNE)
		v.AddArg3(x, y, cond)
		return true
	}
	// match: (CMOVWNE y _ (FlagEQ))
	// result: y
	for {
		y := v_0
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.copyOf(y)
		return true
	}
	// match: (CMOVWNE _ x (FlagGT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWNE _ x (FlagGT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWNE _ x (FlagLT_ULT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (CMOVWNE _ x (FlagLT_UGT))
	// result: x
	for {
		x := v_1
		if v_2.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPB(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMPB x (MOVLconst [c]))
	// result: (CMPBconst x [int8(c)])
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64CMPBconst)
		v.AuxInt = int8ToAuxInt(int8(c))
		v.AddArg(x)
		return true
	}
	// match: (CMPB (MOVLconst [c]) x)
	// result: (InvertFlags (CMPBconst x [int8(c)]))
	for {
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_1
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v0.AuxInt = int8ToAuxInt(int8(c))
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (CMPB x y)
	// cond: canonLessThan(x,y)
	// result: (InvertFlags (CMPB y x))
	for {
		x := v_0
		y := v_1
		if !(canonLessThan(x, y)) {
			break
		}
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPB, types.TypeFlags)
		v0.AddArg2(y, x)
		v.AddArg(v0)
		return true
	}
	// match: (CMPB l:(MOVBload {sym} [off] ptr mem) x)
	// cond: canMergeLoad(v, l) && clobber(l)
	// result: (CMPBload {sym} [off] ptr x mem)
	for {
		l := v_0
		if l.Op != OpAMD64MOVBload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		x := v_1
		if !(canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64CMPBload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (CMPB x l:(MOVBload {sym} [off] ptr mem))
	// cond: canMergeLoad(v, l) && clobber(l)
	// result: (InvertFlags (CMPBload {sym} [off] ptr x mem))
	for {
		x := v_0
		l := v_1
		if l.Op != OpAMD64MOVBload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(l.Pos, OpAMD64CMPBload, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg3(ptr, x, mem)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPBconst(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMPBconst (MOVLconst [x]) [y])
	// cond: int8(x)==y
	// result: (FlagEQ)
	for {
		y := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(int8(x) == y) {
			break
		}
		v.reset(OpAMD64FlagEQ)
		return true
	}
	// match: (CMPBconst (MOVLconst [x]) [y])
	// cond: int8(x)<y && uint8(x)<uint8(y)
	// result: (FlagLT_ULT)
	for {
		y := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(int8(x) < y && uint8(x) < uint8(y)) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPBconst (MOVLconst [x]) [y])
	// cond: int8(x)<y && uint8(x)>uint8(y)
	// result: (FlagLT_UGT)
	for {
		y := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(int8(x) < y && uint8(x) > uint8(y)) {
			break
		}
		v.reset(OpAMD64FlagLT_UGT)
		return true
	}
	// match: (CMPBconst (MOVLconst [x]) [y])
	// cond: int8(x)>y && uint8(x)<uint8(y)
	// result: (FlagGT_ULT)
	for {
		y := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(int8(x) > y && uint8(x) < uint8(y)) {
			break
		}
		v.reset(OpAMD64FlagGT_ULT)
		return true
	}
	// match: (CMPBconst (MOVLconst [x]) [y])
	// cond: int8(x)>y && uint8(x)>uint8(y)
	// result: (FlagGT_UGT)
	for {
		y := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(int8(x) > y && uint8(x) > uint8(y)) {
			break
		}
		v.reset(OpAMD64FlagGT_UGT)
		return true
	}
	// match: (CMPBconst (ANDLconst _ [m]) [n])
	// cond: 0 <= int8(m) && int8(m) < n
	// result: (FlagLT_ULT)
	for {
		n := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64ANDLconst {
			break
		}
		m := auxIntToInt32(v_0.AuxInt)
		if !(0 <= int8(m) && int8(m) < n) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPBconst a:(ANDL x y) [0])
	// cond: a.Uses == 1
	// result: (TESTB x y)
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		a := v_0
		if a.Op != OpAMD64ANDL {
			break
		}
		y := a.Args[1]
		x := a.Args[0]
		if !(a.Uses == 1) {
			break
		}
		v.reset(OpAMD64TESTB)
		v.AddArg2(x, y)
		return true
	}
	// match: (CMPBconst a:(ANDLconst [c] x) [0])
	// cond: a.Uses == 1
	// result: (TESTBconst [int8(c)] x)
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		a := v_0
		if a.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		x := a.Args[0]
		if !(a.Uses == 1) {
			break
		}
		v.reset(OpAMD64TESTBconst)
		v.AuxInt = int8ToAuxInt(int8(c))
		v.AddArg(x)
		return true
	}
	// match: (CMPBconst x [0])
	// result: (TESTB x x)
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.reset(OpAMD64TESTB)
		v.AddArg2(x, x)
		return true
	}
	// match: (CMPBconst l:(MOVBload {sym} [off] ptr mem) [c])
	// cond: l.Uses == 1 && clobber(l)
	// result: @l.Block (CMPBconstload {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		c := auxIntToInt8(v.AuxInt)
		l := v_0
		if l.Op != OpAMD64MOVBload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(l.Uses == 1 && clobber(l)) {
			break
		}
		b = l.Block
		v0 := b.NewValue0(l.Pos, OpAMD64CMPBconstload, types.TypeFlags)
		v.copyOf(v0)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPBconstload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMPBconstload [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (CMPBconstload [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64CMPBconstload)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (CMPBconstload [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (CMPBconstload [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64CMPBconstload)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPBload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMPBload [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (CMPBload [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64CMPBload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (CMPBload [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (CMPBload [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64CMPBload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (CMPBload {sym} [off] ptr (MOVLconst [c]) mem)
	// result: (CMPBconstload {sym} [makeValAndOff(int32(int8(c)),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64CMPBconstload)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(int8(c)), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMPL x (MOVLconst [c]))
	// result: (CMPLconst x [c])
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64CMPLconst)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	// match: (CMPL (MOVLconst [c]) x)
	// result: (InvertFlags (CMPLconst x [c]))
	for {
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_1
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(c)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (CMPL x y)
	// cond: canonLessThan(x,y)
	// result: (InvertFlags (CMPL y x))
	for {
		x := v_0
		y := v_1
		if !(canonLessThan(x, y)) {
			break
		}
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPL, types.TypeFlags)
		v0.AddArg2(y, x)
		v.AddArg(v0)
		return true
	}
	// match: (CMPL l:(MOVLload {sym} [off] ptr mem) x)
	// cond: canMergeLoad(v, l) && clobber(l)
	// result: (CMPLload {sym} [off] ptr x mem)
	for {
		l := v_0
		if l.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		x := v_1
		if !(canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64CMPLload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (CMPL x l:(MOVLload {sym} [off] ptr mem))
	// cond: canMergeLoad(v, l) && clobber(l)
	// result: (InvertFlags (CMPLload {sym} [off] ptr x mem))
	for {
		x := v_0
		l := v_1
		if l.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(l.Pos, OpAMD64CMPLload, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg3(ptr, x, mem)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPLconst(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMPLconst (MOVLconst [x]) [y])
	// cond: x==y
	// result: (FlagEQ)
	for {
		y := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(x == y) {
			break
		}
		v.reset(OpAMD64FlagEQ)
		return true
	}
	// match: (CMPLconst (MOVLconst [x]) [y])
	// cond: x<y && uint32(x)<uint32(y)
	// result: (FlagLT_ULT)
	for {
		y := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(x < y && uint32(x) < uint32(y)) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPLconst (MOVLconst [x]) [y])
	// cond: x<y && uint32(x)>uint32(y)
	// result: (FlagLT_UGT)
	for {
		y := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(x < y && uint32(x) > uint32(y)) {
			break
		}
		v.reset(OpAMD64FlagLT_UGT)
		return true
	}
	// match: (CMPLconst (MOVLconst [x]) [y])
	// cond: x>y && uint32(x)<uint32(y)
	// result: (FlagGT_ULT)
	for {
		y := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(x > y && uint32(x) < uint32(y)) {
			break
		}
		v.reset(OpAMD64FlagGT_ULT)
		return true
	}
	// match: (CMPLconst (MOVLconst [x]) [y])
	// cond: x>y && uint32(x)>uint32(y)
	// result: (FlagGT_UGT)
	for {
		y := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(x > y && uint32(x) > uint32(y)) {
			break
		}
		v.reset(OpAMD64FlagGT_UGT)
		return true
	}
	// match: (CMPLconst (SHRLconst _ [c]) [n])
	// cond: 0 <= n && 0 < c && c <= 32 && (1<<uint64(32-c)) <= uint64(n)
	// result: (FlagLT_ULT)
	for {
		n := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64SHRLconst {
			break
		}
		c := auxIntToInt8(v_0.AuxInt)
		if !(0 <= n && 0 < c && c <= 32 && (1<<uint64(32-c)) <= uint64(n)) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPLconst (ANDLconst _ [m]) [n])
	// cond: 0 <= m && m < n
	// result: (FlagLT_ULT)
	for {
		n := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ANDLconst {
			break
		}
		m := auxIntToInt32(v_0.AuxInt)
		if !(0 <= m && m < n) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPLconst a:(ANDL x y) [0])
	// cond: a.Uses == 1
	// result: (TESTL x y)
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		a := v_0
		if a.Op != OpAMD64ANDL {
			break
		}
		y := a.Args[1]
		x := a.Args[0]
		if !(a.Uses == 1) {
			break
		}
		v.reset(OpAMD64TESTL)
		v.AddArg2(x, y)
		return true
	}
	// match: (CMPLconst a:(ANDLconst [c] x) [0])
	// cond: a.Uses == 1
	// result: (TESTLconst [c] x)
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		a := v_0
		if a.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		x := a.Args[0]
		if !(a.Uses == 1) {
			break
		}
		v.reset(OpAMD64TESTLconst)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	// match: (CMPLconst x [0])
	// result: (TESTL x x)
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.reset(OpAMD64TESTL)
		v.AddArg2(x, x)
		return true
	}
	// match: (CMPLconst l:(MOVLload {sym} [off] ptr mem) [c])
	// cond: l.Uses == 1 && clobber(l)
	// result: @l.Block (CMPLconstload {sym} [makeValAndOff(c,off)] ptr mem)
	for {
		c := auxIntToInt32(v.AuxInt)
		l := v_0
		if l.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(l.Uses == 1 && clobber(l)) {
			break
		}
		b = l.Block
		v0 := b.NewValue0(l.Pos, OpAMD64CMPLconstload, types.TypeFlags)
		v.copyOf(v0)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(c, off))
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPLconstload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMPLconstload [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (CMPLconstload [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64CMPLconstload)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (CMPLconstload [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (CMPLconstload [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64CMPLconstload)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPLload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMPLload [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (CMPLload [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64CMPLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (CMPLload [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (CMPLload [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64CMPLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (CMPLload {sym} [off] ptr (MOVLconst [c]) mem)
	// result: (CMPLconstload {sym} [makeValAndOff(c,off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64CMPLconstload)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(c, off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMPQ x (MOVQconst [c]))
	// cond: is32Bit(c)
	// result: (CMPQconst x [int32(c)])
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		if !(is32Bit(c)) {
			break
		}
		v.reset(OpAMD64CMPQconst)
		v.AuxInt = int32ToAuxInt(int32(c))
		v.AddArg(x)
		return true
	}
	// match: (CMPQ (MOVQconst [c]) x)
	// cond: is32Bit(c)
	// result: (InvertFlags (CMPQconst x [int32(c)]))
	for {
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_0.AuxInt)
		x := v_1
		if !(is32Bit(c)) {
			break
		}
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(int32(c))
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (CMPQ x y)
	// cond: canonLessThan(x,y)
	// result: (InvertFlags (CMPQ y x))
	for {
		x := v_0
		y := v_1
		if !(canonLessThan(x, y)) {
			break
		}
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v0.AddArg2(y, x)
		v.AddArg(v0)
		return true
	}
	// match: (CMPQ (MOVQconst [x]) (MOVQconst [y]))
	// cond: x==y
	// result: (FlagEQ)
	for {
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		x := auxIntToInt64(v_0.AuxInt)
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		y := auxIntToInt64(v_1.AuxInt)
		if !(x == y) {
			break
		}
		v.reset(OpAMD64FlagEQ)
		return true
	}
	// match: (CMPQ (MOVQconst [x]) (MOVQconst [y]))
	// cond: x<y && uint64(x)<uint64(y)
	// result: (FlagLT_ULT)
	for {
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		x := auxIntToInt64(v_0.AuxInt)
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		y := auxIntToInt64(v_1.AuxInt)
		if !(x < y && uint64(x) < uint64(y)) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPQ (MOVQconst [x]) (MOVQconst [y]))
	// cond: x<y && uint64(x)>uint64(y)
	// result: (FlagLT_UGT)
	for {
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		x := auxIntToInt64(v_0.AuxInt)
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		y := auxIntToInt64(v_1.AuxInt)
		if !(x < y && uint64(x) > uint64(y)) {
			break
		}
		v.reset(OpAMD64FlagLT_UGT)
		return true
	}
	// match: (CMPQ (MOVQconst [x]) (MOVQconst [y]))
	// cond: x>y && uint64(x)<uint64(y)
	// result: (FlagGT_ULT)
	for {
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		x := auxIntToInt64(v_0.AuxInt)
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		y := auxIntToInt64(v_1.AuxInt)
		if !(x > y && uint64(x) < uint64(y)) {
			break
		}
		v.reset(OpAMD64FlagGT_ULT)
		return true
	}
	// match: (CMPQ (MOVQconst [x]) (MOVQconst [y]))
	// cond: x>y && uint64(x)>uint64(y)
	// result: (FlagGT_UGT)
	for {
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		x := auxIntToInt64(v_0.AuxInt)
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		y := auxIntToInt64(v_1.AuxInt)
		if !(x > y && uint64(x) > uint64(y)) {
			break
		}
		v.reset(OpAMD64FlagGT_UGT)
		return true
	}
	// match: (CMPQ l:(MOVQload {sym} [off] ptr mem) x)
	// cond: canMergeLoad(v, l) && clobber(l)
	// result: (CMPQload {sym} [off] ptr x mem)
	for {
		l := v_0
		if l.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		x := v_1
		if !(canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64CMPQload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (CMPQ x l:(MOVQload {sym} [off] ptr mem))
	// cond: canMergeLoad(v, l) && clobber(l)
	// result: (InvertFlags (CMPQload {sym} [off] ptr x mem))
	for {
		x := v_0
		l := v_1
		if l.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(l.Pos, OpAMD64CMPQload, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg3(ptr, x, mem)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPQconst(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMPQconst (MOVQconst [x]) [y])
	// cond: x==int64(y)
	// result: (FlagEQ)
	for {
		y := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		x := auxIntToInt64(v_0.AuxInt)
		if !(x == int64(y)) {
			break
		}
		v.reset(OpAMD64FlagEQ)
		return true
	}
	// match: (CMPQconst (MOVQconst [x]) [y])
	// cond: x<int64(y) && uint64(x)<uint64(int64(y))
	// result: (FlagLT_ULT)
	for {
		y := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		x := auxIntToInt64(v_0.AuxInt)
		if !(x < int64(y) && uint64(x) < uint64(int64(y))) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPQconst (MOVQconst [x]) [y])
	// cond: x<int64(y) && uint64(x)>uint64(int64(y))
	// result: (FlagLT_UGT)
	for {
		y := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		x := auxIntToInt64(v_0.AuxInt)
		if !(x < int64(y) && uint64(x) > uint64(int64(y))) {
			break
		}
		v.reset(OpAMD64FlagLT_UGT)
		return true
	}
	// match: (CMPQconst (MOVQconst [x]) [y])
	// cond: x>int64(y) && uint64(x)<uint64(int64(y))
	// result: (FlagGT_ULT)
	for {
		y := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		x := auxIntToInt64(v_0.AuxInt)
		if !(x > int64(y) && uint64(x) < uint64(int64(y))) {
			break
		}
		v.reset(OpAMD64FlagGT_ULT)
		return true
	}
	// match: (CMPQconst (MOVQconst [x]) [y])
	// cond: x>int64(y) && uint64(x)>uint64(int64(y))
	// result: (FlagGT_UGT)
	for {
		y := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		x := auxIntToInt64(v_0.AuxInt)
		if !(x > int64(y) && uint64(x) > uint64(int64(y))) {
			break
		}
		v.reset(OpAMD64FlagGT_UGT)
		return true
	}
	// match: (CMPQconst (MOVBQZX _) [c])
	// cond: 0xFF < c
	// result: (FlagLT_ULT)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVBQZX || !(0xFF < c) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPQconst (MOVWQZX _) [c])
	// cond: 0xFFFF < c
	// result: (FlagLT_ULT)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVWQZX || !(0xFFFF < c) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPQconst (SHRQconst _ [c]) [n])
	// cond: 0 <= n && 0 < c && c <= 64 && (1<<uint64(64-c)) <= uint64(n)
	// result: (FlagLT_ULT)
	for {
		n := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64SHRQconst {
			break
		}
		c := auxIntToInt8(v_0.AuxInt)
		if !(0 <= n && 0 < c && c <= 64 && (1<<uint64(64-c)) <= uint64(n)) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPQconst (ANDQconst _ [m]) [n])
	// cond: 0 <= m && m < n
	// result: (FlagLT_ULT)
	for {
		n := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ANDQconst {
			break
		}
		m := auxIntToInt32(v_0.AuxInt)
		if !(0 <= m && m < n) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPQconst (ANDLconst _ [m]) [n])
	// cond: 0 <= m && m < n
	// result: (FlagLT_ULT)
	for {
		n := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ANDLconst {
			break
		}
		m := auxIntToInt32(v_0.AuxInt)
		if !(0 <= m && m < n) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPQconst a:(ANDQ x y) [0])
	// cond: a.Uses == 1
	// result: (TESTQ x y)
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		a := v_0
		if a.Op != OpAMD64ANDQ {
			break
		}
		y := a.Args[1]
		x := a.Args[0]
		if !(a.Uses == 1) {
			break
		}
		v.reset(OpAMD64TESTQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (CMPQconst a:(ANDQconst [c] x) [0])
	// cond: a.Uses == 1
	// result: (TESTQconst [c] x)
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		a := v_0
		if a.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		x := a.Args[0]
		if !(a.Uses == 1) {
			break
		}
		v.reset(OpAMD64TESTQconst)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	// match: (CMPQconst x [0])
	// result: (TESTQ x x)
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.reset(OpAMD64TESTQ)
		v.AddArg2(x, x)
		return true
	}
	// match: (CMPQconst l:(MOVQload {sym} [off] ptr mem) [c])
	// cond: l.Uses == 1 && clobber(l)
	// result: @l.Block (CMPQconstload {sym} [makeValAndOff(c,off)] ptr mem)
	for {
		c := auxIntToInt32(v.AuxInt)
		l := v_0
		if l.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(l.Uses == 1 && clobber(l)) {
			break
		}
		b = l.Block
		v0 := b.NewValue0(l.Pos, OpAMD64CMPQconstload, types.TypeFlags)
		v.copyOf(v0)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(c, off))
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPQconstload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMPQconstload [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (CMPQconstload [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64CMPQconstload)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (CMPQconstload [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (CMPQconstload [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64CMPQconstload)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPQload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMPQload [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (CMPQload [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64CMPQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (CMPQload [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (CMPQload [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64CMPQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (CMPQload {sym} [off] ptr (MOVQconst [c]) mem)
	// cond: validVal(c)
	// result: (CMPQconstload {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		mem := v_2
		if !(validVal(c)) {
			break
		}
		v.reset(OpAMD64CMPQconstload)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPW(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMPW x (MOVLconst [c]))
	// result: (CMPWconst x [int16(c)])
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64CMPWconst)
		v.AuxInt = int16ToAuxInt(int16(c))
		v.AddArg(x)
		return true
	}
	// match: (CMPW (MOVLconst [c]) x)
	// result: (InvertFlags (CMPWconst x [int16(c)]))
	for {
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_1
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v0.AuxInt = int16ToAuxInt(int16(c))
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (CMPW x y)
	// cond: canonLessThan(x,y)
	// result: (InvertFlags (CMPW y x))
	for {
		x := v_0
		y := v_1
		if !(canonLessThan(x, y)) {
			break
		}
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPW, types.TypeFlags)
		v0.AddArg2(y, x)
		v.AddArg(v0)
		return true
	}
	// match: (CMPW l:(MOVWload {sym} [off] ptr mem) x)
	// cond: canMergeLoad(v, l) && clobber(l)
	// result: (CMPWload {sym} [off] ptr x mem)
	for {
		l := v_0
		if l.Op != OpAMD64MOVWload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		x := v_1
		if !(canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64CMPWload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (CMPW x l:(MOVWload {sym} [off] ptr mem))
	// cond: canMergeLoad(v, l) && clobber(l)
	// result: (InvertFlags (CMPWload {sym} [off] ptr x mem))
	for {
		x := v_0
		l := v_1
		if l.Op != OpAMD64MOVWload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64InvertFlags)
		v0 := b.NewValue0(l.Pos, OpAMD64CMPWload, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg3(ptr, x, mem)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPWconst(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (CMPWconst (MOVLconst [x]) [y])
	// cond: int16(x)==y
	// result: (FlagEQ)
	for {
		y := auxIntToInt16(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(int16(x) == y) {
			break
		}
		v.reset(OpAMD64FlagEQ)
		return true
	}
	// match: (CMPWconst (MOVLconst [x]) [y])
	// cond: int16(x)<y && uint16(x)<uint16(y)
	// result: (FlagLT_ULT)
	for {
		y := auxIntToInt16(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(int16(x) < y && uint16(x) < uint16(y)) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPWconst (MOVLconst [x]) [y])
	// cond: int16(x)<y && uint16(x)>uint16(y)
	// result: (FlagLT_UGT)
	for {
		y := auxIntToInt16(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(int16(x) < y && uint16(x) > uint16(y)) {
			break
		}
		v.reset(OpAMD64FlagLT_UGT)
		return true
	}
	// match: (CMPWconst (MOVLconst [x]) [y])
	// cond: int16(x)>y && uint16(x)<uint16(y)
	// result: (FlagGT_ULT)
	for {
		y := auxIntToInt16(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(int16(x) > y && uint16(x) < uint16(y)) {
			break
		}
		v.reset(OpAMD64FlagGT_ULT)
		return true
	}
	// match: (CMPWconst (MOVLconst [x]) [y])
	// cond: int16(x)>y && uint16(x)>uint16(y)
	// result: (FlagGT_UGT)
	for {
		y := auxIntToInt16(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		x := auxIntToInt32(v_0.AuxInt)
		if !(int16(x) > y && uint16(x) > uint16(y)) {
			break
		}
		v.reset(OpAMD64FlagGT_UGT)
		return true
	}
	// match: (CMPWconst (ANDLconst _ [m]) [n])
	// cond: 0 <= int16(m) && int16(m) < n
	// result: (FlagLT_ULT)
	for {
		n := auxIntToInt16(v.AuxInt)
		if v_0.Op != OpAMD64ANDLconst {
			break
		}
		m := auxIntToInt32(v_0.AuxInt)
		if !(0 <= int16(m) && int16(m) < n) {
			break
		}
		v.reset(OpAMD64FlagLT_ULT)
		return true
	}
	// match: (CMPWconst a:(ANDL x y) [0])
	// cond: a.Uses == 1
	// result: (TESTW x y)
	for {
		if auxIntToInt16(v.AuxInt) != 0 {
			break
		}
		a := v_0
		if a.Op != OpAMD64ANDL {
			break
		}
		y := a.Args[1]
		x := a.Args[0]
		if !(a.Uses == 1) {
			break
		}
		v.reset(OpAMD64TESTW)
		v.AddArg2(x, y)
		return true
	}
	// match: (CMPWconst a:(ANDLconst [c] x) [0])
	// cond: a.Uses == 1
	// result: (TESTWconst [int16(c)] x)
	for {
		if auxIntToInt16(v.AuxInt) != 0 {
			break
		}
		a := v_0
		if a.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		x := a.Args[0]
		if !(a.Uses == 1) {
			break
		}
		v.reset(OpAMD64TESTWconst)
		v.AuxInt = int16ToAuxInt(int16(c))
		v.AddArg(x)
		return true
	}
	// match: (CMPWconst x [0])
	// result: (TESTW x x)
	for {
		if auxIntToInt16(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.reset(OpAMD64TESTW)
		v.AddArg2(x, x)
		return true
	}
	// match: (CMPWconst l:(MOVWload {sym} [off] ptr mem) [c])
	// cond: l.Uses == 1 && clobber(l)
	// result: @l.Block (CMPWconstload {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		c := auxIntToInt16(v.AuxInt)
		l := v_0
		if l.Op != OpAMD64MOVWload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(l.Uses == 1 && clobber(l)) {
			break
		}
		b = l.Block
		v0 := b.NewValue0(l.Pos, OpAMD64CMPWconstload, types.TypeFlags)
		v.copyOf(v0)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPWconstload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMPWconstload [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (CMPWconstload [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64CMPWconstload)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (CMPWconstload [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (CMPWconstload [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64CMPWconstload)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPWload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMPWload [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (CMPWload [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64CMPWload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (CMPWload [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (CMPWload [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64CMPWload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (CMPWload {sym} [off] ptr (MOVLconst [c]) mem)
	// result: (CMPWconstload {sym} [makeValAndOff(int32(int16(c)),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64CMPWconstload)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(int16(c)), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPXCHGLlock(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMPXCHGLlock [off1] {sym} (ADDQconst [off2] ptr) old new_ mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (CMPXCHGLlock [off1+off2] {sym} ptr old new_ mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		old := v_1
		new_ := v_2
		mem := v_3
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64CMPXCHGLlock)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg4(ptr, old, new_, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64CMPXCHGQlock(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (CMPXCHGQlock [off1] {sym} (ADDQconst [off2] ptr) old new_ mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (CMPXCHGQlock [off1+off2] {sym} ptr old new_ mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		old := v_1
		new_ := v_2
		mem := v_3
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64CMPXCHGQlock)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg4(ptr, old, new_, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64DIVSD(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (DIVSD x l:(MOVSDload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (DIVSDload x [off] {sym} ptr mem)
	for {
		x := v_0
		l := v_1
		if l.Op != OpAMD64MOVSDload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
			break
		}
		v.reset(OpAMD64DIVSDload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(x, ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64DIVSDload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (DIVSDload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (DIVSDload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64DIVSDload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (DIVSDload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (DIVSDload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64DIVSDload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64DIVSS(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (DIVSS x l:(MOVSSload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (DIVSSload x [off] {sym} ptr mem)
	for {
		x := v_0
		l := v_1
		if l.Op != OpAMD64MOVSSload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
			break
		}
		v.reset(OpAMD64DIVSSload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(x, ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64DIVSSload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (DIVSSload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (DIVSSload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64DIVSSload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (DIVSSload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (DIVSSload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64DIVSSload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64HMULL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (HMULL x y)
	// cond: !x.rematerializeable() && y.rematerializeable()
	// result: (HMULL y x)
	for {
		x := v_0
		y := v_1
		if !(!x.rematerializeable() && y.rematerializeable()) {
			break
		}
		v.reset(OpAMD64HMULL)
		v.AddArg2(y, x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64HMULLU(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (HMULLU x y)
	// cond: !x.rematerializeable() && y.rematerializeable()
	// result: (HMULLU y x)
	for {
		x := v_0
		y := v_1
		if !(!x.rematerializeable() && y.rematerializeable()) {
			break
		}
		v.reset(OpAMD64HMULLU)
		v.AddArg2(y, x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64HMULQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (HMULQ x y)
	// cond: !x.rematerializeable() && y.rematerializeable()
	// result: (HMULQ y x)
	for {
		x := v_0
		y := v_1
		if !(!x.rematerializeable() && y.rematerializeable()) {
			break
		}
		v.reset(OpAMD64HMULQ)
		v.AddArg2(y, x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64HMULQU(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (HMULQU x y)
	// cond: !x.rematerializeable() && y.rematerializeable()
	// result: (HMULQU y x)
	for {
		x := v_0
		y := v_1
		if !(!x.rematerializeable() && y.rematerializeable()) {
			break
		}
		v.reset(OpAMD64HMULQU)
		v.AddArg2(y, x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64LEAL(v *Value) bool {
	v_0 := v.Args[0]
	// match: (LEAL [c] {s} (ADDLconst [d] x))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAL [c+d] {s} x)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAL)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg(x)
		return true
	}
	// match: (LEAL [c] {s} (ADDL x y))
	// cond: x.Op != OpSB && y.Op != OpSB
	// result: (LEAL1 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDL {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			x := v_0_0
			y := v_0_1
			if !(x.Op != OpSB && y.Op != OpSB) {
				continue
			}
			v.reset(OpAMD64LEAL1)
			v.AuxInt = int32ToAuxInt(c)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64LEAL1(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LEAL1 [c] {s} (ADDLconst [d] x) y)
	// cond: is32Bit(int64(c)+int64(d)) && x.Op != OpSB
	// result: (LEAL1 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64ADDLconst {
				continue
			}
			d := auxIntToInt32(v_0.AuxInt)
			x := v_0.Args[0]
			y := v_1
			if !(is32Bit(int64(c)+int64(d)) && x.Op != OpSB) {
				continue
			}
			v.reset(OpAMD64LEAL1)
			v.AuxInt = int32ToAuxInt(c + d)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (LEAL1 [c] {s} x z:(ADDL y y))
	// cond: x != z
	// result: (LEAL2 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			z := v_1
			if z.Op != OpAMD64ADDL {
				continue
			}
			y := z.Args[1]
			if y != z.Args[0] || !(x != z) {
				continue
			}
			v.reset(OpAMD64LEAL2)
			v.AuxInt = int32ToAuxInt(c)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (LEAL1 [c] {s} x (SHLLconst [2] y))
	// result: (LEAL4 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64SHLLconst || auxIntToInt8(v_1.AuxInt) != 2 {
				continue
			}
			y := v_1.Args[0]
			v.reset(OpAMD64LEAL4)
			v.AuxInt = int32ToAuxInt(c)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (LEAL1 [c] {s} x (SHLLconst [3] y))
	// result: (LEAL8 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64SHLLconst || auxIntToInt8(v_1.AuxInt) != 3 {
				continue
			}
			y := v_1.Args[0]
			v.reset(OpAMD64LEAL8)
			v.AuxInt = int32ToAuxInt(c)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64LEAL2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LEAL2 [c] {s} (ADDLconst [d] x) y)
	// cond: is32Bit(int64(c)+int64(d)) && x.Op != OpSB
	// result: (LEAL2 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		y := v_1
		if !(is32Bit(int64(c)+int64(d)) && x.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAL2)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAL2 [c] {s} x (ADDLconst [d] y))
	// cond: is32Bit(int64(c)+2*int64(d)) && y.Op != OpSB
	// result: (LEAL2 [c+2*d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64ADDLconst {
			break
		}
		d := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(is32Bit(int64(c)+2*int64(d)) && y.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAL2)
		v.AuxInt = int32ToAuxInt(c + 2*d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAL2 [c] {s} x z:(ADDL y y))
	// cond: x != z
	// result: (LEAL4 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		z := v_1
		if z.Op != OpAMD64ADDL {
			break
		}
		y := z.Args[1]
		if y != z.Args[0] || !(x != z) {
			break
		}
		v.reset(OpAMD64LEAL4)
		v.AuxInt = int32ToAuxInt(c)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAL2 [c] {s} x (SHLLconst [2] y))
	// result: (LEAL8 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64SHLLconst || auxIntToInt8(v_1.AuxInt) != 2 {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64LEAL8)
		v.AuxInt = int32ToAuxInt(c)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAL2 [0] {s} (ADDL x x) x)
	// cond: s == nil
	// result: (SHLLconst [2] x)
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDL {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] || x != v_1 || !(s == nil) {
			break
		}
		v.reset(OpAMD64SHLLconst)
		v.AuxInt = int8ToAuxInt(2)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64LEAL4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LEAL4 [c] {s} (ADDLconst [d] x) y)
	// cond: is32Bit(int64(c)+int64(d)) && x.Op != OpSB
	// result: (LEAL4 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		y := v_1
		if !(is32Bit(int64(c)+int64(d)) && x.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAL4)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAL4 [c] {s} x (ADDLconst [d] y))
	// cond: is32Bit(int64(c)+4*int64(d)) && y.Op != OpSB
	// result: (LEAL4 [c+4*d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64ADDLconst {
			break
		}
		d := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(is32Bit(int64(c)+4*int64(d)) && y.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAL4)
		v.AuxInt = int32ToAuxInt(c + 4*d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAL4 [c] {s} x z:(ADDL y y))
	// cond: x != z
	// result: (LEAL8 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		z := v_1
		if z.Op != OpAMD64ADDL {
			break
		}
		y := z.Args[1]
		if y != z.Args[0] || !(x != z) {
			break
		}
		v.reset(OpAMD64LEAL8)
		v.AuxInt = int32ToAuxInt(c)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64LEAL8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LEAL8 [c] {s} (ADDLconst [d] x) y)
	// cond: is32Bit(int64(c)+int64(d)) && x.Op != OpSB
	// result: (LEAL8 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		y := v_1
		if !(is32Bit(int64(c)+int64(d)) && x.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAL8)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAL8 [c] {s} x (ADDLconst [d] y))
	// cond: is32Bit(int64(c)+8*int64(d)) && y.Op != OpSB
	// result: (LEAL8 [c+8*d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64ADDLconst {
			break
		}
		d := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(is32Bit(int64(c)+8*int64(d)) && y.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAL8)
		v.AuxInt = int32ToAuxInt(c + 8*d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64LEAQ(v *Value) bool {
	v_0 := v.Args[0]
	// match: (LEAQ [c] {s} (ADDQconst [d] x))
	// cond: is32Bit(int64(c)+int64(d))
	// result: (LEAQ [c+d] {s} x)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(is32Bit(int64(c) + int64(d))) {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg(x)
		return true
	}
	// match: (LEAQ [c] {s} (ADDQ x y))
	// cond: x.Op != OpSB && y.Op != OpSB
	// result: (LEAQ1 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			x := v_0_0
			y := v_0_1
			if !(x.Op != OpSB && y.Op != OpSB) {
				continue
			}
			v.reset(OpAMD64LEAQ1)
			v.AuxInt = int32ToAuxInt(c)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (LEAQ [off1] {sym1} (LEAQ [off2] {sym2} x))
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (LEAQ [off1+off2] {mergeSym(sym1,sym2)} x)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		x := v_0.Args[0]
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg(x)
		return true
	}
	// match: (LEAQ [off1] {sym1} (LEAQ1 [off2] {sym2} x y))
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (LEAQ1 [off1+off2] {mergeSym(sym1,sym2)} x y)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ1 {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64LEAQ1)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ [off1] {sym1} (LEAQ2 [off2] {sym2} x y))
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (LEAQ2 [off1+off2] {mergeSym(sym1,sym2)} x y)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ2 {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64LEAQ2)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ [off1] {sym1} (LEAQ4 [off2] {sym2} x y))
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (LEAQ4 [off1+off2] {mergeSym(sym1,sym2)} x y)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ4 {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64LEAQ4)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ [off1] {sym1} (LEAQ8 [off2] {sym2} x y))
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (LEAQ8 [off1+off2] {mergeSym(sym1,sym2)} x y)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ8 {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		y := v_0.Args[1]
		x := v_0.Args[0]
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64LEAQ8)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64LEAQ1(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LEAQ1 [c] {s} (ADDQconst [d] x) y)
	// cond: is32Bit(int64(c)+int64(d)) && x.Op != OpSB
	// result: (LEAQ1 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64ADDQconst {
				continue
			}
			d := auxIntToInt32(v_0.AuxInt)
			x := v_0.Args[0]
			y := v_1
			if !(is32Bit(int64(c)+int64(d)) && x.Op != OpSB) {
				continue
			}
			v.reset(OpAMD64LEAQ1)
			v.AuxInt = int32ToAuxInt(c + d)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (LEAQ1 [c] {s} x z:(ADDQ y y))
	// cond: x != z
	// result: (LEAQ2 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			z := v_1
			if z.Op != OpAMD64ADDQ {
				continue
			}
			y := z.Args[1]
			if y != z.Args[0] || !(x != z) {
				continue
			}
			v.reset(OpAMD64LEAQ2)
			v.AuxInt = int32ToAuxInt(c)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (LEAQ1 [c] {s} x (SHLQconst [2] y))
	// result: (LEAQ4 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64SHLQconst || auxIntToInt8(v_1.AuxInt) != 2 {
				continue
			}
			y := v_1.Args[0]
			v.reset(OpAMD64LEAQ4)
			v.AuxInt = int32ToAuxInt(c)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (LEAQ1 [c] {s} x (SHLQconst [3] y))
	// result: (LEAQ8 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64SHLQconst || auxIntToInt8(v_1.AuxInt) != 3 {
				continue
			}
			y := v_1.Args[0]
			v.reset(OpAMD64LEAQ8)
			v.AuxInt = int32ToAuxInt(c)
			v.Aux = symToAux(s)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (LEAQ1 [off1] {sym1} (LEAQ [off2] {sym2} x) y)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && x.Op != OpSB
	// result: (LEAQ1 [off1+off2] {mergeSym(sym1,sym2)} x y)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64LEAQ {
				continue
			}
			off2 := auxIntToInt32(v_0.AuxInt)
			sym2 := auxToSym(v_0.Aux)
			x := v_0.Args[0]
			y := v_1
			if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && x.Op != OpSB) {
				continue
			}
			v.reset(OpAMD64LEAQ1)
			v.AuxInt = int32ToAuxInt(off1 + off2)
			v.Aux = symToAux(mergeSym(sym1, sym2))
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (LEAQ1 [off1] {sym1} x (LEAQ1 [off2] {sym2} y y))
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (LEAQ2 [off1+off2] {mergeSym(sym1, sym2)} x y)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64LEAQ1 {
				continue
			}
			off2 := auxIntToInt32(v_1.AuxInt)
			sym2 := auxToSym(v_1.Aux)
			y := v_1.Args[1]
			if y != v_1.Args[0] || !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
				continue
			}
			v.reset(OpAMD64LEAQ2)
			v.AuxInt = int32ToAuxInt(off1 + off2)
			v.Aux = symToAux(mergeSym(sym1, sym2))
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (LEAQ1 [off1] {sym1} x (LEAQ1 [off2] {sym2} x y))
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (LEAQ2 [off1+off2] {mergeSym(sym1, sym2)} y x)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64LEAQ1 {
				continue
			}
			off2 := auxIntToInt32(v_1.AuxInt)
			sym2 := auxToSym(v_1.Aux)
			_ = v_1.Args[1]
			v_1_0 := v_1.Args[0]
			v_1_1 := v_1.Args[1]
			for _i1 := 0; _i1 <= 1; _i1, v_1_0, v_1_1 = _i1+1, v_1_1, v_1_0 {
				if x != v_1_0 {
					continue
				}
				y := v_1_1
				if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
					continue
				}
				v.reset(OpAMD64LEAQ2)
				v.AuxInt = int32ToAuxInt(off1 + off2)
				v.Aux = symToAux(mergeSym(sym1, sym2))
				v.AddArg2(y, x)
				return true
			}
		}
		break
	}
	// match: (LEAQ1 [0] x y)
	// cond: v.Aux == nil
	// result: (ADDQ x y)
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		x := v_0
		y := v_1
		if !(v.Aux == nil) {
			break
		}
		v.reset(OpAMD64ADDQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64LEAQ2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LEAQ2 [c] {s} (ADDQconst [d] x) y)
	// cond: is32Bit(int64(c)+int64(d)) && x.Op != OpSB
	// result: (LEAQ2 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		y := v_1
		if !(is32Bit(int64(c)+int64(d)) && x.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAQ2)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ2 [c] {s} x (ADDQconst [d] y))
	// cond: is32Bit(int64(c)+2*int64(d)) && y.Op != OpSB
	// result: (LEAQ2 [c+2*d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		d := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(is32Bit(int64(c)+2*int64(d)) && y.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAQ2)
		v.AuxInt = int32ToAuxInt(c + 2*d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ2 [c] {s} x z:(ADDQ y y))
	// cond: x != z
	// result: (LEAQ4 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		z := v_1
		if z.Op != OpAMD64ADDQ {
			break
		}
		y := z.Args[1]
		if y != z.Args[0] || !(x != z) {
			break
		}
		v.reset(OpAMD64LEAQ4)
		v.AuxInt = int32ToAuxInt(c)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ2 [c] {s} x (SHLQconst [2] y))
	// result: (LEAQ8 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64SHLQconst || auxIntToInt8(v_1.AuxInt) != 2 {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64LEAQ8)
		v.AuxInt = int32ToAuxInt(c)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ2 [0] {s} (ADDQ x x) x)
	// cond: s == nil
	// result: (SHLQconst [2] x)
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQ {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] || x != v_1 || !(s == nil) {
			break
		}
		v.reset(OpAMD64SHLQconst)
		v.AuxInt = int8ToAuxInt(2)
		v.AddArg(x)
		return true
	}
	// match: (LEAQ2 [off1] {sym1} (LEAQ [off2] {sym2} x) y)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && x.Op != OpSB
	// result: (LEAQ2 [off1+off2] {mergeSym(sym1,sym2)} x y)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		x := v_0.Args[0]
		y := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && x.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAQ2)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ2 [off1] {sym1} x (LEAQ1 [off2] {sym2} y y))
	// cond: is32Bit(int64(off1)+2*int64(off2)) && sym2 == nil
	// result: (LEAQ4 [off1+2*off2] {sym1} x y)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64LEAQ1 {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		y := v_1.Args[1]
		if y != v_1.Args[0] || !(is32Bit(int64(off1)+2*int64(off2)) && sym2 == nil) {
			break
		}
		v.reset(OpAMD64LEAQ4)
		v.AuxInt = int32ToAuxInt(off1 + 2*off2)
		v.Aux = symToAux(sym1)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ2 [off] {sym} x (MOVQconst [scale]))
	// cond: is32Bit(int64(off)+int64(scale)*2)
	// result: (LEAQ [off+int32(scale)*2] {sym} x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		scale := auxIntToInt64(v_1.AuxInt)
		if !(is32Bit(int64(off) + int64(scale)*2)) {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.AuxInt = int32ToAuxInt(off + int32(scale)*2)
		v.Aux = symToAux(sym)
		v.AddArg(x)
		return true
	}
	// match: (LEAQ2 [off] {sym} x (MOVLconst [scale]))
	// cond: is32Bit(int64(off)+int64(scale)*2)
	// result: (LEAQ [off+int32(scale)*2] {sym} x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		scale := auxIntToInt32(v_1.AuxInt)
		if !(is32Bit(int64(off) + int64(scale)*2)) {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.AuxInt = int32ToAuxInt(off + int32(scale)*2)
		v.Aux = symToAux(sym)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64LEAQ4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LEAQ4 [c] {s} (ADDQconst [d] x) y)
	// cond: is32Bit(int64(c)+int64(d)) && x.Op != OpSB
	// result: (LEAQ4 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		y := v_1
		if !(is32Bit(int64(c)+int64(d)) && x.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAQ4)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ4 [c] {s} x (ADDQconst [d] y))
	// cond: is32Bit(int64(c)+4*int64(d)) && y.Op != OpSB
	// result: (LEAQ4 [c+4*d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		d := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(is32Bit(int64(c)+4*int64(d)) && y.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAQ4)
		v.AuxInt = int32ToAuxInt(c + 4*d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ4 [c] {s} x z:(ADDQ y y))
	// cond: x != z
	// result: (LEAQ8 [c] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		z := v_1
		if z.Op != OpAMD64ADDQ {
			break
		}
		y := z.Args[1]
		if y != z.Args[0] || !(x != z) {
			break
		}
		v.reset(OpAMD64LEAQ8)
		v.AuxInt = int32ToAuxInt(c)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ4 [off1] {sym1} (LEAQ [off2] {sym2} x) y)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && x.Op != OpSB
	// result: (LEAQ4 [off1+off2] {mergeSym(sym1,sym2)} x y)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		x := v_0.Args[0]
		y := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && x.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAQ4)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ4 [off1] {sym1} x (LEAQ1 [off2] {sym2} y y))
	// cond: is32Bit(int64(off1)+4*int64(off2)) && sym2 == nil
	// result: (LEAQ8 [off1+4*off2] {sym1} x y)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64LEAQ1 {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		y := v_1.Args[1]
		if y != v_1.Args[0] || !(is32Bit(int64(off1)+4*int64(off2)) && sym2 == nil) {
			break
		}
		v.reset(OpAMD64LEAQ8)
		v.AuxInt = int32ToAuxInt(off1 + 4*off2)
		v.Aux = symToAux(sym1)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ4 [off] {sym} x (MOVQconst [scale]))
	// cond: is32Bit(int64(off)+int64(scale)*4)
	// result: (LEAQ [off+int32(scale)*4] {sym} x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		scale := auxIntToInt64(v_1.AuxInt)
		if !(is32Bit(int64(off) + int64(scale)*4)) {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.AuxInt = int32ToAuxInt(off + int32(scale)*4)
		v.Aux = symToAux(sym)
		v.AddArg(x)
		return true
	}
	// match: (LEAQ4 [off] {sym} x (MOVLconst [scale]))
	// cond: is32Bit(int64(off)+int64(scale)*4)
	// result: (LEAQ [off+int32(scale)*4] {sym} x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		scale := auxIntToInt32(v_1.AuxInt)
		if !(is32Bit(int64(off) + int64(scale)*4)) {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.AuxInt = int32ToAuxInt(off + int32(scale)*4)
		v.Aux = symToAux(sym)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64LEAQ8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LEAQ8 [c] {s} (ADDQconst [d] x) y)
	// cond: is32Bit(int64(c)+int64(d)) && x.Op != OpSB
	// result: (LEAQ8 [c+d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		y := v_1
		if !(is32Bit(int64(c)+int64(d)) && x.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAQ8)
		v.AuxInt = int32ToAuxInt(c + d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ8 [c] {s} x (ADDQconst [d] y))
	// cond: is32Bit(int64(c)+8*int64(d)) && y.Op != OpSB
	// result: (LEAQ8 [c+8*d] {s} x y)
	for {
		c := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		d := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(is32Bit(int64(c)+8*int64(d)) && y.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAQ8)
		v.AuxInt = int32ToAuxInt(c + 8*d)
		v.Aux = symToAux(s)
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ8 [off1] {sym1} (LEAQ [off2] {sym2} x) y)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && x.Op != OpSB
	// result: (LEAQ8 [off1+off2] {mergeSym(sym1,sym2)} x y)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		x := v_0.Args[0]
		y := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && x.Op != OpSB) {
			break
		}
		v.reset(OpAMD64LEAQ8)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(x, y)
		return true
	}
	// match: (LEAQ8 [off] {sym} x (MOVQconst [scale]))
	// cond: is32Bit(int64(off)+int64(scale)*8)
	// result: (LEAQ [off+int32(scale)*8] {sym} x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		scale := auxIntToInt64(v_1.AuxInt)
		if !(is32Bit(int64(off) + int64(scale)*8)) {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.AuxInt = int32ToAuxInt(off + int32(scale)*8)
		v.Aux = symToAux(sym)
		v.AddArg(x)
		return true
	}
	// match: (LEAQ8 [off] {sym} x (MOVLconst [scale]))
	// cond: is32Bit(int64(off)+int64(scale)*8)
	// result: (LEAQ [off+int32(scale)*8] {sym} x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		scale := auxIntToInt32(v_1.AuxInt)
		if !(is32Bit(int64(off) + int64(scale)*8)) {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.AuxInt = int32ToAuxInt(off + int32(scale)*8)
		v.Aux = symToAux(sym)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVBELstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVBELstore [i] {s} p x:(BSWAPL w) mem)
	// cond: x.Uses == 1
	// result: (MOVLstore [i] {s} p w mem)
	for {
		i := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		p := v_0
		x := v_1
		if x.Op != OpAMD64BSWAPL {
			break
		}
		w := x.Args[0]
		mem := v_2
		if !(x.Uses == 1) {
			break
		}
		v.reset(OpAMD64MOVLstore)
		v.AuxInt = int32ToAuxInt(i)
		v.Aux = symToAux(s)
		v.AddArg3(p, w, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVBEQstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVBEQstore [i] {s} p x:(BSWAPQ w) mem)
	// cond: x.Uses == 1
	// result: (MOVQstore [i] {s} p w mem)
	for {
		i := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		p := v_0
		x := v_1
		if x.Op != OpAMD64BSWAPQ {
			break
		}
		w := x.Args[0]
		mem := v_2
		if !(x.Uses == 1) {
			break
		}
		v.reset(OpAMD64MOVQstore)
		v.AuxInt = int32ToAuxInt(i)
		v.Aux = symToAux(s)
		v.AddArg3(p, w, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVBEWstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVBEWstore [i] {s} p x:(ROLWconst [8] w) mem)
	// cond: x.Uses == 1
	// result: (MOVWstore [i] {s} p w mem)
	for {
		i := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		p := v_0
		x := v_1
		if x.Op != OpAMD64ROLWconst || auxIntToInt8(x.AuxInt) != 8 {
			break
		}
		w := x.Args[0]
		mem := v_2
		if !(x.Uses == 1) {
			break
		}
		v.reset(OpAMD64MOVWstore)
		v.AuxInt = int32ToAuxInt(i)
		v.Aux = symToAux(s)
		v.AddArg3(p, w, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVBQSX(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (MOVBQSX x:(MOVBload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVBQSXload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVBload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVBQSXload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBQSX x:(MOVWload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVBQSXload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVWload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVBQSXload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBQSX x:(MOVLload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVBQSXload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVBQSXload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBQSX x:(MOVQload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVBQSXload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVBQSXload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBQSX (ANDLconst [c] x))
	// cond: c & 0x80 == 0
	// result: (ANDLconst [c & 0x7f] x)
	for {
		if v_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(c&0x80 == 0) {
			break
		}
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(c & 0x7f)
		v.AddArg(x)
		return true
	}
	// match: (MOVBQSX (MOVBQSX x))
	// result: (MOVBQSX x)
	for {
		if v_0.Op != OpAMD64MOVBQSX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVBQSX)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVBQSXload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVBQSXload [off] {sym} ptr (MOVBstore [off2] {sym2} ptr2 x _))
	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
	// result: (MOVBQSX x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVBstore {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		x := v_1.Args[1]
		ptr2 := v_1.Args[0]
		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
			break
		}
		v.reset(OpAMD64MOVBQSX)
		v.AddArg(x)
		return true
	}
	// match: (MOVBQSXload [off1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVBQSXload [off1+off2] {mergeSym(sym1,sym2)} base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVBQSXload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	// match: (MOVBQSXload [off] {sym} (SB) _)
	// cond: symIsRO(sym)
	// result: (MOVQconst [int64(int8(read8(sym, int64(off))))])
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpSB || !(symIsRO(sym)) {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(int8(read8(sym, int64(off)))))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVBQZX(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (MOVBQZX x:(MOVBload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVBload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVBload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVBload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBQZX x:(MOVWload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVBload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVWload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVBload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBQZX x:(MOVLload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVBload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVBload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBQZX x:(MOVQload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVBload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVBload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBQZX (ANDLconst [c] x))
	// result: (ANDLconst [c & 0xff] x)
	for {
		if v_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(c & 0xff)
		v.AddArg(x)
		return true
	}
	// match: (MOVBQZX (MOVBQZX x))
	// result: (MOVBQZX x)
	for {
		if v_0.Op != OpAMD64MOVBQZX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVBQZX)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVBatomicload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVBatomicload [off1] {sym} (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVBatomicload [off1+off2] {sym} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVBatomicload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBatomicload [off1] {sym1} (LEAQ [off2] {sym2} ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVBatomicload [off1+off2] {mergeSym(sym1, sym2)} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVBatomicload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVBload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVBload [off] {sym} ptr (MOVBstore [off2] {sym2} ptr2 x _))
	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
	// result: (MOVBQZX x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVBstore {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		x := v_1.Args[1]
		ptr2 := v_1.Args[0]
		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
			break
		}
		v.reset(OpAMD64MOVBQZX)
		v.AddArg(x)
		return true
	}
	// match: (MOVBload [off1] {sym} (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVBload [off1+off2] {sym} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVBload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBload [off1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVBload [off1+off2] {mergeSym(sym1,sym2)} base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVBload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	// match: (MOVBload [off] {sym} (SB) _)
	// cond: symIsRO(sym)
	// result: (MOVLconst [int32(read8(sym, int64(off)))])
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpSB || !(symIsRO(sym)) {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(int32(read8(sym, int64(off))))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVBstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVBstore [off] {sym} ptr y:(SETL x) mem)
	// cond: y.Uses == 1
	// result: (SETLstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SETL {
			break
		}
		x := y.Args[0]
		mem := v_2
		if !(y.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETLstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr y:(SETLE x) mem)
	// cond: y.Uses == 1
	// result: (SETLEstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SETLE {
			break
		}
		x := y.Args[0]
		mem := v_2
		if !(y.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETLEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr y:(SETG x) mem)
	// cond: y.Uses == 1
	// result: (SETGstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SETG {
			break
		}
		x := y.Args[0]
		mem := v_2
		if !(y.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETGstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr y:(SETGE x) mem)
	// cond: y.Uses == 1
	// result: (SETGEstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SETGE {
			break
		}
		x := y.Args[0]
		mem := v_2
		if !(y.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETGEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr y:(SETEQ x) mem)
	// cond: y.Uses == 1
	// result: (SETEQstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SETEQ {
			break
		}
		x := y.Args[0]
		mem := v_2
		if !(y.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETEQstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr y:(SETNE x) mem)
	// cond: y.Uses == 1
	// result: (SETNEstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SETNE {
			break
		}
		x := y.Args[0]
		mem := v_2
		if !(y.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETNEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr y:(SETB x) mem)
	// cond: y.Uses == 1
	// result: (SETBstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SETB {
			break
		}
		x := y.Args[0]
		mem := v_2
		if !(y.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr y:(SETBE x) mem)
	// cond: y.Uses == 1
	// result: (SETBEstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SETBE {
			break
		}
		x := y.Args[0]
		mem := v_2
		if !(y.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETBEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr y:(SETA x) mem)
	// cond: y.Uses == 1
	// result: (SETAstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SETA {
			break
		}
		x := y.Args[0]
		mem := v_2
		if !(y.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETAstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr y:(SETAE x) mem)
	// cond: y.Uses == 1
	// result: (SETAEstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SETAE {
			break
		}
		x := y.Args[0]
		mem := v_2
		if !(y.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETAEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr (MOVBQSX x) mem)
	// result: (MOVBstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVBQSX {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr (MOVBQZX x) mem)
	// result: (MOVBstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVBQZX {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVBstore [off1] {sym} (ADDQconst [off2] ptr) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVBstore [off1+off2] {sym} ptr val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr (MOVLconst [c]) mem)
	// result: (MOVBstoreconst [makeValAndOff(int32(int8(c)),off)] {sym} ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64MOVBstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(int8(c)), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBstore [off] {sym} ptr (MOVQconst [c]) mem)
	// result: (MOVBstoreconst [makeValAndOff(int32(int8(c)),off)] {sym} ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64MOVBstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(int8(c)), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVBstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVBstoreconst(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVBstoreconst [sc] {s} (ADDQconst [off] ptr) mem)
	// cond: ValAndOff(sc).canAdd32(off)
	// result: (MOVBstoreconst [ValAndOff(sc).addOffset32(off)] {s} ptr mem)
	for {
		sc := auxIntToValAndOff(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(sc).canAdd32(off)) {
			break
		}
		v.reset(OpAMD64MOVBstoreconst)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(sc).addOffset32(off))
		v.Aux = symToAux(s)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVBstoreconst [sc] {sym1} (LEAQ [off] {sym2} ptr) mem)
	// cond: canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd32(off)
	// result: (MOVBstoreconst [ValAndOff(sc).addOffset32(off)] {mergeSym(sym1, sym2)} ptr mem)
	for {
		sc := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		ptr := v_0.Args[0]
		mem := v_1
		if !(canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd32(off)) {
			break
		}
		v.reset(OpAMD64MOVBstoreconst)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(sc).addOffset32(off))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVLQSX(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (MOVLQSX x:(MOVLload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVLQSXload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVLQSXload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLQSX x:(MOVQload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVLQSXload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVLQSXload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLQSX (ANDLconst [c] x))
	// cond: uint32(c) & 0x80000000 == 0
	// result: (ANDLconst [c & 0x7fffffff] x)
	for {
		if v_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(uint32(c)&0x80000000 == 0) {
			break
		}
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(c & 0x7fffffff)
		v.AddArg(x)
		return true
	}
	// match: (MOVLQSX (MOVLQSX x))
	// result: (MOVLQSX x)
	for {
		if v_0.Op != OpAMD64MOVLQSX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVLQSX)
		v.AddArg(x)
		return true
	}
	// match: (MOVLQSX (MOVWQSX x))
	// result: (MOVWQSX x)
	for {
		if v_0.Op != OpAMD64MOVWQSX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVWQSX)
		v.AddArg(x)
		return true
	}
	// match: (MOVLQSX (MOVBQSX x))
	// result: (MOVBQSX x)
	for {
		if v_0.Op != OpAMD64MOVBQSX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVBQSX)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVLQSXload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	config := b.Func.Config
	// match: (MOVLQSXload [off] {sym} ptr (MOVLstore [off2] {sym2} ptr2 x _))
	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
	// result: (MOVLQSX x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLstore {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		x := v_1.Args[1]
		ptr2 := v_1.Args[0]
		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
			break
		}
		v.reset(OpAMD64MOVLQSX)
		v.AddArg(x)
		return true
	}
	// match: (MOVLQSXload [off1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVLQSXload [off1+off2] {mergeSym(sym1,sym2)} base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVLQSXload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	// match: (MOVLQSXload [off] {sym} (SB) _)
	// cond: symIsRO(sym)
	// result: (MOVQconst [int64(int32(read32(sym, int64(off), config.ctxt.Arch.ByteOrder)))])
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpSB || !(symIsRO(sym)) {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(int32(read32(sym, int64(off), config.ctxt.Arch.ByteOrder))))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVLQZX(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (MOVLQZX x:(MOVLload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVLload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVLload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLQZX x:(MOVQload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVLload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVLload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLQZX (ANDLconst [c] x))
	// result: (ANDLconst [c] x)
	for {
		if v_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	// match: (MOVLQZX (MOVLQZX x))
	// result: (MOVLQZX x)
	for {
		if v_0.Op != OpAMD64MOVLQZX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVLQZX)
		v.AddArg(x)
		return true
	}
	// match: (MOVLQZX (MOVWQZX x))
	// result: (MOVWQZX x)
	for {
		if v_0.Op != OpAMD64MOVWQZX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVWQZX)
		v.AddArg(x)
		return true
	}
	// match: (MOVLQZX (MOVBQZX x))
	// result: (MOVBQZX x)
	for {
		if v_0.Op != OpAMD64MOVBQZX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVBQZX)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVLatomicload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVLatomicload [off1] {sym} (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVLatomicload [off1+off2] {sym} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVLatomicload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLatomicload [off1] {sym1} (LEAQ [off2] {sym2} ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVLatomicload [off1+off2] {mergeSym(sym1, sym2)} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVLatomicload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVLf2i(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (MOVLf2i <t> (Arg <u> [off] {sym}))
	// cond: t.Size() == u.Size()
	// result: @b.Func.Entry (Arg <t> [off] {sym})
	for {
		t := v.Type
		if v_0.Op != OpArg {
			break
		}
		u := v_0.Type
		off := auxIntToInt32(v_0.AuxInt)
		sym := auxToSym(v_0.Aux)
		if !(t.Size() == u.Size()) {
			break
		}
		b = b.Func.Entry
		v0 := b.NewValue0(v.Pos, OpArg, t)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVLi2f(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (MOVLi2f <t> (Arg <u> [off] {sym}))
	// cond: t.Size() == u.Size()
	// result: @b.Func.Entry (Arg <t> [off] {sym})
	for {
		t := v.Type
		if v_0.Op != OpArg {
			break
		}
		u := v_0.Type
		off := auxIntToInt32(v_0.AuxInt)
		sym := auxToSym(v_0.Aux)
		if !(t.Size() == u.Size()) {
			break
		}
		b = b.Func.Entry
		v0 := b.NewValue0(v.Pos, OpArg, t)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVLload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	config := b.Func.Config
	// match: (MOVLload [off] {sym} ptr (MOVLstore [off2] {sym2} ptr2 x _))
	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
	// result: (MOVLQZX x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLstore {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		x := v_1.Args[1]
		ptr2 := v_1.Args[0]
		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
			break
		}
		v.reset(OpAMD64MOVLQZX)
		v.AddArg(x)
		return true
	}
	// match: (MOVLload [off1] {sym} (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVLload [off1+off2] {sym} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLload [off1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVLload [off1+off2] {mergeSym(sym1,sym2)} base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	// match: (MOVLload [off] {sym} ptr (MOVSSstore [off] {sym} ptr val _))
	// result: (MOVLf2i val)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVSSstore || auxIntToInt32(v_1.AuxInt) != off || auxToSym(v_1.Aux) != sym {
			break
		}
		val := v_1.Args[1]
		if ptr != v_1.Args[0] {
			break
		}
		v.reset(OpAMD64MOVLf2i)
		v.AddArg(val)
		return true
	}
	// match: (MOVLload [off] {sym} (SB) _)
	// cond: symIsRO(sym)
	// result: (MOVLconst [int32(read32(sym, int64(off), config.ctxt.Arch.ByteOrder))])
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpSB || !(symIsRO(sym)) {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(int32(read32(sym, int64(off), config.ctxt.Arch.ByteOrder)))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVLstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVLstore [off] {sym} ptr (MOVLQSX x) mem)
	// result: (MOVLstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLQSX {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64MOVLstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVLstore [off] {sym} ptr (MOVLQZX x) mem)
	// result: (MOVLstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLQZX {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64MOVLstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVLstore [off1] {sym} (ADDQconst [off2] ptr) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVLstore [off1+off2] {sym} ptr val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVLstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (MOVLstore [off] {sym} ptr (MOVLconst [c]) mem)
	// result: (MOVLstoreconst [makeValAndOff(int32(c),off)] {sym} ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64MOVLstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLstore [off] {sym} ptr (MOVQconst [c]) mem)
	// result: (MOVLstoreconst [makeValAndOff(int32(c),off)] {sym} ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64MOVLstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVLstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVLstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (MOVLstore {sym} [off] ptr y:(ADDLload x [off] {sym} ptr mem) mem)
	// cond: y.Uses==1 && clobber(y)
	// result: (ADDLmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ADDLload || auxIntToInt32(y.AuxInt) != off || auxToSym(y.Aux) != sym {
			break
		}
		mem := y.Args[2]
		x := y.Args[0]
		if ptr != y.Args[1] || mem != v_2 || !(y.Uses == 1 && clobber(y)) {
			break
		}
		v.reset(OpAMD64ADDLmodify)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVLstore {sym} [off] ptr y:(ANDLload x [off] {sym} ptr mem) mem)
	// cond: y.Uses==1 && clobber(y)
	// result: (ANDLmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ANDLload || auxIntToInt32(y.AuxInt) != off || auxToSym(y.Aux) != sym {
			break
		}
		mem := y.Args[2]
		x := y.Args[0]
		if ptr != y.Args[1] || mem != v_2 || !(y.Uses == 1 && clobber(y)) {
			break
		}
		v.reset(OpAMD64ANDLmodify)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVLstore {sym} [off] ptr y:(ORLload x [off] {sym} ptr mem) mem)
	// cond: y.Uses==1 && clobber(y)
	// result: (ORLmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ORLload || auxIntToInt32(y.AuxInt) != off || auxToSym(y.Aux) != sym {
			break
		}
		mem := y.Args[2]
		x := y.Args[0]
		if ptr != y.Args[1] || mem != v_2 || !(y.Uses == 1 && clobber(y)) {
			break
		}
		v.reset(OpAMD64ORLmodify)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVLstore {sym} [off] ptr y:(XORLload x [off] {sym} ptr mem) mem)
	// cond: y.Uses==1 && clobber(y)
	// result: (XORLmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64XORLload || auxIntToInt32(y.AuxInt) != off || auxToSym(y.Aux) != sym {
			break
		}
		mem := y.Args[2]
		x := y.Args[0]
		if ptr != y.Args[1] || mem != v_2 || !(y.Uses == 1 && clobber(y)) {
			break
		}
		v.reset(OpAMD64XORLmodify)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVLstore {sym} [off] ptr y:(ADDL l:(MOVLload [off] {sym} ptr mem) x) mem)
	// cond: y.Uses==1 && l.Uses==1 && clobber(y, l)
	// result: (ADDLmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ADDL {
			break
		}
		_ = y.Args[1]
		y_0 := y.Args[0]
		y_1 := y.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, y_0, y_1 = _i0+1, y_1, y_0 {
			l := y_0
			if l.Op != OpAMD64MOVLload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
				continue
			}
			mem := l.Args[1]
			if ptr != l.Args[0] {
				continue
			}
			x := y_1
			if mem != v_2 || !(y.Uses == 1 && l.Uses == 1 && clobber(y, l)) {
				continue
			}
			v.reset(OpAMD64ADDLmodify)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(ptr, x, mem)
			return true
		}
		break
	}
	// match: (MOVLstore {sym} [off] ptr y:(SUBL l:(MOVLload [off] {sym} ptr mem) x) mem)
	// cond: y.Uses==1 && l.Uses==1 && clobber(y, l)
	// result: (SUBLmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SUBL {
			break
		}
		x := y.Args[1]
		l := y.Args[0]
		if l.Op != OpAMD64MOVLload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		if ptr != l.Args[0] || mem != v_2 || !(y.Uses == 1 && l.Uses == 1 && clobber(y, l)) {
			break
		}
		v.reset(OpAMD64SUBLmodify)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVLstore {sym} [off] ptr y:(ANDL l:(MOVLload [off] {sym} ptr mem) x) mem)
	// cond: y.Uses==1 && l.Uses==1 && clobber(y, l)
	// result: (ANDLmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ANDL {
			break
		}
		_ = y.Args[1]
		y_0 := y.Args[0]
		y_1 := y.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, y_0, y_1 = _i0+1, y_1, y_0 {
			l := y_0
			if l.Op != OpAMD64MOVLload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
				continue
			}
			mem := l.Args[1]
			if ptr != l.Args[0] {
				continue
			}
			x := y_1
			if mem != v_2 || !(y.Uses == 1 && l.Uses == 1 && clobber(y, l)) {
				continue
			}
			v.reset(OpAMD64ANDLmodify)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(ptr, x, mem)
			return true
		}
		break
	}
	// match: (MOVLstore {sym} [off] ptr y:(ORL l:(MOVLload [off] {sym} ptr mem) x) mem)
	// cond: y.Uses==1 && l.Uses==1 && clobber(y, l)
	// result: (ORLmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ORL {
			break
		}
		_ = y.Args[1]
		y_0 := y.Args[0]
		y_1 := y.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, y_0, y_1 = _i0+1, y_1, y_0 {
			l := y_0
			if l.Op != OpAMD64MOVLload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
				continue
			}
			mem := l.Args[1]
			if ptr != l.Args[0] {
				continue
			}
			x := y_1
			if mem != v_2 || !(y.Uses == 1 && l.Uses == 1 && clobber(y, l)) {
				continue
			}
			v.reset(OpAMD64ORLmodify)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(ptr, x, mem)
			return true
		}
		break
	}
	// match: (MOVLstore {sym} [off] ptr y:(XORL l:(MOVLload [off] {sym} ptr mem) x) mem)
	// cond: y.Uses==1 && l.Uses==1 && clobber(y, l)
	// result: (XORLmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64XORL {
			break
		}
		_ = y.Args[1]
		y_0 := y.Args[0]
		y_1 := y.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, y_0, y_1 = _i0+1, y_1, y_0 {
			l := y_0
			if l.Op != OpAMD64MOVLload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
				continue
			}
			mem := l.Args[1]
			if ptr != l.Args[0] {
				continue
			}
			x := y_1
			if mem != v_2 || !(y.Uses == 1 && l.Uses == 1 && clobber(y, l)) {
				continue
			}
			v.reset(OpAMD64XORLmodify)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(ptr, x, mem)
			return true
		}
		break
	}
	// match: (MOVLstore [off] {sym} ptr a:(ADDLconst [c] l:(MOVLload [off] {sym} ptr2 mem)) mem)
	// cond: isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)
	// result: (ADDLconstmodify {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		a := v_1
		if a.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		l := a.Args[0]
		if l.Op != OpAMD64MOVLload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		ptr2 := l.Args[0]
		if mem != v_2 || !(isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)) {
			break
		}
		v.reset(OpAMD64ADDLconstmodify)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLstore [off] {sym} ptr a:(ANDLconst [c] l:(MOVLload [off] {sym} ptr2 mem)) mem)
	// cond: isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)
	// result: (ANDLconstmodify {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		a := v_1
		if a.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		l := a.Args[0]
		if l.Op != OpAMD64MOVLload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		ptr2 := l.Args[0]
		if mem != v_2 || !(isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)) {
			break
		}
		v.reset(OpAMD64ANDLconstmodify)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLstore [off] {sym} ptr a:(ORLconst [c] l:(MOVLload [off] {sym} ptr2 mem)) mem)
	// cond: isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)
	// result: (ORLconstmodify {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		a := v_1
		if a.Op != OpAMD64ORLconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		l := a.Args[0]
		if l.Op != OpAMD64MOVLload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		ptr2 := l.Args[0]
		if mem != v_2 || !(isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)) {
			break
		}
		v.reset(OpAMD64ORLconstmodify)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLstore [off] {sym} ptr a:(XORLconst [c] l:(MOVLload [off] {sym} ptr2 mem)) mem)
	// cond: isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)
	// result: (XORLconstmodify {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		a := v_1
		if a.Op != OpAMD64XORLconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		l := a.Args[0]
		if l.Op != OpAMD64MOVLload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		ptr2 := l.Args[0]
		if mem != v_2 || !(isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)) {
			break
		}
		v.reset(OpAMD64XORLconstmodify)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLstore [off] {sym} ptr (MOVLf2i val) mem)
	// result: (MOVSSstore [off] {sym} ptr val mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLf2i {
			break
		}
		val := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64MOVSSstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (MOVLstore [i] {s} p x:(BSWAPL w) mem)
	// cond: x.Uses == 1 && buildcfg.GOAMD64 >= 3
	// result: (MOVBELstore [i] {s} p w mem)
	for {
		i := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		p := v_0
		x := v_1
		if x.Op != OpAMD64BSWAPL {
			break
		}
		w := x.Args[0]
		mem := v_2
		if !(x.Uses == 1 && buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64MOVBELstore)
		v.AuxInt = int32ToAuxInt(i)
		v.Aux = symToAux(s)
		v.AddArg3(p, w, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVLstoreconst(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVLstoreconst [sc] {s} (ADDQconst [off] ptr) mem)
	// cond: ValAndOff(sc).canAdd32(off)
	// result: (MOVLstoreconst [ValAndOff(sc).addOffset32(off)] {s} ptr mem)
	for {
		sc := auxIntToValAndOff(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(sc).canAdd32(off)) {
			break
		}
		v.reset(OpAMD64MOVLstoreconst)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(sc).addOffset32(off))
		v.Aux = symToAux(s)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVLstoreconst [sc] {sym1} (LEAQ [off] {sym2} ptr) mem)
	// cond: canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd32(off)
	// result: (MOVLstoreconst [ValAndOff(sc).addOffset32(off)] {mergeSym(sym1, sym2)} ptr mem)
	for {
		sc := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		ptr := v_0.Args[0]
		mem := v_1
		if !(canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd32(off)) {
			break
		}
		v.reset(OpAMD64MOVLstoreconst)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(sc).addOffset32(off))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVOload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVOload [off1] {sym} (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVOload [off1+off2] {sym} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVOload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVOload [off1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVOload [off1+off2] {mergeSym(sym1,sym2)} base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVOload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVOstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	config := b.Func.Config
	typ := &b.Func.Config.Types
	// match: (MOVOstore [off1] {sym} (ADDQconst [off2] ptr) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVOstore [off1+off2] {sym} ptr val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVOstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (MOVOstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVOstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVOstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (MOVOstore [dstOff] {dstSym} ptr (MOVOload [srcOff] {srcSym} (SB) _) mem)
	// cond: symIsRO(srcSym)
	// result: (MOVQstore [dstOff+8] {dstSym} ptr (MOVQconst [int64(read64(srcSym, int64(srcOff)+8, config.ctxt.Arch.ByteOrder))]) (MOVQstore [dstOff] {dstSym} ptr (MOVQconst [int64(read64(srcSym, int64(srcOff), config.ctxt.Arch.ByteOrder))]) mem))
	for {
		dstOff := auxIntToInt32(v.AuxInt)
		dstSym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVOload {
			break
		}
		srcOff := auxIntToInt32(v_1.AuxInt)
		srcSym := auxToSym(v_1.Aux)
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpSB {
			break
		}
		mem := v_2
		if !(symIsRO(srcSym)) {
			break
		}
		v.reset(OpAMD64MOVQstore)
		v.AuxInt = int32ToAuxInt(dstOff + 8)
		v.Aux = symToAux(dstSym)
		v0 := b.NewValue0(v_1.Pos, OpAMD64MOVQconst, typ.UInt64)
		v0.AuxInt = int64ToAuxInt(int64(read64(srcSym, int64(srcOff)+8, config.ctxt.Arch.ByteOrder)))
		v1 := b.NewValue0(v_1.Pos, OpAMD64MOVQstore, types.TypeMem)
		v1.AuxInt = int32ToAuxInt(dstOff)
		v1.Aux = symToAux(dstSym)
		v2 := b.NewValue0(v_1.Pos, OpAMD64MOVQconst, typ.UInt64)
		v2.AuxInt = int64ToAuxInt(int64(read64(srcSym, int64(srcOff), config.ctxt.Arch.ByteOrder)))
		v1.AddArg3(ptr, v2, mem)
		v.AddArg3(ptr, v0, v1)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVOstoreconst(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVOstoreconst [sc] {s} (ADDQconst [off] ptr) mem)
	// cond: ValAndOff(sc).canAdd32(off)
	// result: (MOVOstoreconst [ValAndOff(sc).addOffset32(off)] {s} ptr mem)
	for {
		sc := auxIntToValAndOff(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(sc).canAdd32(off)) {
			break
		}
		v.reset(OpAMD64MOVOstoreconst)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(sc).addOffset32(off))
		v.Aux = symToAux(s)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVOstoreconst [sc] {sym1} (LEAQ [off] {sym2} ptr) mem)
	// cond: canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd32(off)
	// result: (MOVOstoreconst [ValAndOff(sc).addOffset32(off)] {mergeSym(sym1, sym2)} ptr mem)
	for {
		sc := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		ptr := v_0.Args[0]
		mem := v_1
		if !(canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd32(off)) {
			break
		}
		v.reset(OpAMD64MOVOstoreconst)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(sc).addOffset32(off))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVQatomicload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVQatomicload [off1] {sym} (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVQatomicload [off1+off2] {sym} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVQatomicload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQatomicload [off1] {sym1} (LEAQ [off2] {sym2} ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVQatomicload [off1+off2] {mergeSym(sym1, sym2)} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVQatomicload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVQf2i(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (MOVQf2i <t> (Arg <u> [off] {sym}))
	// cond: t.Size() == u.Size()
	// result: @b.Func.Entry (Arg <t> [off] {sym})
	for {
		t := v.Type
		if v_0.Op != OpArg {
			break
		}
		u := v_0.Type
		off := auxIntToInt32(v_0.AuxInt)
		sym := auxToSym(v_0.Aux)
		if !(t.Size() == u.Size()) {
			break
		}
		b = b.Func.Entry
		v0 := b.NewValue0(v.Pos, OpArg, t)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVQi2f(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (MOVQi2f <t> (Arg <u> [off] {sym}))
	// cond: t.Size() == u.Size()
	// result: @b.Func.Entry (Arg <t> [off] {sym})
	for {
		t := v.Type
		if v_0.Op != OpArg {
			break
		}
		u := v_0.Type
		off := auxIntToInt32(v_0.AuxInt)
		sym := auxToSym(v_0.Aux)
		if !(t.Size() == u.Size()) {
			break
		}
		b = b.Func.Entry
		v0 := b.NewValue0(v.Pos, OpArg, t)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVQload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	config := b.Func.Config
	// match: (MOVQload [off] {sym} ptr (MOVQstore [off2] {sym2} ptr2 x _))
	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
	// result: x
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQstore {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		x := v_1.Args[1]
		ptr2 := v_1.Args[0]
		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (MOVQload [off1] {sym} (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVQload [off1+off2] {sym} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQload [off1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVQload [off1+off2] {mergeSym(sym1,sym2)} base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	// match: (MOVQload [off] {sym} ptr (MOVSDstore [off] {sym} ptr val _))
	// result: (MOVQf2i val)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVSDstore || auxIntToInt32(v_1.AuxInt) != off || auxToSym(v_1.Aux) != sym {
			break
		}
		val := v_1.Args[1]
		if ptr != v_1.Args[0] {
			break
		}
		v.reset(OpAMD64MOVQf2i)
		v.AddArg(val)
		return true
	}
	// match: (MOVQload [off] {sym} (SB) _)
	// cond: symIsRO(sym)
	// result: (MOVQconst [int64(read64(sym, int64(off), config.ctxt.Arch.ByteOrder))])
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpSB || !(symIsRO(sym)) {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(read64(sym, int64(off), config.ctxt.Arch.ByteOrder)))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVQstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVQstore [off1] {sym} (ADDQconst [off2] ptr) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVQstore [off1+off2] {sym} ptr val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVQstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (MOVQstore [off] {sym} ptr (MOVQconst [c]) mem)
	// cond: validVal(c)
	// result: (MOVQstoreconst [makeValAndOff(int32(c),off)] {sym} ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		mem := v_2
		if !(validVal(c)) {
			break
		}
		v.reset(OpAMD64MOVQstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVQstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVQstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (MOVQstore {sym} [off] ptr y:(ADDQload x [off] {sym} ptr mem) mem)
	// cond: y.Uses==1 && clobber(y)
	// result: (ADDQmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ADDQload || auxIntToInt32(y.AuxInt) != off || auxToSym(y.Aux) != sym {
			break
		}
		mem := y.Args[2]
		x := y.Args[0]
		if ptr != y.Args[1] || mem != v_2 || !(y.Uses == 1 && clobber(y)) {
			break
		}
		v.reset(OpAMD64ADDQmodify)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVQstore {sym} [off] ptr y:(ANDQload x [off] {sym} ptr mem) mem)
	// cond: y.Uses==1 && clobber(y)
	// result: (ANDQmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ANDQload || auxIntToInt32(y.AuxInt) != off || auxToSym(y.Aux) != sym {
			break
		}
		mem := y.Args[2]
		x := y.Args[0]
		if ptr != y.Args[1] || mem != v_2 || !(y.Uses == 1 && clobber(y)) {
			break
		}
		v.reset(OpAMD64ANDQmodify)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVQstore {sym} [off] ptr y:(ORQload x [off] {sym} ptr mem) mem)
	// cond: y.Uses==1 && clobber(y)
	// result: (ORQmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ORQload || auxIntToInt32(y.AuxInt) != off || auxToSym(y.Aux) != sym {
			break
		}
		mem := y.Args[2]
		x := y.Args[0]
		if ptr != y.Args[1] || mem != v_2 || !(y.Uses == 1 && clobber(y)) {
			break
		}
		v.reset(OpAMD64ORQmodify)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVQstore {sym} [off] ptr y:(XORQload x [off] {sym} ptr mem) mem)
	// cond: y.Uses==1 && clobber(y)
	// result: (XORQmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64XORQload || auxIntToInt32(y.AuxInt) != off || auxToSym(y.Aux) != sym {
			break
		}
		mem := y.Args[2]
		x := y.Args[0]
		if ptr != y.Args[1] || mem != v_2 || !(y.Uses == 1 && clobber(y)) {
			break
		}
		v.reset(OpAMD64XORQmodify)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVQstore {sym} [off] ptr y:(ADDQ l:(MOVQload [off] {sym} ptr mem) x) mem)
	// cond: y.Uses==1 && l.Uses==1 && clobber(y, l)
	// result: (ADDQmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ADDQ {
			break
		}
		_ = y.Args[1]
		y_0 := y.Args[0]
		y_1 := y.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, y_0, y_1 = _i0+1, y_1, y_0 {
			l := y_0
			if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
				continue
			}
			mem := l.Args[1]
			if ptr != l.Args[0] {
				continue
			}
			x := y_1
			if mem != v_2 || !(y.Uses == 1 && l.Uses == 1 && clobber(y, l)) {
				continue
			}
			v.reset(OpAMD64ADDQmodify)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(ptr, x, mem)
			return true
		}
		break
	}
	// match: (MOVQstore {sym} [off] ptr y:(SUBQ l:(MOVQload [off] {sym} ptr mem) x) mem)
	// cond: y.Uses==1 && l.Uses==1 && clobber(y, l)
	// result: (SUBQmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64SUBQ {
			break
		}
		x := y.Args[1]
		l := y.Args[0]
		if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		if ptr != l.Args[0] || mem != v_2 || !(y.Uses == 1 && l.Uses == 1 && clobber(y, l)) {
			break
		}
		v.reset(OpAMD64SUBQmodify)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVQstore {sym} [off] ptr y:(ANDQ l:(MOVQload [off] {sym} ptr mem) x) mem)
	// cond: y.Uses==1 && l.Uses==1 && clobber(y, l)
	// result: (ANDQmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ANDQ {
			break
		}
		_ = y.Args[1]
		y_0 := y.Args[0]
		y_1 := y.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, y_0, y_1 = _i0+1, y_1, y_0 {
			l := y_0
			if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
				continue
			}
			mem := l.Args[1]
			if ptr != l.Args[0] {
				continue
			}
			x := y_1
			if mem != v_2 || !(y.Uses == 1 && l.Uses == 1 && clobber(y, l)) {
				continue
			}
			v.reset(OpAMD64ANDQmodify)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(ptr, x, mem)
			return true
		}
		break
	}
	// match: (MOVQstore {sym} [off] ptr y:(ORQ l:(MOVQload [off] {sym} ptr mem) x) mem)
	// cond: y.Uses==1 && l.Uses==1 && clobber(y, l)
	// result: (ORQmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64ORQ {
			break
		}
		_ = y.Args[1]
		y_0 := y.Args[0]
		y_1 := y.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, y_0, y_1 = _i0+1, y_1, y_0 {
			l := y_0
			if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
				continue
			}
			mem := l.Args[1]
			if ptr != l.Args[0] {
				continue
			}
			x := y_1
			if mem != v_2 || !(y.Uses == 1 && l.Uses == 1 && clobber(y, l)) {
				continue
			}
			v.reset(OpAMD64ORQmodify)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(ptr, x, mem)
			return true
		}
		break
	}
	// match: (MOVQstore {sym} [off] ptr y:(XORQ l:(MOVQload [off] {sym} ptr mem) x) mem)
	// cond: y.Uses==1 && l.Uses==1 && clobber(y, l)
	// result: (XORQmodify [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		y := v_1
		if y.Op != OpAMD64XORQ {
			break
		}
		_ = y.Args[1]
		y_0 := y.Args[0]
		y_1 := y.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, y_0, y_1 = _i0+1, y_1, y_0 {
			l := y_0
			if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
				continue
			}
			mem := l.Args[1]
			if ptr != l.Args[0] {
				continue
			}
			x := y_1
			if mem != v_2 || !(y.Uses == 1 && l.Uses == 1 && clobber(y, l)) {
				continue
			}
			v.reset(OpAMD64XORQmodify)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(ptr, x, mem)
			return true
		}
		break
	}
	// match: (MOVQstore {sym} [off] ptr x:(BTSQconst [c] l:(MOVQload {sym} [off] ptr mem)) mem)
	// cond: x.Uses == 1 && l.Uses == 1 && clobber(x, l)
	// result: (BTSQconstmodify {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		x := v_1
		if x.Op != OpAMD64BTSQconst {
			break
		}
		c := auxIntToInt8(x.AuxInt)
		l := x.Args[0]
		if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		if ptr != l.Args[0] || mem != v_2 || !(x.Uses == 1 && l.Uses == 1 && clobber(x, l)) {
			break
		}
		v.reset(OpAMD64BTSQconstmodify)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQstore {sym} [off] ptr x:(BTRQconst [c] l:(MOVQload {sym} [off] ptr mem)) mem)
	// cond: x.Uses == 1 && l.Uses == 1 && clobber(x, l)
	// result: (BTRQconstmodify {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		x := v_1
		if x.Op != OpAMD64BTRQconst {
			break
		}
		c := auxIntToInt8(x.AuxInt)
		l := x.Args[0]
		if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		if ptr != l.Args[0] || mem != v_2 || !(x.Uses == 1 && l.Uses == 1 && clobber(x, l)) {
			break
		}
		v.reset(OpAMD64BTRQconstmodify)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQstore {sym} [off] ptr x:(BTCQconst [c] l:(MOVQload {sym} [off] ptr mem)) mem)
	// cond: x.Uses == 1 && l.Uses == 1 && clobber(x, l)
	// result: (BTCQconstmodify {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		x := v_1
		if x.Op != OpAMD64BTCQconst {
			break
		}
		c := auxIntToInt8(x.AuxInt)
		l := x.Args[0]
		if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		if ptr != l.Args[0] || mem != v_2 || !(x.Uses == 1 && l.Uses == 1 && clobber(x, l)) {
			break
		}
		v.reset(OpAMD64BTCQconstmodify)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQstore [off] {sym} ptr a:(ADDQconst [c] l:(MOVQload [off] {sym} ptr2 mem)) mem)
	// cond: isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)
	// result: (ADDQconstmodify {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		a := v_1
		if a.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		l := a.Args[0]
		if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		ptr2 := l.Args[0]
		if mem != v_2 || !(isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)) {
			break
		}
		v.reset(OpAMD64ADDQconstmodify)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQstore [off] {sym} ptr a:(ANDQconst [c] l:(MOVQload [off] {sym} ptr2 mem)) mem)
	// cond: isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)
	// result: (ANDQconstmodify {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		a := v_1
		if a.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		l := a.Args[0]
		if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		ptr2 := l.Args[0]
		if mem != v_2 || !(isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)) {
			break
		}
		v.reset(OpAMD64ANDQconstmodify)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQstore [off] {sym} ptr a:(ORQconst [c] l:(MOVQload [off] {sym} ptr2 mem)) mem)
	// cond: isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)
	// result: (ORQconstmodify {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		a := v_1
		if a.Op != OpAMD64ORQconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		l := a.Args[0]
		if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		ptr2 := l.Args[0]
		if mem != v_2 || !(isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)) {
			break
		}
		v.reset(OpAMD64ORQconstmodify)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQstore [off] {sym} ptr a:(XORQconst [c] l:(MOVQload [off] {sym} ptr2 mem)) mem)
	// cond: isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)
	// result: (XORQconstmodify {sym} [makeValAndOff(int32(c),off)] ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		a := v_1
		if a.Op != OpAMD64XORQconst {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		l := a.Args[0]
		if l.Op != OpAMD64MOVQload || auxIntToInt32(l.AuxInt) != off || auxToSym(l.Aux) != sym {
			break
		}
		mem := l.Args[1]
		ptr2 := l.Args[0]
		if mem != v_2 || !(isSamePtr(ptr, ptr2) && a.Uses == 1 && l.Uses == 1 && clobber(l, a)) {
			break
		}
		v.reset(OpAMD64XORQconstmodify)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(c), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQstore [off] {sym} ptr (MOVQf2i val) mem)
	// result: (MOVSDstore [off] {sym} ptr val mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQf2i {
			break
		}
		val := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64MOVSDstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (MOVQstore [i] {s} p x:(BSWAPQ w) mem)
	// cond: x.Uses == 1 && buildcfg.GOAMD64 >= 3
	// result: (MOVBEQstore [i] {s} p w mem)
	for {
		i := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		p := v_0
		x := v_1
		if x.Op != OpAMD64BSWAPQ {
			break
		}
		w := x.Args[0]
		mem := v_2
		if !(x.Uses == 1 && buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64MOVBEQstore)
		v.AuxInt = int32ToAuxInt(i)
		v.Aux = symToAux(s)
		v.AddArg3(p, w, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVQstoreconst(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVQstoreconst [sc] {s} (ADDQconst [off] ptr) mem)
	// cond: ValAndOff(sc).canAdd32(off)
	// result: (MOVQstoreconst [ValAndOff(sc).addOffset32(off)] {s} ptr mem)
	for {
		sc := auxIntToValAndOff(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(sc).canAdd32(off)) {
			break
		}
		v.reset(OpAMD64MOVQstoreconst)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(sc).addOffset32(off))
		v.Aux = symToAux(s)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQstoreconst [sc] {sym1} (LEAQ [off] {sym2} ptr) mem)
	// cond: canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd32(off)
	// result: (MOVQstoreconst [ValAndOff(sc).addOffset32(off)] {mergeSym(sym1, sym2)} ptr mem)
	for {
		sc := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		ptr := v_0.Args[0]
		mem := v_1
		if !(canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd32(off)) {
			break
		}
		v.reset(OpAMD64MOVQstoreconst)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(sc).addOffset32(off))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVQstoreconst [c] {s} p1 x:(MOVQstoreconst [a] {s} p0 mem))
	// cond: x.Uses == 1 && sequentialAddresses(p0, p1, int64(a.Off()+8-c.Off())) && a.Val() == 0 && c.Val() == 0 && setPos(v, x.Pos) && clobber(x)
	// result: (MOVOstoreconst [makeValAndOff(0,a.Off())] {s} p0 mem)
	for {
		c := auxIntToValAndOff(v.AuxInt)
		s := auxToSym(v.Aux)
		p1 := v_0
		x := v_1
		if x.Op != OpAMD64MOVQstoreconst {
			break
		}
		a := auxIntToValAndOff(x.AuxInt)
		if auxToSym(x.Aux) != s {
			break
		}
		mem := x.Args[1]
		p0 := x.Args[0]
		if !(x.Uses == 1 && sequentialAddresses(p0, p1, int64(a.Off()+8-c.Off())) && a.Val() == 0 && c.Val() == 0 && setPos(v, x.Pos) && clobber(x)) {
			break
		}
		v.reset(OpAMD64MOVOstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, a.Off()))
		v.Aux = symToAux(s)
		v.AddArg2(p0, mem)
		return true
	}
	// match: (MOVQstoreconst [a] {s} p0 x:(MOVQstoreconst [c] {s} p1 mem))
	// cond: x.Uses == 1 && sequentialAddresses(p0, p1, int64(a.Off()+8-c.Off())) && a.Val() == 0 && c.Val() == 0 && setPos(v, x.Pos) && clobber(x)
	// result: (MOVOstoreconst [makeValAndOff(0,a.Off())] {s} p0 mem)
	for {
		a := auxIntToValAndOff(v.AuxInt)
		s := auxToSym(v.Aux)
		p0 := v_0
		x := v_1
		if x.Op != OpAMD64MOVQstoreconst {
			break
		}
		c := auxIntToValAndOff(x.AuxInt)
		if auxToSym(x.Aux) != s {
			break
		}
		mem := x.Args[1]
		p1 := x.Args[0]
		if !(x.Uses == 1 && sequentialAddresses(p0, p1, int64(a.Off()+8-c.Off())) && a.Val() == 0 && c.Val() == 0 && setPos(v, x.Pos) && clobber(x)) {
			break
		}
		v.reset(OpAMD64MOVOstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, a.Off()))
		v.Aux = symToAux(s)
		v.AddArg2(p0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVSDload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVSDload [off1] {sym} (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVSDload [off1+off2] {sym} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVSDload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVSDload [off1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVSDload [off1+off2] {mergeSym(sym1,sym2)} base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVSDload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	// match: (MOVSDload [off] {sym} ptr (MOVQstore [off] {sym} ptr val _))
	// result: (MOVQi2f val)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQstore || auxIntToInt32(v_1.AuxInt) != off || auxToSym(v_1.Aux) != sym {
			break
		}
		val := v_1.Args[1]
		if ptr != v_1.Args[0] {
			break
		}
		v.reset(OpAMD64MOVQi2f)
		v.AddArg(val)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVSDstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MOVSDstore [off1] {sym} (ADDQconst [off2] ptr) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVSDstore [off1+off2] {sym} ptr val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVSDstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (MOVSDstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVSDstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVSDstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (MOVSDstore [off] {sym} ptr (MOVQi2f val) mem)
	// result: (MOVQstore [off] {sym} ptr val mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQi2f {
			break
		}
		val := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64MOVQstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (MOVSDstore [off] {sym} ptr (MOVSDconst [f]) mem)
	// cond: f == f
	// result: (MOVQstore [off] {sym} ptr (MOVQconst [int64(math.Float64bits(f))]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVSDconst {
			break
		}
		f := auxIntToFloat64(v_1.AuxInt)
		mem := v_2
		if !(f == f) {
			break
		}
		v.reset(OpAMD64MOVQstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, typ.UInt64)
		v0.AuxInt = int64ToAuxInt(int64(math.Float64bits(f)))
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVSSload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVSSload [off1] {sym} (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVSSload [off1+off2] {sym} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVSSload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVSSload [off1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVSSload [off1+off2] {mergeSym(sym1,sym2)} base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVSSload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	// match: (MOVSSload [off] {sym} ptr (MOVLstore [off] {sym} ptr val _))
	// result: (MOVLi2f val)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLstore || auxIntToInt32(v_1.AuxInt) != off || auxToSym(v_1.Aux) != sym {
			break
		}
		val := v_1.Args[1]
		if ptr != v_1.Args[0] {
			break
		}
		v.reset(OpAMD64MOVLi2f)
		v.AddArg(val)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVSSstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MOVSSstore [off1] {sym} (ADDQconst [off2] ptr) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVSSstore [off1+off2] {sym} ptr val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVSSstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (MOVSSstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVSSstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVSSstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (MOVSSstore [off] {sym} ptr (MOVLi2f val) mem)
	// result: (MOVLstore [off] {sym} ptr val mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLi2f {
			break
		}
		val := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64MOVLstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (MOVSSstore [off] {sym} ptr (MOVSSconst [f]) mem)
	// cond: f == f
	// result: (MOVLstore [off] {sym} ptr (MOVLconst [int32(math.Float32bits(f))]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVSSconst {
			break
		}
		f := auxIntToFloat32(v_1.AuxInt)
		mem := v_2
		if !(f == f) {
			break
		}
		v.reset(OpAMD64MOVLstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt32)
		v0.AuxInt = int32ToAuxInt(int32(math.Float32bits(f)))
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVWQSX(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (MOVWQSX x:(MOVWload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVWQSXload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVWload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVWQSXload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVWQSX x:(MOVLload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVWQSXload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVWQSXload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVWQSX x:(MOVQload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVWQSXload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVWQSXload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVWQSX (ANDLconst [c] x))
	// cond: c & 0x8000 == 0
	// result: (ANDLconst [c & 0x7fff] x)
	for {
		if v_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(c&0x8000 == 0) {
			break
		}
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(c & 0x7fff)
		v.AddArg(x)
		return true
	}
	// match: (MOVWQSX (MOVWQSX x))
	// result: (MOVWQSX x)
	for {
		if v_0.Op != OpAMD64MOVWQSX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVWQSX)
		v.AddArg(x)
		return true
	}
	// match: (MOVWQSX (MOVBQSX x))
	// result: (MOVBQSX x)
	for {
		if v_0.Op != OpAMD64MOVBQSX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVBQSX)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVWQSXload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	config := b.Func.Config
	// match: (MOVWQSXload [off] {sym} ptr (MOVWstore [off2] {sym2} ptr2 x _))
	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
	// result: (MOVWQSX x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVWstore {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		x := v_1.Args[1]
		ptr2 := v_1.Args[0]
		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
			break
		}
		v.reset(OpAMD64MOVWQSX)
		v.AddArg(x)
		return true
	}
	// match: (MOVWQSXload [off1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVWQSXload [off1+off2] {mergeSym(sym1,sym2)} base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVWQSXload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	// match: (MOVWQSXload [off] {sym} (SB) _)
	// cond: symIsRO(sym)
	// result: (MOVQconst [int64(int16(read16(sym, int64(off), config.ctxt.Arch.ByteOrder)))])
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpSB || !(symIsRO(sym)) {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(int16(read16(sym, int64(off), config.ctxt.Arch.ByteOrder))))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVWQZX(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (MOVWQZX x:(MOVWload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVWload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVWload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVWload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVWQZX x:(MOVLload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVWload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVWload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVWQZX x:(MOVQload [off] {sym} ptr mem))
	// cond: x.Uses == 1 && clobber(x)
	// result: @x.Block (MOVWload <v.Type> [off] {sym} ptr mem)
	for {
		x := v_0
		if x.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(x.AuxInt)
		sym := auxToSym(x.Aux)
		mem := x.Args[1]
		ptr := x.Args[0]
		if !(x.Uses == 1 && clobber(x)) {
			break
		}
		b = x.Block
		v0 := b.NewValue0(x.Pos, OpAMD64MOVWload, v.Type)
		v.copyOf(v0)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVWQZX (ANDLconst [c] x))
	// result: (ANDLconst [c & 0xffff] x)
	for {
		if v_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(c & 0xffff)
		v.AddArg(x)
		return true
	}
	// match: (MOVWQZX (MOVWQZX x))
	// result: (MOVWQZX x)
	for {
		if v_0.Op != OpAMD64MOVWQZX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVWQZX)
		v.AddArg(x)
		return true
	}
	// match: (MOVWQZX (MOVBQZX x))
	// result: (MOVBQZX x)
	for {
		if v_0.Op != OpAMD64MOVBQZX {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64MOVBQZX)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVWload(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	config := b.Func.Config
	// match: (MOVWload [off] {sym} ptr (MOVWstore [off2] {sym2} ptr2 x _))
	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
	// result: (MOVWQZX x)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVWstore {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		x := v_1.Args[1]
		ptr2 := v_1.Args[0]
		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
			break
		}
		v.reset(OpAMD64MOVWQZX)
		v.AddArg(x)
		return true
	}
	// match: (MOVWload [off1] {sym} (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVWload [off1+off2] {sym} ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVWload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVWload [off1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVWload [off1+off2] {mergeSym(sym1,sym2)} base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVWload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	// match: (MOVWload [off] {sym} (SB) _)
	// cond: symIsRO(sym)
	// result: (MOVLconst [int32(read16(sym, int64(off), config.ctxt.Arch.ByteOrder))])
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpSB || !(symIsRO(sym)) {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(int32(read16(sym, int64(off), config.ctxt.Arch.ByteOrder)))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVWstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVWstore [off] {sym} ptr (MOVWQSX x) mem)
	// result: (MOVWstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVWQSX {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64MOVWstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVWstore [off] {sym} ptr (MOVWQZX x) mem)
	// result: (MOVWstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVWQZX {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64MOVWstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (MOVWstore [off1] {sym} (ADDQconst [off2] ptr) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MOVWstore [off1+off2] {sym} ptr val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MOVWstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (MOVWstore [off] {sym} ptr (MOVLconst [c]) mem)
	// result: (MOVWstoreconst [makeValAndOff(int32(int16(c)),off)] {sym} ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64MOVWstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(int16(c)), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVWstore [off] {sym} ptr (MOVQconst [c]) mem)
	// result: (MOVWstoreconst [makeValAndOff(int32(int16(c)),off)] {sym} ptr mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64MOVWstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(int32(int16(c)), off))
		v.Aux = symToAux(sym)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVWstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MOVWstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MOVWstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (MOVWstore [i] {s} p x:(ROLWconst [8] w) mem)
	// cond: x.Uses == 1 && buildcfg.GOAMD64 >= 3
	// result: (MOVBEWstore [i] {s} p w mem)
	for {
		i := auxIntToInt32(v.AuxInt)
		s := auxToSym(v.Aux)
		p := v_0
		x := v_1
		if x.Op != OpAMD64ROLWconst || auxIntToInt8(x.AuxInt) != 8 {
			break
		}
		w := x.Args[0]
		mem := v_2
		if !(x.Uses == 1 && buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64MOVBEWstore)
		v.AuxInt = int32ToAuxInt(i)
		v.Aux = symToAux(s)
		v.AddArg3(p, w, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MOVWstoreconst(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MOVWstoreconst [sc] {s} (ADDQconst [off] ptr) mem)
	// cond: ValAndOff(sc).canAdd32(off)
	// result: (MOVWstoreconst [ValAndOff(sc).addOffset32(off)] {s} ptr mem)
	for {
		sc := auxIntToValAndOff(v.AuxInt)
		s := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off := auxIntToInt32(v_0.AuxInt)
		ptr := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(sc).canAdd32(off)) {
			break
		}
		v.reset(OpAMD64MOVWstoreconst)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(sc).addOffset32(off))
		v.Aux = symToAux(s)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (MOVWstoreconst [sc] {sym1} (LEAQ [off] {sym2} ptr) mem)
	// cond: canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd32(off)
	// result: (MOVWstoreconst [ValAndOff(sc).addOffset32(off)] {mergeSym(sym1, sym2)} ptr mem)
	for {
		sc := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		ptr := v_0.Args[0]
		mem := v_1
		if !(canMergeSym(sym1, sym2) && ValAndOff(sc).canAdd32(off)) {
			break
		}
		v.reset(OpAMD64MOVWstoreconst)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(sc).addOffset32(off))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MULL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MULL x (MOVLconst [c]))
	// result: (MULLconst [c] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVLconst {
				continue
			}
			c := auxIntToInt32(v_1.AuxInt)
			v.reset(OpAMD64MULLconst)
			v.AuxInt = int32ToAuxInt(c)
			v.AddArg(x)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64MULLconst(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	config := b.Func.Config
	// match: (MULLconst [c] (MULLconst [d] x))
	// result: (MULLconst [c * d] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MULLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		v.reset(OpAMD64MULLconst)
		v.AuxInt = int32ToAuxInt(c * d)
		v.AddArg(x)
		return true
	}
	// match: (MULLconst [ 0] _)
	// result: (MOVLconst [0])
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (MULLconst [ 1] x)
	// result: x
	for {
		if auxIntToInt32(v.AuxInt) != 1 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (MULLconst [c] x)
	// cond: v.Type.Size() <= 4 && canMulStrengthReduce32(config, c)
	// result: {mulStrengthReduce32(v, x, c)}
	for {
		c := auxIntToInt32(v.AuxInt)
		x := v_0
		if !(v.Type.Size() <= 4 && canMulStrengthReduce32(config, c)) {
			break
		}
		v.copyOf(mulStrengthReduce32(v, x, c))
		return true
	}
	// match: (MULLconst [c] (MOVLconst [d]))
	// result: (MOVLconst [c*d])
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(c * d)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MULQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MULQ x (MOVQconst [c]))
	// cond: is32Bit(c)
	// result: (MULQconst [int32(c)] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_1.AuxInt)
			if !(is32Bit(c)) {
				continue
			}
			v.reset(OpAMD64MULQconst)
			v.AuxInt = int32ToAuxInt(int32(c))
			v.AddArg(x)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64MULQconst(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	config := b.Func.Config
	// match: (MULQconst [c] (MULQconst [d] x))
	// cond: is32Bit(int64(c)*int64(d))
	// result: (MULQconst [c * d] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MULQconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(is32Bit(int64(c) * int64(d))) {
			break
		}
		v.reset(OpAMD64MULQconst)
		v.AuxInt = int32ToAuxInt(c * d)
		v.AddArg(x)
		return true
	}
	// match: (MULQconst [ 0] _)
	// result: (MOVQconst [0])
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(0)
		return true
	}
	// match: (MULQconst [ 1] x)
	// result: x
	for {
		if auxIntToInt32(v.AuxInt) != 1 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (MULQconst [c] x)
	// cond: canMulStrengthReduce(config, int64(c))
	// result: {mulStrengthReduce(v, x, int64(c))}
	for {
		c := auxIntToInt32(v.AuxInt)
		x := v_0
		if !(canMulStrengthReduce(config, int64(c))) {
			break
		}
		v.copyOf(mulStrengthReduce(v, x, int64(c)))
		return true
	}
	// match: (MULQconst [c] (MOVQconst [d]))
	// result: (MOVQconst [int64(c)*d])
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(c) * d)
		return true
	}
	// match: (MULQconst [c] (NEGQ x))
	// cond: c != -(1<<31)
	// result: (MULQconst [-c] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64NEGQ {
			break
		}
		x := v_0.Args[0]
		if !(c != -(1 << 31)) {
			break
		}
		v.reset(OpAMD64MULQconst)
		v.AuxInt = int32ToAuxInt(-c)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MULSD(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MULSD x l:(MOVSDload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (MULSDload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVSDload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64MULSDload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64MULSDload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MULSDload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MULSDload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MULSDload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (MULSDload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MULSDload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MULSDload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (MULSDload x [off] {sym} ptr (MOVQstore [off] {sym} ptr y _))
	// result: (MULSD x (MOVQi2f y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVQstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64MULSD)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVQi2f, typ.Float64)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64MULSS(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (MULSS x l:(MOVSSload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (MULSSload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVSSload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64MULSSload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64MULSSload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MULSSload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (MULSSload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64MULSSload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (MULSSload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (MULSSload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64MULSSload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (MULSSload x [off] {sym} ptr (MOVLstore [off] {sym} ptr y _))
	// result: (MULSS x (MOVLi2f y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVLstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64MULSS)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVLi2f, typ.Float32)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64NEGL(v *Value) bool {
	v_0 := v.Args[0]
	// match: (NEGL (NEGL x))
	// result: x
	for {
		if v_0.Op != OpAMD64NEGL {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	// match: (NEGL s:(SUBL x y))
	// cond: s.Uses == 1
	// result: (SUBL y x)
	for {
		s := v_0
		if s.Op != OpAMD64SUBL {
			break
		}
		y := s.Args[1]
		x := s.Args[0]
		if !(s.Uses == 1) {
			break
		}
		v.reset(OpAMD64SUBL)
		v.AddArg2(y, x)
		return true
	}
	// match: (NEGL (MOVLconst [c]))
	// result: (MOVLconst [-c])
	for {
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(-c)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64NEGQ(v *Value) bool {
	v_0 := v.Args[0]
	// match: (NEGQ (NEGQ x))
	// result: x
	for {
		if v_0.Op != OpAMD64NEGQ {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	// match: (NEGQ s:(SUBQ x y))
	// cond: s.Uses == 1
	// result: (SUBQ y x)
	for {
		s := v_0
		if s.Op != OpAMD64SUBQ {
			break
		}
		y := s.Args[1]
		x := s.Args[0]
		if !(s.Uses == 1) {
			break
		}
		v.reset(OpAMD64SUBQ)
		v.AddArg2(y, x)
		return true
	}
	// match: (NEGQ (MOVQconst [c]))
	// result: (MOVQconst [-c])
	for {
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(-c)
		return true
	}
	// match: (NEGQ (ADDQconst [c] (NEGQ x)))
	// cond: c != -(1<<31)
	// result: (ADDQconst [-c] x)
	for {
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		v_0_0 := v_0.Args[0]
		if v_0_0.Op != OpAMD64NEGQ {
			break
		}
		x := v_0_0.Args[0]
		if !(c != -(1 << 31)) {
			break
		}
		v.reset(OpAMD64ADDQconst)
		v.AuxInt = int32ToAuxInt(-c)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64NOTL(v *Value) bool {
	v_0 := v.Args[0]
	// match: (NOTL (MOVLconst [c]))
	// result: (MOVLconst [^c])
	for {
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(^c)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64NOTQ(v *Value) bool {
	v_0 := v.Args[0]
	// match: (NOTQ (MOVQconst [c]))
	// result: (MOVQconst [^c])
	for {
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(^c)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ORL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ORL (SHLL (MOVLconst [1]) y) x)
	// result: (BTSL x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64SHLL {
				continue
			}
			y := v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_0_0.AuxInt) != 1 {
				continue
			}
			x := v_1
			v.reset(OpAMD64BTSL)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ORL x (MOVLconst [c]))
	// result: (ORLconst [c] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVLconst {
				continue
			}
			c := auxIntToInt32(v_1.AuxInt)
			v.reset(OpAMD64ORLconst)
			v.AuxInt = int32ToAuxInt(c)
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ORL x x)
	// result: x
	for {
		x := v_0
		if x != v_1 {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (ORL x l:(MOVLload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (ORLload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVLload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64ORLload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64ORLconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (ORLconst [c] (ORLconst [d] x))
	// result: (ORLconst [c | d] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ORLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		v.reset(OpAMD64ORLconst)
		v.AuxInt = int32ToAuxInt(c | d)
		v.AddArg(x)
		return true
	}
	// match: (ORLconst [c] x)
	// cond: c==0
	// result: x
	for {
		c := auxIntToInt32(v.AuxInt)
		x := v_0
		if !(c == 0) {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (ORLconst [c] _)
	// cond: c==-1
	// result: (MOVLconst [-1])
	for {
		c := auxIntToInt32(v.AuxInt)
		if !(c == -1) {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(-1)
		return true
	}
	// match: (ORLconst [c] (MOVLconst [d]))
	// result: (MOVLconst [c|d])
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(c | d)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ORLconstmodify(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ORLconstmodify [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (ORLconstmodify [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64ORLconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (ORLconstmodify [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (ORLconstmodify [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ORLconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ORLload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (ORLload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ORLload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ORLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ORLload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ORLload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ORLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: ( ORLload x [off] {sym} ptr (MOVSSstore [off] {sym} ptr y _))
	// result: ( ORL x (MOVLf2i y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVSSstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64ORL)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVLf2i, typ.UInt32)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ORLmodify(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ORLmodify [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ORLmodify [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ORLmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (ORLmodify [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ORLmodify [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ORLmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ORQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ORQ (SHLQ (MOVQconst [1]) y) x)
	// result: (BTSQ x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64SHLQ {
				continue
			}
			y := v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64MOVQconst || auxIntToInt64(v_0_0.AuxInt) != 1 {
				continue
			}
			x := v_1
			v.reset(OpAMD64BTSQ)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (ORQ (MOVQconst [c]) x)
	// cond: isUint64PowerOfTwo(c) && uint64(c) >= 1<<31
	// result: (BTSQconst [int8(log64(c))] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_0.AuxInt)
			x := v_1
			if !(isUint64PowerOfTwo(c) && uint64(c) >= 1<<31) {
				continue
			}
			v.reset(OpAMD64BTSQconst)
			v.AuxInt = int8ToAuxInt(int8(log64(c)))
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ORQ x (MOVQconst [c]))
	// cond: is32Bit(c)
	// result: (ORQconst [int32(c)] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_1.AuxInt)
			if !(is32Bit(c)) {
				continue
			}
			v.reset(OpAMD64ORQconst)
			v.AuxInt = int32ToAuxInt(int32(c))
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ORQ x (MOVLconst [c]))
	// result: (ORQconst [c] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVLconst {
				continue
			}
			c := auxIntToInt32(v_1.AuxInt)
			v.reset(OpAMD64ORQconst)
			v.AuxInt = int32ToAuxInt(c)
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (ORQ (SHRQ lo bits) (SHLQ hi (NEGQ bits)))
	// result: (SHRDQ lo hi bits)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64SHRQ {
				continue
			}
			bits := v_0.Args[1]
			lo := v_0.Args[0]
			if v_1.Op != OpAMD64SHLQ {
				continue
			}
			_ = v_1.Args[1]
			hi := v_1.Args[0]
			v_1_1 := v_1.Args[1]
			if v_1_1.Op != OpAMD64NEGQ || bits != v_1_1.Args[0] {
				continue
			}
			v.reset(OpAMD64SHRDQ)
			v.AddArg3(lo, hi, bits)
			return true
		}
		break
	}
	// match: (ORQ (SHLQ lo bits) (SHRQ hi (NEGQ bits)))
	// result: (SHLDQ lo hi bits)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64SHLQ {
				continue
			}
			bits := v_0.Args[1]
			lo := v_0.Args[0]
			if v_1.Op != OpAMD64SHRQ {
				continue
			}
			_ = v_1.Args[1]
			hi := v_1.Args[0]
			v_1_1 := v_1.Args[1]
			if v_1_1.Op != OpAMD64NEGQ || bits != v_1_1.Args[0] {
				continue
			}
			v.reset(OpAMD64SHLDQ)
			v.AddArg3(lo, hi, bits)
			return true
		}
		break
	}
	// match: (ORQ (SHRXQ lo bits) (SHLXQ hi (NEGQ bits)))
	// result: (SHRDQ lo hi bits)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64SHRXQ {
				continue
			}
			bits := v_0.Args[1]
			lo := v_0.Args[0]
			if v_1.Op != OpAMD64SHLXQ {
				continue
			}
			_ = v_1.Args[1]
			hi := v_1.Args[0]
			v_1_1 := v_1.Args[1]
			if v_1_1.Op != OpAMD64NEGQ || bits != v_1_1.Args[0] {
				continue
			}
			v.reset(OpAMD64SHRDQ)
			v.AddArg3(lo, hi, bits)
			return true
		}
		break
	}
	// match: (ORQ (SHLXQ lo bits) (SHRXQ hi (NEGQ bits)))
	// result: (SHLDQ lo hi bits)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64SHLXQ {
				continue
			}
			bits := v_0.Args[1]
			lo := v_0.Args[0]
			if v_1.Op != OpAMD64SHRXQ {
				continue
			}
			_ = v_1.Args[1]
			hi := v_1.Args[0]
			v_1_1 := v_1.Args[1]
			if v_1_1.Op != OpAMD64NEGQ || bits != v_1_1.Args[0] {
				continue
			}
			v.reset(OpAMD64SHLDQ)
			v.AddArg3(lo, hi, bits)
			return true
		}
		break
	}
	// match: (ORQ (MOVQconst [c]) (MOVQconst [d]))
	// result: (MOVQconst [c|d])
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_0.AuxInt)
			if v_1.Op != OpAMD64MOVQconst {
				continue
			}
			d := auxIntToInt64(v_1.AuxInt)
			v.reset(OpAMD64MOVQconst)
			v.AuxInt = int64ToAuxInt(c | d)
			return true
		}
		break
	}
	// match: (ORQ x x)
	// result: x
	for {
		x := v_0
		if x != v_1 {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (ORQ x l:(MOVQload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (ORQload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVQload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64ORQload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64ORQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (ORQconst [c] (ORQconst [d] x))
	// result: (ORQconst [c | d] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64ORQconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		v.reset(OpAMD64ORQconst)
		v.AuxInt = int32ToAuxInt(c | d)
		v.AddArg(x)
		return true
	}
	// match: (ORQconst [0] x)
	// result: x
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (ORQconst [-1] _)
	// result: (MOVQconst [-1])
	for {
		if auxIntToInt32(v.AuxInt) != -1 {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(-1)
		return true
	}
	// match: (ORQconst [c] (MOVQconst [d]))
	// result: (MOVQconst [int64(c)|d])
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(c) | d)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ORQconstmodify(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ORQconstmodify [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (ORQconstmodify [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64ORQconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (ORQconstmodify [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (ORQconstmodify [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ORQconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ORQload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (ORQload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ORQload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ORQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (ORQload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ORQload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ORQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: ( ORQload x [off] {sym} ptr (MOVSDstore [off] {sym} ptr y _))
	// result: ( ORQ x (MOVQf2i y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVSDstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64ORQ)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVQf2i, typ.UInt64)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ORQmodify(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ORQmodify [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (ORQmodify [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64ORQmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (ORQmodify [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (ORQmodify [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64ORQmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ROLB(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ROLB x (NEGQ y))
	// result: (RORB x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64RORB)
		v.AddArg2(x, y)
		return true
	}
	// match: (ROLB x (NEGL y))
	// result: (RORB x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64RORB)
		v.AddArg2(x, y)
		return true
	}
	// match: (ROLB x (MOVQconst [c]))
	// result: (ROLBconst [int8(c&7) ] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64ROLBconst)
		v.AuxInt = int8ToAuxInt(int8(c & 7))
		v.AddArg(x)
		return true
	}
	// match: (ROLB x (MOVLconst [c]))
	// result: (ROLBconst [int8(c&7) ] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64ROLBconst)
		v.AuxInt = int8ToAuxInt(int8(c & 7))
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ROLBconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (ROLBconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ROLL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ROLL x (NEGQ y))
	// result: (RORL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64RORL)
		v.AddArg2(x, y)
		return true
	}
	// match: (ROLL x (NEGL y))
	// result: (RORL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64RORL)
		v.AddArg2(x, y)
		return true
	}
	// match: (ROLL x (MOVQconst [c]))
	// result: (ROLLconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64ROLLconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	// match: (ROLL x (MOVLconst [c]))
	// result: (ROLLconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64ROLLconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ROLLconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (ROLLconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ROLQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ROLQ x (NEGQ y))
	// result: (RORQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64RORQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (ROLQ x (NEGL y))
	// result: (RORQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64RORQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (ROLQ x (MOVQconst [c]))
	// result: (ROLQconst [int8(c&63)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64ROLQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v.AddArg(x)
		return true
	}
	// match: (ROLQ x (MOVLconst [c]))
	// result: (ROLQconst [int8(c&63)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64ROLQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ROLQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (ROLQconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ROLW(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ROLW x (NEGQ y))
	// result: (RORW x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64RORW)
		v.AddArg2(x, y)
		return true
	}
	// match: (ROLW x (NEGL y))
	// result: (RORW x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64RORW)
		v.AddArg2(x, y)
		return true
	}
	// match: (ROLW x (MOVQconst [c]))
	// result: (ROLWconst [int8(c&15)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64ROLWconst)
		v.AuxInt = int8ToAuxInt(int8(c & 15))
		v.AddArg(x)
		return true
	}
	// match: (ROLW x (MOVLconst [c]))
	// result: (ROLWconst [int8(c&15)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64ROLWconst)
		v.AuxInt = int8ToAuxInt(int8(c & 15))
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64ROLWconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (ROLWconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64RORB(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (RORB x (NEGQ y))
	// result: (ROLB x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64ROLB)
		v.AddArg2(x, y)
		return true
	}
	// match: (RORB x (NEGL y))
	// result: (ROLB x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64ROLB)
		v.AddArg2(x, y)
		return true
	}
	// match: (RORB x (MOVQconst [c]))
	// result: (ROLBconst [int8((-c)&7) ] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64ROLBconst)
		v.AuxInt = int8ToAuxInt(int8((-c) & 7))
		v.AddArg(x)
		return true
	}
	// match: (RORB x (MOVLconst [c]))
	// result: (ROLBconst [int8((-c)&7) ] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64ROLBconst)
		v.AuxInt = int8ToAuxInt(int8((-c) & 7))
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64RORL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (RORL x (NEGQ y))
	// result: (ROLL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64ROLL)
		v.AddArg2(x, y)
		return true
	}
	// match: (RORL x (NEGL y))
	// result: (ROLL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64ROLL)
		v.AddArg2(x, y)
		return true
	}
	// match: (RORL x (MOVQconst [c]))
	// result: (ROLLconst [int8((-c)&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64ROLLconst)
		v.AuxInt = int8ToAuxInt(int8((-c) & 31))
		v.AddArg(x)
		return true
	}
	// match: (RORL x (MOVLconst [c]))
	// result: (ROLLconst [int8((-c)&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64ROLLconst)
		v.AuxInt = int8ToAuxInt(int8((-c) & 31))
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64RORQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (RORQ x (NEGQ y))
	// result: (ROLQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64ROLQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (RORQ x (NEGL y))
	// result: (ROLQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64ROLQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (RORQ x (MOVQconst [c]))
	// result: (ROLQconst [int8((-c)&63)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64ROLQconst)
		v.AuxInt = int8ToAuxInt(int8((-c) & 63))
		v.AddArg(x)
		return true
	}
	// match: (RORQ x (MOVLconst [c]))
	// result: (ROLQconst [int8((-c)&63)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64ROLQconst)
		v.AuxInt = int8ToAuxInt(int8((-c) & 63))
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64RORW(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (RORW x (NEGQ y))
	// result: (ROLW x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64ROLW)
		v.AddArg2(x, y)
		return true
	}
	// match: (RORW x (NEGL y))
	// result: (ROLW x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		y := v_1.Args[0]
		v.reset(OpAMD64ROLW)
		v.AddArg2(x, y)
		return true
	}
	// match: (RORW x (MOVQconst [c]))
	// result: (ROLWconst [int8((-c)&15)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64ROLWconst)
		v.AuxInt = int8ToAuxInt(int8((-c) & 15))
		v.AddArg(x)
		return true
	}
	// match: (RORW x (MOVLconst [c]))
	// result: (ROLWconst [int8((-c)&15)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64ROLWconst)
		v.AuxInt = int8ToAuxInt(int8((-c) & 15))
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SARB(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SARB x (MOVQconst [c]))
	// result: (SARBconst [int8(min(int64(c)&31,7))] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64SARBconst)
		v.AuxInt = int8ToAuxInt(int8(min(int64(c)&31, 7)))
		v.AddArg(x)
		return true
	}
	// match: (SARB x (MOVLconst [c]))
	// result: (SARBconst [int8(min(int64(c)&31,7))] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64SARBconst)
		v.AuxInt = int8ToAuxInt(int8(min(int64(c)&31, 7)))
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SARBconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SARBconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (SARBconst [c] (MOVQconst [d]))
	// result: (MOVQconst [int64(int8(d))>>uint64(c)])
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(int8(d)) >> uint64(c))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SARL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (SARL x (MOVQconst [c]))
	// result: (SARLconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64SARLconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	// match: (SARL x (MOVLconst [c]))
	// result: (SARLconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64SARLconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	// match: (SARL x (ADDQconst [c] y))
	// cond: c & 31 == 0
	// result: (SARL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SARL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SARL x (NEGQ <t> (ADDQconst [c] y)))
	// cond: c & 31 == 0
	// result: (SARL x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SARL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SARL x (ANDQconst [c] y))
	// cond: c & 31 == 31
	// result: (SARL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SARL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SARL x (NEGQ <t> (ANDQconst [c] y)))
	// cond: c & 31 == 31
	// result: (SARL x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SARL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SARL x (ADDLconst [c] y))
	// cond: c & 31 == 0
	// result: (SARL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SARL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SARL x (NEGL <t> (ADDLconst [c] y)))
	// cond: c & 31 == 0
	// result: (SARL x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SARL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SARL x (ANDLconst [c] y))
	// cond: c & 31 == 31
	// result: (SARL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SARL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SARL x (NEGL <t> (ANDLconst [c] y)))
	// cond: c & 31 == 31
	// result: (SARL x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SARL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SARL l:(MOVLload [off] {sym} ptr mem) x)
	// cond: buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)
	// result: (SARXLload [off] {sym} ptr x mem)
	for {
		l := v_0
		if l.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		x := v_1
		if !(buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64SARXLload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SARLconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SARLconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (SARLconst [c] (MOVQconst [d]))
	// result: (MOVQconst [int64(int32(d))>>uint64(c)])
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(int32(d)) >> uint64(c))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SARQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (SARQ x (MOVQconst [c]))
	// result: (SARQconst [int8(c&63)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64SARQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v.AddArg(x)
		return true
	}
	// match: (SARQ x (MOVLconst [c]))
	// result: (SARQconst [int8(c&63)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64SARQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v.AddArg(x)
		return true
	}
	// match: (SARQ x (ADDQconst [c] y))
	// cond: c & 63 == 0
	// result: (SARQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SARQ x (NEGQ <t> (ADDQconst [c] y)))
	// cond: c & 63 == 0
	// result: (SARQ x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SARQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SARQ x (ANDQconst [c] y))
	// cond: c & 63 == 63
	// result: (SARQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SARQ x (NEGQ <t> (ANDQconst [c] y)))
	// cond: c & 63 == 63
	// result: (SARQ x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SARQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SARQ x (ADDLconst [c] y))
	// cond: c & 63 == 0
	// result: (SARQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SARQ x (NEGL <t> (ADDLconst [c] y)))
	// cond: c & 63 == 0
	// result: (SARQ x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SARQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SARQ x (ANDLconst [c] y))
	// cond: c & 63 == 63
	// result: (SARQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SARQ x (NEGL <t> (ANDLconst [c] y)))
	// cond: c & 63 == 63
	// result: (SARQ x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SARQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SARQ l:(MOVQload [off] {sym} ptr mem) x)
	// cond: buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)
	// result: (SARXQload [off] {sym} ptr x mem)
	for {
		l := v_0
		if l.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		x := v_1
		if !(buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64SARXQload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SARQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SARQconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (SARQconst [c] (MOVQconst [d]))
	// result: (MOVQconst [d>>uint64(c)])
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(d >> uint64(c))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SARW(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SARW x (MOVQconst [c]))
	// result: (SARWconst [int8(min(int64(c)&31,15))] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64SARWconst)
		v.AuxInt = int8ToAuxInt(int8(min(int64(c)&31, 15)))
		v.AddArg(x)
		return true
	}
	// match: (SARW x (MOVLconst [c]))
	// result: (SARWconst [int8(min(int64(c)&31,15))] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64SARWconst)
		v.AuxInt = int8ToAuxInt(int8(min(int64(c)&31, 15)))
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SARWconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SARWconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (SARWconst [c] (MOVQconst [d]))
	// result: (MOVQconst [int64(int16(d))>>uint64(c)])
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(int16(d)) >> uint64(c))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SARXLload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SARXLload [off] {sym} ptr (MOVLconst [c]) mem)
	// result: (SARLconst [int8(c&31)] (MOVLload [off] {sym} ptr mem))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64SARLconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, typ.UInt32)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SARXQload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SARXQload [off] {sym} ptr (MOVQconst [c]) mem)
	// result: (SARQconst [int8(c&63)] (MOVQload [off] {sym} ptr mem))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64SARQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		v.AddArg(v0)
		return true
	}
	// match: (SARXQload [off] {sym} ptr (MOVLconst [c]) mem)
	// result: (SARQconst [int8(c&63)] (MOVQload [off] {sym} ptr mem))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64SARQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SBBLcarrymask(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SBBLcarrymask (FlagEQ))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SBBLcarrymask (FlagLT_ULT))
	// result: (MOVLconst [-1])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(-1)
		return true
	}
	// match: (SBBLcarrymask (FlagLT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SBBLcarrymask (FlagGT_ULT))
	// result: (MOVLconst [-1])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(-1)
		return true
	}
	// match: (SBBLcarrymask (FlagGT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SBBQ(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SBBQ x (MOVQconst [c]) borrow)
	// cond: is32Bit(c)
	// result: (SBBQconst x [int32(c)] borrow)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		borrow := v_2
		if !(is32Bit(c)) {
			break
		}
		v.reset(OpAMD64SBBQconst)
		v.AuxInt = int32ToAuxInt(int32(c))
		v.AddArg2(x, borrow)
		return true
	}
	// match: (SBBQ x y (FlagEQ))
	// result: (SUBQborrow x y)
	for {
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64SUBQborrow)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SBBQcarrymask(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SBBQcarrymask (FlagEQ))
	// result: (MOVQconst [0])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(0)
		return true
	}
	// match: (SBBQcarrymask (FlagLT_ULT))
	// result: (MOVQconst [-1])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(-1)
		return true
	}
	// match: (SBBQcarrymask (FlagLT_UGT))
	// result: (MOVQconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(0)
		return true
	}
	// match: (SBBQcarrymask (FlagGT_ULT))
	// result: (MOVQconst [-1])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(-1)
		return true
	}
	// match: (SBBQcarrymask (FlagGT_UGT))
	// result: (MOVQconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SBBQconst(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SBBQconst x [c] (FlagEQ))
	// result: (SUBQconstborrow x [c])
	for {
		c := auxIntToInt32(v.AuxInt)
		x := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64SUBQconstborrow)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETA(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SETA (InvertFlags x))
	// result: (SETB x)
	for {
		if v_0.Op != OpAMD64InvertFlags {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETB)
		v.AddArg(x)
		return true
	}
	// match: (SETA (FlagEQ))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETA (FlagLT_ULT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETA (FlagLT_UGT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETA (FlagGT_ULT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETA (FlagGT_UGT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETAE(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SETAE (TESTQ x x))
	// result: (ConstBool [true])
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpConstBool)
		v.AuxInt = boolToAuxInt(true)
		return true
	}
	// match: (SETAE (TESTL x x))
	// result: (ConstBool [true])
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpConstBool)
		v.AuxInt = boolToAuxInt(true)
		return true
	}
	// match: (SETAE (TESTW x x))
	// result: (ConstBool [true])
	for {
		if v_0.Op != OpAMD64TESTW {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpConstBool)
		v.AuxInt = boolToAuxInt(true)
		return true
	}
	// match: (SETAE (TESTB x x))
	// result: (ConstBool [true])
	for {
		if v_0.Op != OpAMD64TESTB {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpConstBool)
		v.AuxInt = boolToAuxInt(true)
		return true
	}
	// match: (SETAE (BTLconst [0] x))
	// result: (XORLconst [1] (ANDLconst <typ.Bool> [1] x))
	for {
		if v_0.Op != OpAMD64BTLconst || auxIntToInt8(v_0.AuxInt) != 0 {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64XORLconst)
		v.AuxInt = int32ToAuxInt(1)
		v0 := b.NewValue0(v.Pos, OpAMD64ANDLconst, typ.Bool)
		v0.AuxInt = int32ToAuxInt(1)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETAE (BTQconst [0] x))
	// result: (XORLconst [1] (ANDLconst <typ.Bool> [1] x))
	for {
		if v_0.Op != OpAMD64BTQconst || auxIntToInt8(v_0.AuxInt) != 0 {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64XORLconst)
		v.AuxInt = int32ToAuxInt(1)
		v0 := b.NewValue0(v.Pos, OpAMD64ANDLconst, typ.Bool)
		v0.AuxInt = int32ToAuxInt(1)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETAE c:(CMPQconst [128] x))
	// cond: c.Uses == 1
	// result: (SETA (CMPQconst [127] x))
	for {
		c := v_0
		if c.Op != OpAMD64CMPQconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		x := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETA)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETAE c:(CMPLconst [128] x))
	// cond: c.Uses == 1
	// result: (SETA (CMPLconst [127] x))
	for {
		c := v_0
		if c.Op != OpAMD64CMPLconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		x := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETA)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETAE (InvertFlags x))
	// result: (SETBE x)
	for {
		if v_0.Op != OpAMD64InvertFlags {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETBE)
		v.AddArg(x)
		return true
	}
	// match: (SETAE (FlagEQ))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETAE (FlagLT_ULT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETAE (FlagLT_UGT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETAE (FlagGT_ULT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETAE (FlagGT_UGT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETAEstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SETAEstore [off] {sym} ptr (InvertFlags x) mem)
	// result: (SETBEstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64InvertFlags {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64SETBEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (SETAEstore [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SETAEstore [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SETAEstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETAEstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SETAEstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SETAEstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETAEstore [off] {sym} ptr (FlagEQ) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETAEstore [off] {sym} ptr (FlagLT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETAEstore [off] {sym} ptr (FlagLT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETAEstore [off] {sym} ptr (FlagGT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETAEstore [off] {sym} ptr (FlagGT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETAstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SETAstore [off] {sym} ptr (InvertFlags x) mem)
	// result: (SETBstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64InvertFlags {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64SETBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (SETAstore [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SETAstore [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SETAstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETAstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SETAstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SETAstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETAstore [off] {sym} ptr (FlagEQ) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETAstore [off] {sym} ptr (FlagLT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETAstore [off] {sym} ptr (FlagLT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETAstore [off] {sym} ptr (FlagGT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETAstore [off] {sym} ptr (FlagGT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETB(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (SETB (TESTQ x x))
	// result: (ConstBool [false])
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpConstBool)
		v.AuxInt = boolToAuxInt(false)
		return true
	}
	// match: (SETB (TESTL x x))
	// result: (ConstBool [false])
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpConstBool)
		v.AuxInt = boolToAuxInt(false)
		return true
	}
	// match: (SETB (TESTW x x))
	// result: (ConstBool [false])
	for {
		if v_0.Op != OpAMD64TESTW {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpConstBool)
		v.AuxInt = boolToAuxInt(false)
		return true
	}
	// match: (SETB (TESTB x x))
	// result: (ConstBool [false])
	for {
		if v_0.Op != OpAMD64TESTB {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpConstBool)
		v.AuxInt = boolToAuxInt(false)
		return true
	}
	// match: (SETB (BTLconst [0] x))
	// result: (ANDLconst [1] x)
	for {
		if v_0.Op != OpAMD64BTLconst || auxIntToInt8(v_0.AuxInt) != 0 {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(1)
		v.AddArg(x)
		return true
	}
	// match: (SETB (BTQconst [0] x))
	// result: (ANDQconst [1] x)
	for {
		if v_0.Op != OpAMD64BTQconst || auxIntToInt8(v_0.AuxInt) != 0 {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64ANDQconst)
		v.AuxInt = int32ToAuxInt(1)
		v.AddArg(x)
		return true
	}
	// match: (SETB c:(CMPQconst [128] x))
	// cond: c.Uses == 1
	// result: (SETBE (CMPQconst [127] x))
	for {
		c := v_0
		if c.Op != OpAMD64CMPQconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		x := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETBE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETB c:(CMPLconst [128] x))
	// cond: c.Uses == 1
	// result: (SETBE (CMPLconst [127] x))
	for {
		c := v_0
		if c.Op != OpAMD64CMPLconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		x := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETBE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETB (InvertFlags x))
	// result: (SETA x)
	for {
		if v_0.Op != OpAMD64InvertFlags {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETA)
		v.AddArg(x)
		return true
	}
	// match: (SETB (FlagEQ))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETB (FlagLT_ULT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETB (FlagLT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETB (FlagGT_ULT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETB (FlagGT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETBE(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SETBE (InvertFlags x))
	// result: (SETAE x)
	for {
		if v_0.Op != OpAMD64InvertFlags {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETAE)
		v.AddArg(x)
		return true
	}
	// match: (SETBE (FlagEQ))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETBE (FlagLT_ULT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETBE (FlagLT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETBE (FlagGT_ULT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETBE (FlagGT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETBEstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SETBEstore [off] {sym} ptr (InvertFlags x) mem)
	// result: (SETAEstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64InvertFlags {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64SETAEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (SETBEstore [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SETBEstore [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SETBEstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETBEstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SETBEstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SETBEstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETBEstore [off] {sym} ptr (FlagEQ) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETBEstore [off] {sym} ptr (FlagLT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETBEstore [off] {sym} ptr (FlagLT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETBEstore [off] {sym} ptr (FlagGT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETBEstore [off] {sym} ptr (FlagGT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETBstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SETBstore [off] {sym} ptr (InvertFlags x) mem)
	// result: (SETAstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64InvertFlags {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64SETAstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (SETBstore [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SETBstore [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SETBstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETBstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SETBstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SETBstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETBstore [off] {sym} ptr (FlagEQ) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETBstore [off] {sym} ptr (FlagLT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETBstore [off] {sym} ptr (FlagLT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETBstore [off] {sym} ptr (FlagGT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETBstore [off] {sym} ptr (FlagGT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETEQ(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (SETEQ (TESTL (SHLL (MOVLconst [1]) x) y))
	// result: (SETAE (BTL x y))
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			if v_0_0.Op != OpAMD64SHLL {
				continue
			}
			x := v_0_0.Args[1]
			v_0_0_0 := v_0_0.Args[0]
			if v_0_0_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_0_0_0.AuxInt) != 1 {
				continue
			}
			y := v_0_1
			v.reset(OpAMD64SETAE)
			v0 := b.NewValue0(v.Pos, OpAMD64BTL, types.TypeFlags)
			v0.AddArg2(x, y)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETEQ (TESTQ (SHLQ (MOVQconst [1]) x) y))
	// result: (SETAE (BTQ x y))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			if v_0_0.Op != OpAMD64SHLQ {
				continue
			}
			x := v_0_0.Args[1]
			v_0_0_0 := v_0_0.Args[0]
			if v_0_0_0.Op != OpAMD64MOVQconst || auxIntToInt64(v_0_0_0.AuxInt) != 1 {
				continue
			}
			y := v_0_1
			v.reset(OpAMD64SETAE)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQ, types.TypeFlags)
			v0.AddArg2(x, y)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETEQ (TESTLconst [c] x))
	// cond: isUint32PowerOfTwo(int64(c))
	// result: (SETAE (BTLconst [int8(log32(c))] x))
	for {
		if v_0.Op != OpAMD64TESTLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(isUint32PowerOfTwo(int64(c))) {
			break
		}
		v.reset(OpAMD64SETAE)
		v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
		v0.AuxInt = int8ToAuxInt(int8(log32(c)))
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETEQ (TESTQconst [c] x))
	// cond: isUint64PowerOfTwo(int64(c))
	// result: (SETAE (BTQconst [int8(log32(c))] x))
	for {
		if v_0.Op != OpAMD64TESTQconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(isUint64PowerOfTwo(int64(c))) {
			break
		}
		v.reset(OpAMD64SETAE)
		v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
		v0.AuxInt = int8ToAuxInt(int8(log32(c)))
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETEQ (TESTQ (MOVQconst [c]) x))
	// cond: isUint64PowerOfTwo(c)
	// result: (SETAE (BTQconst [int8(log64(c))] x))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			if v_0_0.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_0_0.AuxInt)
			x := v_0_1
			if !(isUint64PowerOfTwo(c)) {
				continue
			}
			v.reset(OpAMD64SETAE)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(int8(log64(c)))
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETEQ (CMPLconst [1] s:(ANDLconst [1] _)))
	// result: (SETNE (CMPLconst [0] s))
	for {
		if v_0.Op != OpAMD64CMPLconst || auxIntToInt32(v_0.AuxInt) != 1 {
			break
		}
		s := v_0.Args[0]
		if s.Op != OpAMD64ANDLconst || auxIntToInt32(s.AuxInt) != 1 {
			break
		}
		v.reset(OpAMD64SETNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v0.AddArg(s)
		v.AddArg(v0)
		return true
	}
	// match: (SETEQ (CMPQconst [1] s:(ANDQconst [1] _)))
	// result: (SETNE (CMPQconst [0] s))
	for {
		if v_0.Op != OpAMD64CMPQconst || auxIntToInt32(v_0.AuxInt) != 1 {
			break
		}
		s := v_0.Args[0]
		if s.Op != OpAMD64ANDQconst || auxIntToInt32(s.AuxInt) != 1 {
			break
		}
		v.reset(OpAMD64SETNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v0.AddArg(s)
		v.AddArg(v0)
		return true
	}
	// match: (SETEQ (TESTQ z1:(SHLQconst [63] (SHRQconst [63] x)) z2))
	// cond: z1==z2
	// result: (SETAE (BTQconst [63] x))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHLQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHRQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAE)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(63)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETEQ (TESTL z1:(SHLLconst [31] (SHRQconst [31] x)) z2))
	// cond: z1==z2
	// result: (SETAE (BTQconst [31] x))
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHLLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHRQconst || auxIntToInt8(z1_0.AuxInt) != 31 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAE)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(31)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETEQ (TESTQ z1:(SHRQconst [63] (SHLQconst [63] x)) z2))
	// cond: z1==z2
	// result: (SETAE (BTQconst [0] x))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHLQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAE)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(0)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETEQ (TESTL z1:(SHRLconst [31] (SHLLconst [31] x)) z2))
	// cond: z1==z2
	// result: (SETAE (BTLconst [0] x))
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHLLconst || auxIntToInt8(z1_0.AuxInt) != 31 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAE)
			v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(0)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETEQ (TESTQ z1:(SHRQconst [63] x) z2))
	// cond: z1==z2
	// result: (SETAE (BTQconst [63] x))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			x := z1.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAE)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(63)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETEQ (TESTL z1:(SHRLconst [31] x) z2))
	// cond: z1==z2
	// result: (SETAE (BTLconst [31] x))
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			x := z1.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAE)
			v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(31)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETEQ (InvertFlags x))
	// result: (SETEQ x)
	for {
		if v_0.Op != OpAMD64InvertFlags {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETEQ)
		v.AddArg(x)
		return true
	}
	// match: (SETEQ (FlagEQ))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETEQ (FlagLT_ULT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETEQ (FlagLT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETEQ (FlagGT_ULT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETEQ (FlagGT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETEQ (TESTQ s:(Select0 blsr:(BLSRQ _)) s))
	// result: (SETEQ (Select1 <types.TypeFlags> blsr))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			s := v_0_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRQ || s != v_0_1 {
				continue
			}
			v.reset(OpAMD64SETEQ)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETEQ (TESTL s:(Select0 blsr:(BLSRL _)) s))
	// result: (SETEQ (Select1 <types.TypeFlags> blsr))
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			s := v_0_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRL || s != v_0_1 {
				continue
			}
			v.reset(OpAMD64SETEQ)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg(v0)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETEQstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SETEQstore [off] {sym} ptr (TESTL (SHLL (MOVLconst [1]) x) y) mem)
	// result: (SETAEstore [off] {sym} ptr (BTL x y) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTL {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			if v_1_0.Op != OpAMD64SHLL {
				continue
			}
			x := v_1_0.Args[1]
			v_1_0_0 := v_1_0.Args[0]
			if v_1_0_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_1_0_0.AuxInt) != 1 {
				continue
			}
			y := v_1_1
			mem := v_2
			v.reset(OpAMD64SETAEstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTL, types.TypeFlags)
			v0.AddArg2(x, y)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETEQstore [off] {sym} ptr (TESTQ (SHLQ (MOVQconst [1]) x) y) mem)
	// result: (SETAEstore [off] {sym} ptr (BTQ x y) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQ {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			if v_1_0.Op != OpAMD64SHLQ {
				continue
			}
			x := v_1_0.Args[1]
			v_1_0_0 := v_1_0.Args[0]
			if v_1_0_0.Op != OpAMD64MOVQconst || auxIntToInt64(v_1_0_0.AuxInt) != 1 {
				continue
			}
			y := v_1_1
			mem := v_2
			v.reset(OpAMD64SETAEstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQ, types.TypeFlags)
			v0.AddArg2(x, y)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETEQstore [off] {sym} ptr (TESTLconst [c] x) mem)
	// cond: isUint32PowerOfTwo(int64(c))
	// result: (SETAEstore [off] {sym} ptr (BTLconst [int8(log32(c))] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		x := v_1.Args[0]
		mem := v_2
		if !(isUint32PowerOfTwo(int64(c))) {
			break
		}
		v.reset(OpAMD64SETAEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
		v0.AuxInt = int8ToAuxInt(int8(log32(c)))
		v0.AddArg(x)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETEQstore [off] {sym} ptr (TESTQconst [c] x) mem)
	// cond: isUint64PowerOfTwo(int64(c))
	// result: (SETAEstore [off] {sym} ptr (BTQconst [int8(log32(c))] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		x := v_1.Args[0]
		mem := v_2
		if !(isUint64PowerOfTwo(int64(c))) {
			break
		}
		v.reset(OpAMD64SETAEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
		v0.AuxInt = int8ToAuxInt(int8(log32(c)))
		v0.AddArg(x)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETEQstore [off] {sym} ptr (TESTQ (MOVQconst [c]) x) mem)
	// cond: isUint64PowerOfTwo(c)
	// result: (SETAEstore [off] {sym} ptr (BTQconst [int8(log64(c))] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQ {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			if v_1_0.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_1_0.AuxInt)
			x := v_1_1
			mem := v_2
			if !(isUint64PowerOfTwo(c)) {
				continue
			}
			v.reset(OpAMD64SETAEstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(int8(log64(c)))
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETEQstore [off] {sym} ptr (CMPLconst [1] s:(ANDLconst [1] _)) mem)
	// result: (SETNEstore [off] {sym} ptr (CMPLconst [0] s) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64CMPLconst || auxIntToInt32(v_1.AuxInt) != 1 {
			break
		}
		s := v_1.Args[0]
		if s.Op != OpAMD64ANDLconst || auxIntToInt32(s.AuxInt) != 1 {
			break
		}
		mem := v_2
		v.reset(OpAMD64SETNEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v0.AddArg(s)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETEQstore [off] {sym} ptr (CMPQconst [1] s:(ANDQconst [1] _)) mem)
	// result: (SETNEstore [off] {sym} ptr (CMPQconst [0] s) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64CMPQconst || auxIntToInt32(v_1.AuxInt) != 1 {
			break
		}
		s := v_1.Args[0]
		if s.Op != OpAMD64ANDQconst || auxIntToInt32(s.AuxInt) != 1 {
			break
		}
		mem := v_2
		v.reset(OpAMD64SETNEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v0.AddArg(s)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETEQstore [off] {sym} ptr (TESTQ z1:(SHLQconst [63] (SHRQconst [63] x)) z2) mem)
	// cond: z1==z2
	// result: (SETAEstore [off] {sym} ptr (BTQconst [63] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQ {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHLQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHRQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAEstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(63)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETEQstore [off] {sym} ptr (TESTL z1:(SHLLconst [31] (SHRLconst [31] x)) z2) mem)
	// cond: z1==z2
	// result: (SETAEstore [off] {sym} ptr (BTLconst [31] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTL {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHLLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHRLconst || auxIntToInt8(z1_0.AuxInt) != 31 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAEstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(31)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETEQstore [off] {sym} ptr (TESTQ z1:(SHRQconst [63] (SHLQconst [63] x)) z2) mem)
	// cond: z1==z2
	// result: (SETAEstore [off] {sym} ptr (BTQconst [0] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQ {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHLQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAEstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(0)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETEQstore [off] {sym} ptr (TESTL z1:(SHRLconst [31] (SHLLconst [31] x)) z2) mem)
	// cond: z1==z2
	// result: (SETAEstore [off] {sym} ptr (BTLconst [0] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTL {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHLLconst || auxIntToInt8(z1_0.AuxInt) != 31 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAEstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(0)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETEQstore [off] {sym} ptr (TESTQ z1:(SHRQconst [63] x) z2) mem)
	// cond: z1==z2
	// result: (SETAEstore [off] {sym} ptr (BTQconst [63] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQ {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			x := z1.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAEstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(63)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETEQstore [off] {sym} ptr (TESTL z1:(SHRLconst [31] x) z2) mem)
	// cond: z1==z2
	// result: (SETAEstore [off] {sym} ptr (BTLconst [31] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTL {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			x := z1.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETAEstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(31)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETEQstore [off] {sym} ptr (InvertFlags x) mem)
	// result: (SETEQstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64InvertFlags {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64SETEQstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (SETEQstore [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SETEQstore [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SETEQstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETEQstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SETEQstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SETEQstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETEQstore [off] {sym} ptr (FlagEQ) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETEQstore [off] {sym} ptr (FlagLT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETEQstore [off] {sym} ptr (FlagLT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETEQstore [off] {sym} ptr (FlagGT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETEQstore [off] {sym} ptr (FlagGT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETG(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SETG (InvertFlags x))
	// result: (SETL x)
	for {
		if v_0.Op != OpAMD64InvertFlags {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETL)
		v.AddArg(x)
		return true
	}
	// match: (SETG (FlagEQ))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETG (FlagLT_ULT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETG (FlagLT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETG (FlagGT_ULT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETG (FlagGT_UGT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETGE(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (SETGE c:(CMPQconst [128] x))
	// cond: c.Uses == 1
	// result: (SETG (CMPQconst [127] x))
	for {
		c := v_0
		if c.Op != OpAMD64CMPQconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		x := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETG)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETGE c:(CMPLconst [128] x))
	// cond: c.Uses == 1
	// result: (SETG (CMPLconst [127] x))
	for {
		c := v_0
		if c.Op != OpAMD64CMPLconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		x := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETG)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETGE (InvertFlags x))
	// result: (SETLE x)
	for {
		if v_0.Op != OpAMD64InvertFlags {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETLE)
		v.AddArg(x)
		return true
	}
	// match: (SETGE (FlagEQ))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETGE (FlagLT_ULT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETGE (FlagLT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETGE (FlagGT_ULT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETGE (FlagGT_UGT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETGEstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SETGEstore [off] {sym} ptr (InvertFlags x) mem)
	// result: (SETLEstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64InvertFlags {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64SETLEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (SETGEstore [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SETGEstore [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SETGEstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETGEstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SETGEstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SETGEstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETGEstore [off] {sym} ptr (FlagEQ) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETGEstore [off] {sym} ptr (FlagLT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETGEstore [off] {sym} ptr (FlagLT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETGEstore [off] {sym} ptr (FlagGT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETGEstore [off] {sym} ptr (FlagGT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETGstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SETGstore [off] {sym} ptr (InvertFlags x) mem)
	// result: (SETLstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64InvertFlags {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64SETLstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (SETGstore [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SETGstore [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SETGstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETGstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SETGstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SETGstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETGstore [off] {sym} ptr (FlagEQ) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETGstore [off] {sym} ptr (FlagLT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETGstore [off] {sym} ptr (FlagLT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETGstore [off] {sym} ptr (FlagGT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETGstore [off] {sym} ptr (FlagGT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETL(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (SETL c:(CMPQconst [128] x))
	// cond: c.Uses == 1
	// result: (SETLE (CMPQconst [127] x))
	for {
		c := v_0
		if c.Op != OpAMD64CMPQconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		x := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETLE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETL c:(CMPLconst [128] x))
	// cond: c.Uses == 1
	// result: (SETLE (CMPLconst [127] x))
	for {
		c := v_0
		if c.Op != OpAMD64CMPLconst || auxIntToInt32(c.AuxInt) != 128 {
			break
		}
		x := c.Args[0]
		if !(c.Uses == 1) {
			break
		}
		v.reset(OpAMD64SETLE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(127)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETL (InvertFlags x))
	// result: (SETG x)
	for {
		if v_0.Op != OpAMD64InvertFlags {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETG)
		v.AddArg(x)
		return true
	}
	// match: (SETL (FlagEQ))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETL (FlagLT_ULT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETL (FlagLT_UGT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETL (FlagGT_ULT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETL (FlagGT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETLE(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SETLE (InvertFlags x))
	// result: (SETGE x)
	for {
		if v_0.Op != OpAMD64InvertFlags {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETGE)
		v.AddArg(x)
		return true
	}
	// match: (SETLE (FlagEQ))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETLE (FlagLT_ULT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETLE (FlagLT_UGT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETLE (FlagGT_ULT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETLE (FlagGT_UGT))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETLEstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SETLEstore [off] {sym} ptr (InvertFlags x) mem)
	// result: (SETGEstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64InvertFlags {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64SETGEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (SETLEstore [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SETLEstore [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SETLEstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETLEstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SETLEstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SETLEstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETLEstore [off] {sym} ptr (FlagEQ) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETLEstore [off] {sym} ptr (FlagLT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETLEstore [off] {sym} ptr (FlagLT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETLEstore [off] {sym} ptr (FlagGT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETLEstore [off] {sym} ptr (FlagGT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETLstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SETLstore [off] {sym} ptr (InvertFlags x) mem)
	// result: (SETGstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64InvertFlags {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64SETGstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (SETLstore [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SETLstore [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SETLstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETLstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SETLstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SETLstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETLstore [off] {sym} ptr (FlagEQ) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETLstore [off] {sym} ptr (FlagLT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETLstore [off] {sym} ptr (FlagLT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETLstore [off] {sym} ptr (FlagGT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETLstore [off] {sym} ptr (FlagGT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETNE(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (SETNE (TESTBconst [1] x))
	// result: (ANDLconst [1] x)
	for {
		if v_0.Op != OpAMD64TESTBconst || auxIntToInt8(v_0.AuxInt) != 1 {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(1)
		v.AddArg(x)
		return true
	}
	// match: (SETNE (TESTWconst [1] x))
	// result: (ANDLconst [1] x)
	for {
		if v_0.Op != OpAMD64TESTWconst || auxIntToInt16(v_0.AuxInt) != 1 {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(1)
		v.AddArg(x)
		return true
	}
	// match: (SETNE (TESTL (SHLL (MOVLconst [1]) x) y))
	// result: (SETB (BTL x y))
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			if v_0_0.Op != OpAMD64SHLL {
				continue
			}
			x := v_0_0.Args[1]
			v_0_0_0 := v_0_0.Args[0]
			if v_0_0_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_0_0_0.AuxInt) != 1 {
				continue
			}
			y := v_0_1
			v.reset(OpAMD64SETB)
			v0 := b.NewValue0(v.Pos, OpAMD64BTL, types.TypeFlags)
			v0.AddArg2(x, y)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETNE (TESTQ (SHLQ (MOVQconst [1]) x) y))
	// result: (SETB (BTQ x y))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			if v_0_0.Op != OpAMD64SHLQ {
				continue
			}
			x := v_0_0.Args[1]
			v_0_0_0 := v_0_0.Args[0]
			if v_0_0_0.Op != OpAMD64MOVQconst || auxIntToInt64(v_0_0_0.AuxInt) != 1 {
				continue
			}
			y := v_0_1
			v.reset(OpAMD64SETB)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQ, types.TypeFlags)
			v0.AddArg2(x, y)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETNE (TESTLconst [c] x))
	// cond: isUint32PowerOfTwo(int64(c))
	// result: (SETB (BTLconst [int8(log32(c))] x))
	for {
		if v_0.Op != OpAMD64TESTLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(isUint32PowerOfTwo(int64(c))) {
			break
		}
		v.reset(OpAMD64SETB)
		v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
		v0.AuxInt = int8ToAuxInt(int8(log32(c)))
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETNE (TESTQconst [c] x))
	// cond: isUint64PowerOfTwo(int64(c))
	// result: (SETB (BTQconst [int8(log32(c))] x))
	for {
		if v_0.Op != OpAMD64TESTQconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(isUint64PowerOfTwo(int64(c))) {
			break
		}
		v.reset(OpAMD64SETB)
		v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
		v0.AuxInt = int8ToAuxInt(int8(log32(c)))
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SETNE (TESTQ (MOVQconst [c]) x))
	// cond: isUint64PowerOfTwo(c)
	// result: (SETB (BTQconst [int8(log64(c))] x))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			if v_0_0.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_0_0.AuxInt)
			x := v_0_1
			if !(isUint64PowerOfTwo(c)) {
				continue
			}
			v.reset(OpAMD64SETB)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(int8(log64(c)))
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETNE (CMPLconst [1] s:(ANDLconst [1] _)))
	// result: (SETEQ (CMPLconst [0] s))
	for {
		if v_0.Op != OpAMD64CMPLconst || auxIntToInt32(v_0.AuxInt) != 1 {
			break
		}
		s := v_0.Args[0]
		if s.Op != OpAMD64ANDLconst || auxIntToInt32(s.AuxInt) != 1 {
			break
		}
		v.reset(OpAMD64SETEQ)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v0.AddArg(s)
		v.AddArg(v0)
		return true
	}
	// match: (SETNE (CMPQconst [1] s:(ANDQconst [1] _)))
	// result: (SETEQ (CMPQconst [0] s))
	for {
		if v_0.Op != OpAMD64CMPQconst || auxIntToInt32(v_0.AuxInt) != 1 {
			break
		}
		s := v_0.Args[0]
		if s.Op != OpAMD64ANDQconst || auxIntToInt32(s.AuxInt) != 1 {
			break
		}
		v.reset(OpAMD64SETEQ)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v0.AddArg(s)
		v.AddArg(v0)
		return true
	}
	// match: (SETNE (TESTQ z1:(SHLQconst [63] (SHRQconst [63] x)) z2))
	// cond: z1==z2
	// result: (SETB (BTQconst [63] x))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHLQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHRQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETB)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(63)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETNE (TESTL z1:(SHLLconst [31] (SHRQconst [31] x)) z2))
	// cond: z1==z2
	// result: (SETB (BTQconst [31] x))
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHLLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHRQconst || auxIntToInt8(z1_0.AuxInt) != 31 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETB)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(31)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETNE (TESTQ z1:(SHRQconst [63] (SHLQconst [63] x)) z2))
	// cond: z1==z2
	// result: (SETB (BTQconst [0] x))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHLQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETB)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(0)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETNE (TESTL z1:(SHRLconst [31] (SHLLconst [31] x)) z2))
	// cond: z1==z2
	// result: (SETB (BTLconst [0] x))
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHLLconst || auxIntToInt8(z1_0.AuxInt) != 31 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETB)
			v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(0)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETNE (TESTQ z1:(SHRQconst [63] x) z2))
	// cond: z1==z2
	// result: (SETB (BTQconst [63] x))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			x := z1.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETB)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(63)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETNE (TESTL z1:(SHRLconst [31] x) z2))
	// cond: z1==z2
	// result: (SETB (BTLconst [31] x))
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			z1 := v_0_0
			if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			x := z1.Args[0]
			z2 := v_0_1
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETB)
			v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(31)
			v0.AddArg(x)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETNE (InvertFlags x))
	// result: (SETNE x)
	for {
		if v_0.Op != OpAMD64InvertFlags {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETNE)
		v.AddArg(x)
		return true
	}
	// match: (SETNE (FlagEQ))
	// result: (MOVLconst [0])
	for {
		if v_0.Op != OpAMD64FlagEQ {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SETNE (FlagLT_ULT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagLT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETNE (FlagLT_UGT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagLT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETNE (FlagGT_ULT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagGT_ULT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETNE (FlagGT_UGT))
	// result: (MOVLconst [1])
	for {
		if v_0.Op != OpAMD64FlagGT_UGT {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(1)
		return true
	}
	// match: (SETNE (TESTQ s:(Select0 blsr:(BLSRQ _)) s))
	// result: (SETNE (Select1 <types.TypeFlags> blsr))
	for {
		if v_0.Op != OpAMD64TESTQ {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			s := v_0_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRQ || s != v_0_1 {
				continue
			}
			v.reset(OpAMD64SETNE)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg(v0)
			return true
		}
		break
	}
	// match: (SETNE (TESTL s:(Select0 blsr:(BLSRL _)) s))
	// result: (SETNE (Select1 <types.TypeFlags> blsr))
	for {
		if v_0.Op != OpAMD64TESTL {
			break
		}
		_ = v_0.Args[1]
		v_0_0 := v_0.Args[0]
		v_0_1 := v_0.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
			s := v_0_0
			if s.Op != OpSelect0 {
				continue
			}
			blsr := s.Args[0]
			if blsr.Op != OpAMD64BLSRL || s != v_0_1 {
				continue
			}
			v.reset(OpAMD64SETNE)
			v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
			v0.AddArg(blsr)
			v.AddArg(v0)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64SETNEstore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SETNEstore [off] {sym} ptr (TESTL (SHLL (MOVLconst [1]) x) y) mem)
	// result: (SETBstore [off] {sym} ptr (BTL x y) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTL {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			if v_1_0.Op != OpAMD64SHLL {
				continue
			}
			x := v_1_0.Args[1]
			v_1_0_0 := v_1_0.Args[0]
			if v_1_0_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_1_0_0.AuxInt) != 1 {
				continue
			}
			y := v_1_1
			mem := v_2
			v.reset(OpAMD64SETBstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTL, types.TypeFlags)
			v0.AddArg2(x, y)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETNEstore [off] {sym} ptr (TESTQ (SHLQ (MOVQconst [1]) x) y) mem)
	// result: (SETBstore [off] {sym} ptr (BTQ x y) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQ {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			if v_1_0.Op != OpAMD64SHLQ {
				continue
			}
			x := v_1_0.Args[1]
			v_1_0_0 := v_1_0.Args[0]
			if v_1_0_0.Op != OpAMD64MOVQconst || auxIntToInt64(v_1_0_0.AuxInt) != 1 {
				continue
			}
			y := v_1_1
			mem := v_2
			v.reset(OpAMD64SETBstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQ, types.TypeFlags)
			v0.AddArg2(x, y)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETNEstore [off] {sym} ptr (TESTLconst [c] x) mem)
	// cond: isUint32PowerOfTwo(int64(c))
	// result: (SETBstore [off] {sym} ptr (BTLconst [int8(log32(c))] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		x := v_1.Args[0]
		mem := v_2
		if !(isUint32PowerOfTwo(int64(c))) {
			break
		}
		v.reset(OpAMD64SETBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
		v0.AuxInt = int8ToAuxInt(int8(log32(c)))
		v0.AddArg(x)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETNEstore [off] {sym} ptr (TESTQconst [c] x) mem)
	// cond: isUint64PowerOfTwo(int64(c))
	// result: (SETBstore [off] {sym} ptr (BTQconst [int8(log32(c))] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		x := v_1.Args[0]
		mem := v_2
		if !(isUint64PowerOfTwo(int64(c))) {
			break
		}
		v.reset(OpAMD64SETBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
		v0.AuxInt = int8ToAuxInt(int8(log32(c)))
		v0.AddArg(x)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETNEstore [off] {sym} ptr (TESTQ (MOVQconst [c]) x) mem)
	// cond: isUint64PowerOfTwo(c)
	// result: (SETBstore [off] {sym} ptr (BTQconst [int8(log64(c))] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQ {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			if v_1_0.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_1_0.AuxInt)
			x := v_1_1
			mem := v_2
			if !(isUint64PowerOfTwo(c)) {
				continue
			}
			v.reset(OpAMD64SETBstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(int8(log64(c)))
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETNEstore [off] {sym} ptr (CMPLconst [1] s:(ANDLconst [1] _)) mem)
	// result: (SETEQstore [off] {sym} ptr (CMPLconst [0] s) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64CMPLconst || auxIntToInt32(v_1.AuxInt) != 1 {
			break
		}
		s := v_1.Args[0]
		if s.Op != OpAMD64ANDLconst || auxIntToInt32(s.AuxInt) != 1 {
			break
		}
		mem := v_2
		v.reset(OpAMD64SETEQstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v0.AddArg(s)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETNEstore [off] {sym} ptr (CMPQconst [1] s:(ANDQconst [1] _)) mem)
	// result: (SETEQstore [off] {sym} ptr (CMPQconst [0] s) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64CMPQconst || auxIntToInt32(v_1.AuxInt) != 1 {
			break
		}
		s := v_1.Args[0]
		if s.Op != OpAMD64ANDQconst || auxIntToInt32(s.AuxInt) != 1 {
			break
		}
		mem := v_2
		v.reset(OpAMD64SETEQstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v0.AddArg(s)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETNEstore [off] {sym} ptr (TESTQ z1:(SHLQconst [63] (SHRQconst [63] x)) z2) mem)
	// cond: z1==z2
	// result: (SETBstore [off] {sym} ptr (BTQconst [63] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQ {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHLQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHRQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETBstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(63)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETNEstore [off] {sym} ptr (TESTL z1:(SHLLconst [31] (SHRLconst [31] x)) z2) mem)
	// cond: z1==z2
	// result: (SETBstore [off] {sym} ptr (BTLconst [31] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTL {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHLLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHRLconst || auxIntToInt8(z1_0.AuxInt) != 31 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETBstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(31)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETNEstore [off] {sym} ptr (TESTQ z1:(SHRQconst [63] (SHLQconst [63] x)) z2) mem)
	// cond: z1==z2
	// result: (SETBstore [off] {sym} ptr (BTQconst [0] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQ {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHLQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETBstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(0)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETNEstore [off] {sym} ptr (TESTL z1:(SHRLconst [31] (SHLLconst [31] x)) z2) mem)
	// cond: z1==z2
	// result: (SETBstore [off] {sym} ptr (BTLconst [0] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTL {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			z1_0 := z1.Args[0]
			if z1_0.Op != OpAMD64SHLLconst || auxIntToInt8(z1_0.AuxInt) != 31 {
				continue
			}
			x := z1_0.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETBstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(0)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETNEstore [off] {sym} ptr (TESTQ z1:(SHRQconst [63] x) z2) mem)
	// cond: z1==z2
	// result: (SETBstore [off] {sym} ptr (BTQconst [63] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTQ {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
				continue
			}
			x := z1.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETBstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(63)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETNEstore [off] {sym} ptr (TESTL z1:(SHRLconst [31] x) z2) mem)
	// cond: z1==z2
	// result: (SETBstore [off] {sym} ptr (BTLconst [31] x) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64TESTL {
			break
		}
		_ = v_1.Args[1]
		v_1_0 := v_1.Args[0]
		v_1_1 := v_1.Args[1]
		for _i0 := 0; _i0 <= 1; _i0, v_1_0, v_1_1 = _i0+1, v_1_1, v_1_0 {
			z1 := v_1_0
			if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
				continue
			}
			x := z1.Args[0]
			z2 := v_1_1
			mem := v_2
			if !(z1 == z2) {
				continue
			}
			v.reset(OpAMD64SETBstore)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v0 := b.NewValue0(v.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(31)
			v0.AddArg(x)
			v.AddArg3(ptr, v0, mem)
			return true
		}
		break
	}
	// match: (SETNEstore [off] {sym} ptr (InvertFlags x) mem)
	// result: (SETNEstore [off] {sym} ptr x mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64InvertFlags {
			break
		}
		x := v_1.Args[0]
		mem := v_2
		v.reset(OpAMD64SETNEstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	// match: (SETNEstore [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SETNEstore [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SETNEstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETNEstore [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SETNEstore [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SETNEstore)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SETNEstore [off] {sym} ptr (FlagEQ) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [0]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagEQ {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(0)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETNEstore [off] {sym} ptr (FlagLT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETNEstore [off] {sym} ptr (FlagLT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagLT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETNEstore [off] {sym} ptr (FlagGT_ULT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_ULT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	// match: (SETNEstore [off] {sym} ptr (FlagGT_UGT) mem)
	// result: (MOVBstore [off] {sym} ptr (MOVLconst <typ.UInt8> [1]) mem)
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64FlagGT_UGT {
			break
		}
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLconst, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(1)
		v.AddArg3(ptr, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHLL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (SHLL x (MOVQconst [c]))
	// result: (SHLLconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64SHLLconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	// match: (SHLL x (MOVLconst [c]))
	// result: (SHLLconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64SHLLconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	// match: (SHLL x (ADDQconst [c] y))
	// cond: c & 31 == 0
	// result: (SHLL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHLL x (NEGQ <t> (ADDQconst [c] y)))
	// cond: c & 31 == 0
	// result: (SHLL x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SHLL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHLL x (ANDQconst [c] y))
	// cond: c & 31 == 31
	// result: (SHLL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHLL x (NEGQ <t> (ANDQconst [c] y)))
	// cond: c & 31 == 31
	// result: (SHLL x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SHLL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHLL x (ADDLconst [c] y))
	// cond: c & 31 == 0
	// result: (SHLL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHLL x (NEGL <t> (ADDLconst [c] y)))
	// cond: c & 31 == 0
	// result: (SHLL x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SHLL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHLL x (ANDLconst [c] y))
	// cond: c & 31 == 31
	// result: (SHLL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHLL x (NEGL <t> (ANDLconst [c] y)))
	// cond: c & 31 == 31
	// result: (SHLL x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SHLL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHLL l:(MOVLload [off] {sym} ptr mem) x)
	// cond: buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)
	// result: (SHLXLload [off] {sym} ptr x mem)
	for {
		l := v_0
		if l.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		x := v_1
		if !(buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64SHLXLload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHLLconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SHLLconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (SHLLconst [1] x)
	// result: (ADDL x x)
	for {
		if auxIntToInt8(v.AuxInt) != 1 {
			break
		}
		x := v_0
		v.reset(OpAMD64ADDL)
		v.AddArg2(x, x)
		return true
	}
	// match: (SHLLconst [c] (ADDL x x))
	// result: (SHLLconst [c+1] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64ADDL {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpAMD64SHLLconst)
		v.AuxInt = int8ToAuxInt(c + 1)
		v.AddArg(x)
		return true
	}
	// match: (SHLLconst [d] (MOVLconst [c]))
	// result: (MOVLconst [c << uint64(d)])
	for {
		d := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(c << uint64(d))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHLQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (SHLQ x (MOVQconst [c]))
	// result: (SHLQconst [int8(c&63)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64SHLQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v.AddArg(x)
		return true
	}
	// match: (SHLQ x (MOVLconst [c]))
	// result: (SHLQconst [int8(c&63)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64SHLQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v.AddArg(x)
		return true
	}
	// match: (SHLQ x (ADDQconst [c] y))
	// cond: c & 63 == 0
	// result: (SHLQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHLQ x (NEGQ <t> (ADDQconst [c] y)))
	// cond: c & 63 == 0
	// result: (SHLQ x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHLQ x (ANDQconst [c] y))
	// cond: c & 63 == 63
	// result: (SHLQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHLQ x (NEGQ <t> (ANDQconst [c] y)))
	// cond: c & 63 == 63
	// result: (SHLQ x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHLQ x (ADDLconst [c] y))
	// cond: c & 63 == 0
	// result: (SHLQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHLQ x (NEGL <t> (ADDLconst [c] y)))
	// cond: c & 63 == 0
	// result: (SHLQ x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHLQ x (ANDLconst [c] y))
	// cond: c & 63 == 63
	// result: (SHLQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHLQ x (NEGL <t> (ANDLconst [c] y)))
	// cond: c & 63 == 63
	// result: (SHLQ x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHLQ l:(MOVQload [off] {sym} ptr mem) x)
	// cond: buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)
	// result: (SHLXQload [off] {sym} ptr x mem)
	for {
		l := v_0
		if l.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		x := v_1
		if !(buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64SHLXQload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHLQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SHLQconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (SHLQconst [1] x)
	// result: (ADDQ x x)
	for {
		if auxIntToInt8(v.AuxInt) != 1 {
			break
		}
		x := v_0
		v.reset(OpAMD64ADDQ)
		v.AddArg2(x, x)
		return true
	}
	// match: (SHLQconst [c] (ADDQ x x))
	// result: (SHLQconst [c+1] x)
	for {
		c := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64ADDQ {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpAMD64SHLQconst)
		v.AuxInt = int8ToAuxInt(c + 1)
		v.AddArg(x)
		return true
	}
	// match: (SHLQconst [d] (MOVQconst [c]))
	// result: (MOVQconst [c << uint64(d)])
	for {
		d := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(c << uint64(d))
		return true
	}
	// match: (SHLQconst [d] (MOVLconst [c]))
	// result: (MOVQconst [int64(c) << uint64(d)])
	for {
		d := auxIntToInt8(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(c) << uint64(d))
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHLXLload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SHLXLload [off] {sym} ptr (MOVLconst [c]) mem)
	// result: (SHLLconst [int8(c&31)] (MOVLload [off] {sym} ptr mem))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64SHLLconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, typ.UInt32)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHLXQload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SHLXQload [off] {sym} ptr (MOVQconst [c]) mem)
	// result: (SHLQconst [int8(c&63)] (MOVQload [off] {sym} ptr mem))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64SHLQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		v.AddArg(v0)
		return true
	}
	// match: (SHLXQload [off] {sym} ptr (MOVLconst [c]) mem)
	// result: (SHLQconst [int8(c&63)] (MOVQload [off] {sym} ptr mem))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64SHLQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHRB(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SHRB x (MOVQconst [c]))
	// cond: c&31 < 8
	// result: (SHRBconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		if !(c&31 < 8) {
			break
		}
		v.reset(OpAMD64SHRBconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	// match: (SHRB x (MOVLconst [c]))
	// cond: c&31 < 8
	// result: (SHRBconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		if !(c&31 < 8) {
			break
		}
		v.reset(OpAMD64SHRBconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	// match: (SHRB _ (MOVQconst [c]))
	// cond: c&31 >= 8
	// result: (MOVLconst [0])
	for {
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		if !(c&31 >= 8) {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SHRB _ (MOVLconst [c]))
	// cond: c&31 >= 8
	// result: (MOVLconst [0])
	for {
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		if !(c&31 >= 8) {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHRBconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SHRBconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHRL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (SHRL x (MOVQconst [c]))
	// result: (SHRLconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64SHRLconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	// match: (SHRL x (MOVLconst [c]))
	// result: (SHRLconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64SHRLconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	// match: (SHRL x (ADDQconst [c] y))
	// cond: c & 31 == 0
	// result: (SHRL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SHRL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHRL x (NEGQ <t> (ADDQconst [c] y)))
	// cond: c & 31 == 0
	// result: (SHRL x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SHRL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHRL x (ANDQconst [c] y))
	// cond: c & 31 == 31
	// result: (SHRL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SHRL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHRL x (NEGQ <t> (ANDQconst [c] y)))
	// cond: c & 31 == 31
	// result: (SHRL x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SHRL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHRL x (ADDLconst [c] y))
	// cond: c & 31 == 0
	// result: (SHRL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SHRL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHRL x (NEGL <t> (ADDLconst [c] y)))
	// cond: c & 31 == 0
	// result: (SHRL x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 0) {
			break
		}
		v.reset(OpAMD64SHRL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHRL x (ANDLconst [c] y))
	// cond: c & 31 == 31
	// result: (SHRL x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SHRL)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHRL x (NEGL <t> (ANDLconst [c] y)))
	// cond: c & 31 == 31
	// result: (SHRL x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&31 == 31) {
			break
		}
		v.reset(OpAMD64SHRL)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHRL l:(MOVLload [off] {sym} ptr mem) x)
	// cond: buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)
	// result: (SHRXLload [off] {sym} ptr x mem)
	for {
		l := v_0
		if l.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		x := v_1
		if !(buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64SHRXLload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHRLconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SHRLconst [1] (ADDL x x))
	// result: (ANDLconst [0x7fffffff] x)
	for {
		if auxIntToInt8(v.AuxInt) != 1 || v_0.Op != OpAMD64ADDL {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpAMD64ANDLconst)
		v.AuxInt = int32ToAuxInt(0x7fffffff)
		v.AddArg(x)
		return true
	}
	// match: (SHRLconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHRQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (SHRQ x (MOVQconst [c]))
	// result: (SHRQconst [int8(c&63)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		v.reset(OpAMD64SHRQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v.AddArg(x)
		return true
	}
	// match: (SHRQ x (MOVLconst [c]))
	// result: (SHRQconst [int8(c&63)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64SHRQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v.AddArg(x)
		return true
	}
	// match: (SHRQ x (ADDQconst [c] y))
	// cond: c & 63 == 0
	// result: (SHRQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHRQ x (NEGQ <t> (ADDQconst [c] y)))
	// cond: c & 63 == 0
	// result: (SHRQ x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHRQ x (ANDQconst [c] y))
	// cond: c & 63 == 63
	// result: (SHRQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHRQ x (NEGQ <t> (ANDQconst [c] y)))
	// cond: c & 63 == 63
	// result: (SHRQ x (NEGQ <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGQ {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDQconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHRQ x (ADDLconst [c] y))
	// cond: c & 63 == 0
	// result: (SHRQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHRQ x (NEGL <t> (ADDLconst [c] y)))
	// cond: c & 63 == 0
	// result: (SHRQ x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ADDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 0) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHRQ x (ANDLconst [c] y))
	// cond: c & 63 == 63
	// result: (SHRQ x y)
	for {
		x := v_0
		if v_1.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		y := v_1.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v.AddArg2(x, y)
		return true
	}
	// match: (SHRQ x (NEGL <t> (ANDLconst [c] y)))
	// cond: c & 63 == 63
	// result: (SHRQ x (NEGL <t> y))
	for {
		x := v_0
		if v_1.Op != OpAMD64NEGL {
			break
		}
		t := v_1.Type
		v_1_0 := v_1.Args[0]
		if v_1_0.Op != OpAMD64ANDLconst {
			break
		}
		c := auxIntToInt32(v_1_0.AuxInt)
		y := v_1_0.Args[0]
		if !(c&63 == 63) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGL, t)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	// match: (SHRQ l:(MOVQload [off] {sym} ptr mem) x)
	// cond: buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)
	// result: (SHRXQload [off] {sym} ptr x mem)
	for {
		l := v_0
		if l.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		x := v_1
		if !(buildcfg.GOAMD64 >= 3 && canMergeLoad(v, l) && clobber(l)) {
			break
		}
		v.reset(OpAMD64SHRXQload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(ptr, x, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHRQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SHRQconst [1] (ADDQ x x))
	// result: (BTRQconst [63] x)
	for {
		if auxIntToInt8(v.AuxInt) != 1 || v_0.Op != OpAMD64ADDQ {
			break
		}
		x := v_0.Args[1]
		if x != v_0.Args[0] {
			break
		}
		v.reset(OpAMD64BTRQconst)
		v.AuxInt = int8ToAuxInt(63)
		v.AddArg(x)
		return true
	}
	// match: (SHRQconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHRW(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SHRW x (MOVQconst [c]))
	// cond: c&31 < 16
	// result: (SHRWconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		if !(c&31 < 16) {
			break
		}
		v.reset(OpAMD64SHRWconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	// match: (SHRW x (MOVLconst [c]))
	// cond: c&31 < 16
	// result: (SHRWconst [int8(c&31)] x)
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		if !(c&31 < 16) {
			break
		}
		v.reset(OpAMD64SHRWconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v.AddArg(x)
		return true
	}
	// match: (SHRW _ (MOVQconst [c]))
	// cond: c&31 >= 16
	// result: (MOVLconst [0])
	for {
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		if !(c&31 >= 16) {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SHRW _ (MOVLconst [c]))
	// cond: c&31 >= 16
	// result: (MOVLconst [0])
	for {
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		if !(c&31 >= 16) {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHRWconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SHRWconst x [0])
	// result: x
	for {
		if auxIntToInt8(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHRXLload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SHRXLload [off] {sym} ptr (MOVLconst [c]) mem)
	// result: (SHRLconst [int8(c&31)] (MOVLload [off] {sym} ptr mem))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64SHRLconst)
		v.AuxInt = int8ToAuxInt(int8(c & 31))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, typ.UInt32)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SHRXQload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SHRXQload [off] {sym} ptr (MOVQconst [c]) mem)
	// result: (SHRQconst [int8(c&63)] (MOVQload [off] {sym} ptr mem))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64SHRQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		v.AddArg(v0)
		return true
	}
	// match: (SHRXQload [off] {sym} ptr (MOVLconst [c]) mem)
	// result: (SHRQconst [int8(c&63)] (MOVQload [off] {sym} ptr mem))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		ptr := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		mem := v_2
		v.reset(OpAMD64SHRQconst)
		v.AuxInt = int8ToAuxInt(int8(c & 63))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v0.AuxInt = int32ToAuxInt(off)
		v0.Aux = symToAux(sym)
		v0.AddArg2(ptr, mem)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (SUBL x (MOVLconst [c]))
	// result: (SUBLconst x [c])
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_1.AuxInt)
		v.reset(OpAMD64SUBLconst)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	// match: (SUBL (MOVLconst [c]) x)
	// result: (NEGL (SUBLconst <v.Type> x [c]))
	for {
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		c := auxIntToInt32(v_0.AuxInt)
		x := v_1
		v.reset(OpAMD64NEGL)
		v0 := b.NewValue0(v.Pos, OpAMD64SUBLconst, v.Type)
		v0.AuxInt = int32ToAuxInt(c)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SUBL x x)
	// result: (MOVLconst [0])
	for {
		x := v_0
		if x != v_1 {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (SUBL x l:(MOVLload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (SUBLload x [off] {sym} ptr mem)
	for {
		x := v_0
		l := v_1
		if l.Op != OpAMD64MOVLload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
			break
		}
		v.reset(OpAMD64SUBLload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(x, ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBLconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SUBLconst [c] x)
	// cond: c==0
	// result: x
	for {
		c := auxIntToInt32(v.AuxInt)
		x := v_0
		if !(c == 0) {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (SUBLconst [c] x)
	// result: (ADDLconst [-c] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		x := v_0
		v.reset(OpAMD64ADDLconst)
		v.AuxInt = int32ToAuxInt(-c)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpAMD64SUBLload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SUBLload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SUBLload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SUBLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (SUBLload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SUBLload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SUBLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (SUBLload x [off] {sym} ptr (MOVSSstore [off] {sym} ptr y _))
	// result: (SUBL x (MOVLf2i y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVSSstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64SUBL)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVLf2i, typ.UInt32)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBLmodify(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SUBLmodify [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SUBLmodify [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SUBLmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SUBLmodify [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SUBLmodify [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SUBLmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (SUBQ x (MOVQconst [c]))
	// cond: is32Bit(c)
	// result: (SUBQconst x [int32(c)])
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		if !(is32Bit(c)) {
			break
		}
		v.reset(OpAMD64SUBQconst)
		v.AuxInt = int32ToAuxInt(int32(c))
		v.AddArg(x)
		return true
	}
	// match: (SUBQ (MOVQconst [c]) x)
	// cond: is32Bit(c)
	// result: (NEGQ (SUBQconst <v.Type> x [int32(c)]))
	for {
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_0.AuxInt)
		x := v_1
		if !(is32Bit(c)) {
			break
		}
		v.reset(OpAMD64NEGQ)
		v0 := b.NewValue0(v.Pos, OpAMD64SUBQconst, v.Type)
		v0.AuxInt = int32ToAuxInt(int32(c))
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	// match: (SUBQ x x)
	// result: (MOVQconst [0])
	for {
		x := v_0
		if x != v_1 {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(0)
		return true
	}
	// match: (SUBQ x l:(MOVQload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (SUBQload x [off] {sym} ptr mem)
	for {
		x := v_0
		l := v_1
		if l.Op != OpAMD64MOVQload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
			break
		}
		v.reset(OpAMD64SUBQload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(x, ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBQborrow(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SUBQborrow x (MOVQconst [c]))
	// cond: is32Bit(c)
	// result: (SUBQconstborrow x [int32(c)])
	for {
		x := v_0
		if v_1.Op != OpAMD64MOVQconst {
			break
		}
		c := auxIntToInt64(v_1.AuxInt)
		if !(is32Bit(c)) {
			break
		}
		v.reset(OpAMD64SUBQconstborrow)
		v.AuxInt = int32ToAuxInt(int32(c))
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (SUBQconst [0] x)
	// result: x
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (SUBQconst [c] x)
	// cond: c != -(1<<31)
	// result: (ADDQconst [-c] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		x := v_0
		if !(c != -(1 << 31)) {
			break
		}
		v.reset(OpAMD64ADDQconst)
		v.AuxInt = int32ToAuxInt(-c)
		v.AddArg(x)
		return true
	}
	// match: (SUBQconst (MOVQconst [d]) [c])
	// result: (MOVQconst [d-int64(c)])
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(d - int64(c))
		return true
	}
	// match: (SUBQconst (SUBQconst x [d]) [c])
	// cond: is32Bit(int64(-c)-int64(d))
	// result: (ADDQconst [-c-d] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64SUBQconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		if !(is32Bit(int64(-c) - int64(d))) {
			break
		}
		v.reset(OpAMD64ADDQconst)
		v.AuxInt = int32ToAuxInt(-c - d)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBQload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SUBQload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SUBQload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SUBQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (SUBQload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SUBQload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SUBQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (SUBQload x [off] {sym} ptr (MOVSDstore [off] {sym} ptr y _))
	// result: (SUBQ x (MOVQf2i y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVSDstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64SUBQ)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVQf2i, typ.UInt64)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBQmodify(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SUBQmodify [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SUBQmodify [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SUBQmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (SUBQmodify [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SUBQmodify [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SUBQmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBSD(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SUBSD x l:(MOVSDload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (SUBSDload x [off] {sym} ptr mem)
	for {
		x := v_0
		l := v_1
		if l.Op != OpAMD64MOVSDload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
			break
		}
		v.reset(OpAMD64SUBSDload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(x, ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBSDload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SUBSDload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SUBSDload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SUBSDload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (SUBSDload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SUBSDload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SUBSDload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (SUBSDload x [off] {sym} ptr (MOVQstore [off] {sym} ptr y _))
	// result: (SUBSD x (MOVQi2f y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVQstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64SUBSD)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVQi2f, typ.Float64)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBSS(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SUBSS x l:(MOVSSload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (SUBSSload x [off] {sym} ptr mem)
	for {
		x := v_0
		l := v_1
		if l.Op != OpAMD64MOVSSload {
			break
		}
		off := auxIntToInt32(l.AuxInt)
		sym := auxToSym(l.Aux)
		mem := l.Args[1]
		ptr := l.Args[0]
		if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
			break
		}
		v.reset(OpAMD64SUBSSload)
		v.AuxInt = int32ToAuxInt(off)
		v.Aux = symToAux(sym)
		v.AddArg3(x, ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64SUBSSload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SUBSSload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (SUBSSload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64SUBSSload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (SUBSSload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (SUBSSload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64SUBSSload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (SUBSSload x [off] {sym} ptr (MOVLstore [off] {sym} ptr y _))
	// result: (SUBSS x (MOVLi2f y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVLstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64SUBSS)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVLi2f, typ.Float32)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64TESTB(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (TESTB (MOVLconst [c]) x)
	// result: (TESTBconst [int8(c)] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64MOVLconst {
				continue
			}
			c := auxIntToInt32(v_0.AuxInt)
			x := v_1
			v.reset(OpAMD64TESTBconst)
			v.AuxInt = int8ToAuxInt(int8(c))
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (TESTB l:(MOVBload {sym} [off] ptr mem) l2)
	// cond: l == l2 && l.Uses == 2 && clobber(l)
	// result: @l.Block (CMPBconstload {sym} [makeValAndOff(0, off)] ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			l := v_0
			if l.Op != OpAMD64MOVBload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			l2 := v_1
			if !(l == l2 && l.Uses == 2 && clobber(l)) {
				continue
			}
			b = l.Block
			v0 := b.NewValue0(l.Pos, OpAMD64CMPBconstload, types.TypeFlags)
			v.copyOf(v0)
			v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, off))
			v0.Aux = symToAux(sym)
			v0.AddArg2(ptr, mem)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64TESTBconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TESTBconst [-1] x)
	// cond: x.Op != OpAMD64MOVLconst
	// result: (TESTB x x)
	for {
		if auxIntToInt8(v.AuxInt) != -1 {
			break
		}
		x := v_0
		if !(x.Op != OpAMD64MOVLconst) {
			break
		}
		v.reset(OpAMD64TESTB)
		v.AddArg2(x, x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64TESTL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (TESTL (MOVLconst [c]) x)
	// result: (TESTLconst [c] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64MOVLconst {
				continue
			}
			c := auxIntToInt32(v_0.AuxInt)
			x := v_1
			v.reset(OpAMD64TESTLconst)
			v.AuxInt = int32ToAuxInt(c)
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (TESTL l:(MOVLload {sym} [off] ptr mem) l2)
	// cond: l == l2 && l.Uses == 2 && clobber(l)
	// result: @l.Block (CMPLconstload {sym} [makeValAndOff(0, off)] ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			l := v_0
			if l.Op != OpAMD64MOVLload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			l2 := v_1
			if !(l == l2 && l.Uses == 2 && clobber(l)) {
				continue
			}
			b = l.Block
			v0 := b.NewValue0(l.Pos, OpAMD64CMPLconstload, types.TypeFlags)
			v.copyOf(v0)
			v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, off))
			v0.Aux = symToAux(sym)
			v0.AddArg2(ptr, mem)
			return true
		}
		break
	}
	// match: (TESTL a:(ANDLload [off] {sym} x ptr mem) a)
	// cond: a.Uses == 2 && a.Block == v.Block && clobber(a)
	// result: (TESTL (MOVLload <a.Type> [off] {sym} ptr mem) x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			a := v_0
			if a.Op != OpAMD64ANDLload {
				continue
			}
			off := auxIntToInt32(a.AuxInt)
			sym := auxToSym(a.Aux)
			mem := a.Args[2]
			x := a.Args[0]
			ptr := a.Args[1]
			if a != v_1 || !(a.Uses == 2 && a.Block == v.Block && clobber(a)) {
				continue
			}
			v.reset(OpAMD64TESTL)
			v0 := b.NewValue0(a.Pos, OpAMD64MOVLload, a.Type)
			v0.AuxInt = int32ToAuxInt(off)
			v0.Aux = symToAux(sym)
			v0.AddArg2(ptr, mem)
			v.AddArg2(v0, x)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64TESTLconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TESTLconst [c] (MOVLconst [c]))
	// cond: c == 0
	// result: (FlagEQ)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_0.AuxInt) != c || !(c == 0) {
			break
		}
		v.reset(OpAMD64FlagEQ)
		return true
	}
	// match: (TESTLconst [c] (MOVLconst [c]))
	// cond: c < 0
	// result: (FlagLT_UGT)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_0.AuxInt) != c || !(c < 0) {
			break
		}
		v.reset(OpAMD64FlagLT_UGT)
		return true
	}
	// match: (TESTLconst [c] (MOVLconst [c]))
	// cond: c > 0
	// result: (FlagGT_UGT)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_0.AuxInt) != c || !(c > 0) {
			break
		}
		v.reset(OpAMD64FlagGT_UGT)
		return true
	}
	// match: (TESTLconst [-1] x)
	// cond: x.Op != OpAMD64MOVLconst
	// result: (TESTL x x)
	for {
		if auxIntToInt32(v.AuxInt) != -1 {
			break
		}
		x := v_0
		if !(x.Op != OpAMD64MOVLconst) {
			break
		}
		v.reset(OpAMD64TESTL)
		v.AddArg2(x, x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64TESTQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (TESTQ (MOVQconst [c]) x)
	// cond: is32Bit(c)
	// result: (TESTQconst [int32(c)] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_0.AuxInt)
			x := v_1
			if !(is32Bit(c)) {
				continue
			}
			v.reset(OpAMD64TESTQconst)
			v.AuxInt = int32ToAuxInt(int32(c))
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (TESTQ l:(MOVQload {sym} [off] ptr mem) l2)
	// cond: l == l2 && l.Uses == 2 && clobber(l)
	// result: @l.Block (CMPQconstload {sym} [makeValAndOff(0, off)] ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			l := v_0
			if l.Op != OpAMD64MOVQload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			l2 := v_1
			if !(l == l2 && l.Uses == 2 && clobber(l)) {
				continue
			}
			b = l.Block
			v0 := b.NewValue0(l.Pos, OpAMD64CMPQconstload, types.TypeFlags)
			v.copyOf(v0)
			v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, off))
			v0.Aux = symToAux(sym)
			v0.AddArg2(ptr, mem)
			return true
		}
		break
	}
	// match: (TESTQ a:(ANDQload [off] {sym} x ptr mem) a)
	// cond: a.Uses == 2 && a.Block == v.Block && clobber(a)
	// result: (TESTQ (MOVQload <a.Type> [off] {sym} ptr mem) x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			a := v_0
			if a.Op != OpAMD64ANDQload {
				continue
			}
			off := auxIntToInt32(a.AuxInt)
			sym := auxToSym(a.Aux)
			mem := a.Args[2]
			x := a.Args[0]
			ptr := a.Args[1]
			if a != v_1 || !(a.Uses == 2 && a.Block == v.Block && clobber(a)) {
				continue
			}
			v.reset(OpAMD64TESTQ)
			v0 := b.NewValue0(a.Pos, OpAMD64MOVQload, a.Type)
			v0.AuxInt = int32ToAuxInt(off)
			v0.Aux = symToAux(sym)
			v0.AddArg2(ptr, mem)
			v.AddArg2(v0, x)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64TESTQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TESTQconst [c] (MOVQconst [d]))
	// cond: int64(c) == d && c == 0
	// result: (FlagEQ)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		if !(int64(c) == d && c == 0) {
			break
		}
		v.reset(OpAMD64FlagEQ)
		return true
	}
	// match: (TESTQconst [c] (MOVQconst [d]))
	// cond: int64(c) == d && c < 0
	// result: (FlagLT_UGT)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		if !(int64(c) == d && c < 0) {
			break
		}
		v.reset(OpAMD64FlagLT_UGT)
		return true
	}
	// match: (TESTQconst [c] (MOVQconst [d]))
	// cond: int64(c) == d && c > 0
	// result: (FlagGT_UGT)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		if !(int64(c) == d && c > 0) {
			break
		}
		v.reset(OpAMD64FlagGT_UGT)
		return true
	}
	// match: (TESTQconst [-1] x)
	// cond: x.Op != OpAMD64MOVQconst
	// result: (TESTQ x x)
	for {
		if auxIntToInt32(v.AuxInt) != -1 {
			break
		}
		x := v_0
		if !(x.Op != OpAMD64MOVQconst) {
			break
		}
		v.reset(OpAMD64TESTQ)
		v.AddArg2(x, x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64TESTW(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (TESTW (MOVLconst [c]) x)
	// result: (TESTWconst [int16(c)] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64MOVLconst {
				continue
			}
			c := auxIntToInt32(v_0.AuxInt)
			x := v_1
			v.reset(OpAMD64TESTWconst)
			v.AuxInt = int16ToAuxInt(int16(c))
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (TESTW l:(MOVWload {sym} [off] ptr mem) l2)
	// cond: l == l2 && l.Uses == 2 && clobber(l)
	// result: @l.Block (CMPWconstload {sym} [makeValAndOff(0, off)] ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			l := v_0
			if l.Op != OpAMD64MOVWload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			l2 := v_1
			if !(l == l2 && l.Uses == 2 && clobber(l)) {
				continue
			}
			b = l.Block
			v0 := b.NewValue0(l.Pos, OpAMD64CMPWconstload, types.TypeFlags)
			v.copyOf(v0)
			v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, off))
			v0.Aux = symToAux(sym)
			v0.AddArg2(ptr, mem)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64TESTWconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TESTWconst [-1] x)
	// cond: x.Op != OpAMD64MOVLconst
	// result: (TESTW x x)
	for {
		if auxIntToInt16(v.AuxInt) != -1 {
			break
		}
		x := v_0
		if !(x.Op != OpAMD64MOVLconst) {
			break
		}
		v.reset(OpAMD64TESTW)
		v.AddArg2(x, x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec16x16ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec16x16ToM (VPMOVMToVec16x16 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec16x16 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec16x32ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec16x32ToM (VPMOVMToVec16x32 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec16x32 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec16x8ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec16x8ToM (VPMOVMToVec16x8 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec16x8 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec32x16ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec32x16ToM (VPMOVMToVec32x16 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec32x16 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec32x4ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec32x4ToM (VPMOVMToVec32x4 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec32x4 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec32x8ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec32x8ToM (VPMOVMToVec32x8 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec32x8 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec64x2ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec64x2ToM (VPMOVMToVec64x2 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec64x2 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec64x4ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec64x4ToM (VPMOVMToVec64x4 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec64x4 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec64x8ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec64x8ToM (VPMOVMToVec64x8 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec64x8 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec8x16ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec8x16ToM (VPMOVMToVec8x16 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec8x16 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec8x32ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec8x32ToM (VPMOVMToVec8x32 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec8x32 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64VPMOVVec8x64ToM(v *Value) bool {
	v_0 := v.Args[0]
	// match: (VPMOVVec8x64ToM (VPMOVMToVec8x64 x))
	// result: x
	for {
		if v_0.Op != OpAMD64VPMOVMToVec8x64 {
			break
		}
		x := v_0.Args[0]
		v.copyOf(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XADDLlock(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (XADDLlock [off1] {sym} val (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (XADDLlock [off1+off2] {sym} val ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		ptr := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64XADDLlock)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XADDQlock(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (XADDQlock [off1] {sym} val (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (XADDQlock [off1+off2] {sym} val ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		ptr := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64XADDQlock)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XCHGL(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (XCHGL [off1] {sym} val (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (XCHGL [off1+off2] {sym} val ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		ptr := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64XCHGL)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, ptr, mem)
		return true
	}
	// match: (XCHGL [off1] {sym1} val (LEAQ [off2] {sym2} ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && ptr.Op != OpSB
	// result: (XCHGL [off1+off2] {mergeSym(sym1,sym2)} val ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		ptr := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && ptr.Op != OpSB) {
			break
		}
		v.reset(OpAMD64XCHGL)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XCHGQ(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (XCHGQ [off1] {sym} val (ADDQconst [off2] ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (XCHGQ [off1+off2] {sym} val ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		ptr := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64XCHGQ)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, ptr, mem)
		return true
	}
	// match: (XCHGQ [off1] {sym1} val (LEAQ [off2] {sym2} ptr) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && ptr.Op != OpSB
	// result: (XCHGQ [off1+off2] {mergeSym(sym1,sym2)} val ptr mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		ptr := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2) && ptr.Op != OpSB) {
			break
		}
		v.reset(OpAMD64XCHGQ)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XORL(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (XORL (SHLL (MOVLconst [1]) y) x)
	// result: (BTCL x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64SHLL {
				continue
			}
			y := v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_0_0.AuxInt) != 1 {
				continue
			}
			x := v_1
			v.reset(OpAMD64BTCL)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (XORL x (MOVLconst [c]))
	// result: (XORLconst [c] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVLconst {
				continue
			}
			c := auxIntToInt32(v_1.AuxInt)
			v.reset(OpAMD64XORLconst)
			v.AuxInt = int32ToAuxInt(c)
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (XORL x x)
	// result: (MOVLconst [0])
	for {
		x := v_0
		if x != v_1 {
			break
		}
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(0)
		return true
	}
	// match: (XORL x l:(MOVLload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (XORLload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVLload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64XORLload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	// match: (XORL x (ADDLconst [-1] x))
	// cond: buildcfg.GOAMD64 >= 3
	// result: (BLSMSKL x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64ADDLconst || auxIntToInt32(v_1.AuxInt) != -1 || x != v_1.Args[0] || !(buildcfg.GOAMD64 >= 3) {
				continue
			}
			v.reset(OpAMD64BLSMSKL)
			v.AddArg(x)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64XORLconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (XORLconst [1] (SETNE x))
	// result: (SETEQ x)
	for {
		if auxIntToInt32(v.AuxInt) != 1 || v_0.Op != OpAMD64SETNE {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETEQ)
		v.AddArg(x)
		return true
	}
	// match: (XORLconst [1] (SETEQ x))
	// result: (SETNE x)
	for {
		if auxIntToInt32(v.AuxInt) != 1 || v_0.Op != OpAMD64SETEQ {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETNE)
		v.AddArg(x)
		return true
	}
	// match: (XORLconst [1] (SETL x))
	// result: (SETGE x)
	for {
		if auxIntToInt32(v.AuxInt) != 1 || v_0.Op != OpAMD64SETL {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETGE)
		v.AddArg(x)
		return true
	}
	// match: (XORLconst [1] (SETGE x))
	// result: (SETL x)
	for {
		if auxIntToInt32(v.AuxInt) != 1 || v_0.Op != OpAMD64SETGE {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETL)
		v.AddArg(x)
		return true
	}
	// match: (XORLconst [1] (SETLE x))
	// result: (SETG x)
	for {
		if auxIntToInt32(v.AuxInt) != 1 || v_0.Op != OpAMD64SETLE {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETG)
		v.AddArg(x)
		return true
	}
	// match: (XORLconst [1] (SETG x))
	// result: (SETLE x)
	for {
		if auxIntToInt32(v.AuxInt) != 1 || v_0.Op != OpAMD64SETG {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETLE)
		v.AddArg(x)
		return true
	}
	// match: (XORLconst [1] (SETB x))
	// result: (SETAE x)
	for {
		if auxIntToInt32(v.AuxInt) != 1 || v_0.Op != OpAMD64SETB {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETAE)
		v.AddArg(x)
		return true
	}
	// match: (XORLconst [1] (SETAE x))
	// result: (SETB x)
	for {
		if auxIntToInt32(v.AuxInt) != 1 || v_0.Op != OpAMD64SETAE {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETB)
		v.AddArg(x)
		return true
	}
	// match: (XORLconst [1] (SETBE x))
	// result: (SETA x)
	for {
		if auxIntToInt32(v.AuxInt) != 1 || v_0.Op != OpAMD64SETBE {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETA)
		v.AddArg(x)
		return true
	}
	// match: (XORLconst [1] (SETA x))
	// result: (SETBE x)
	for {
		if auxIntToInt32(v.AuxInt) != 1 || v_0.Op != OpAMD64SETA {
			break
		}
		x := v_0.Args[0]
		v.reset(OpAMD64SETBE)
		v.AddArg(x)
		return true
	}
	// match: (XORLconst [c] (XORLconst [d] x))
	// result: (XORLconst [c ^ d] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64XORLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		v.reset(OpAMD64XORLconst)
		v.AuxInt = int32ToAuxInt(c ^ d)
		v.AddArg(x)
		return true
	}
	// match: (XORLconst [c] x)
	// cond: c==0
	// result: x
	for {
		c := auxIntToInt32(v.AuxInt)
		x := v_0
		if !(c == 0) {
			break
		}
		v.copyOf(x)
		return true
	}
	// match: (XORLconst [c] (MOVLconst [d]))
	// result: (MOVLconst [c^d])
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVLconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(c ^ d)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XORLconstmodify(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (XORLconstmodify [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (XORLconstmodify [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64XORLconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (XORLconstmodify [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (XORLconstmodify [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64XORLconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XORLload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (XORLload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (XORLload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64XORLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (XORLload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (XORLload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64XORLload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (XORLload x [off] {sym} ptr (MOVSSstore [off] {sym} ptr y _))
	// result: (XORL x (MOVLf2i y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVSSstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64XORL)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVLf2i, typ.UInt32)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XORLmodify(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (XORLmodify [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (XORLmodify [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64XORLmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (XORLmodify [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (XORLmodify [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64XORLmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XORQ(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (XORQ (SHLQ (MOVQconst [1]) y) x)
	// result: (BTCQ x y)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64SHLQ {
				continue
			}
			y := v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64MOVQconst || auxIntToInt64(v_0_0.AuxInt) != 1 {
				continue
			}
			x := v_1
			v.reset(OpAMD64BTCQ)
			v.AddArg2(x, y)
			return true
		}
		break
	}
	// match: (XORQ (MOVQconst [c]) x)
	// cond: isUint64PowerOfTwo(c) && uint64(c) >= 1<<31
	// result: (BTCQconst [int8(log64(c))] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			if v_0.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_0.AuxInt)
			x := v_1
			if !(isUint64PowerOfTwo(c) && uint64(c) >= 1<<31) {
				continue
			}
			v.reset(OpAMD64BTCQconst)
			v.AuxInt = int8ToAuxInt(int8(log64(c)))
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (XORQ x (MOVQconst [c]))
	// cond: is32Bit(c)
	// result: (XORQconst [int32(c)] x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64MOVQconst {
				continue
			}
			c := auxIntToInt64(v_1.AuxInt)
			if !(is32Bit(c)) {
				continue
			}
			v.reset(OpAMD64XORQconst)
			v.AuxInt = int32ToAuxInt(int32(c))
			v.AddArg(x)
			return true
		}
		break
	}
	// match: (XORQ x x)
	// result: (MOVQconst [0])
	for {
		x := v_0
		if x != v_1 {
			break
		}
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(0)
		return true
	}
	// match: (XORQ x l:(MOVQload [off] {sym} ptr mem))
	// cond: canMergeLoadClobber(v, l, x) && clobber(l)
	// result: (XORQload x [off] {sym} ptr mem)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			l := v_1
			if l.Op != OpAMD64MOVQload {
				continue
			}
			off := auxIntToInt32(l.AuxInt)
			sym := auxToSym(l.Aux)
			mem := l.Args[1]
			ptr := l.Args[0]
			if !(canMergeLoadClobber(v, l, x) && clobber(l)) {
				continue
			}
			v.reset(OpAMD64XORQload)
			v.AuxInt = int32ToAuxInt(off)
			v.Aux = symToAux(sym)
			v.AddArg3(x, ptr, mem)
			return true
		}
		break
	}
	// match: (XORQ x (ADDQconst [-1] x))
	// cond: buildcfg.GOAMD64 >= 3
	// result: (BLSMSKQ x)
	for {
		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
			x := v_0
			if v_1.Op != OpAMD64ADDQconst || auxIntToInt32(v_1.AuxInt) != -1 || x != v_1.Args[0] || !(buildcfg.GOAMD64 >= 3) {
				continue
			}
			v.reset(OpAMD64BLSMSKQ)
			v.AddArg(x)
			return true
		}
		break
	}
	return false
}
func rewriteValueAMD64_OpAMD64XORQconst(v *Value) bool {
	v_0 := v.Args[0]
	// match: (XORQconst [c] (XORQconst [d] x))
	// result: (XORQconst [c ^ d] x)
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64XORQconst {
			break
		}
		d := auxIntToInt32(v_0.AuxInt)
		x := v_0.Args[0]
		v.reset(OpAMD64XORQconst)
		v.AuxInt = int32ToAuxInt(c ^ d)
		v.AddArg(x)
		return true
	}
	// match: (XORQconst [0] x)
	// result: x
	for {
		if auxIntToInt32(v.AuxInt) != 0 {
			break
		}
		x := v_0
		v.copyOf(x)
		return true
	}
	// match: (XORQconst [c] (MOVQconst [d]))
	// result: (MOVQconst [int64(c)^d])
	for {
		c := auxIntToInt32(v.AuxInt)
		if v_0.Op != OpAMD64MOVQconst {
			break
		}
		d := auxIntToInt64(v_0.AuxInt)
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(int64(c) ^ d)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XORQconstmodify(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (XORQconstmodify [valoff1] {sym} (ADDQconst [off2] base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2)
	// result: (XORQconstmodify [ValAndOff(valoff1).addOffset32(off2)] {sym} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2)) {
			break
		}
		v.reset(OpAMD64XORQconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(sym)
		v.AddArg2(base, mem)
		return true
	}
	// match: (XORQconstmodify [valoff1] {sym1} (LEAQ [off2] {sym2} base) mem)
	// cond: ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)
	// result: (XORQconstmodify [ValAndOff(valoff1).addOffset32(off2)] {mergeSym(sym1,sym2)} base mem)
	for {
		valoff1 := auxIntToValAndOff(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		mem := v_1
		if !(ValAndOff(valoff1).canAdd32(off2) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64XORQconstmodify)
		v.AuxInt = valAndOffToAuxInt(ValAndOff(valoff1).addOffset32(off2))
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg2(base, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XORQload(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (XORQload [off1] {sym} val (ADDQconst [off2] base) mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (XORQload [off1+off2] {sym} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64XORQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (XORQload [off1] {sym1} val (LEAQ [off2] {sym2} base) mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (XORQload [off1+off2] {mergeSym(sym1,sym2)} val base mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		val := v_0
		if v_1.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_1.AuxInt)
		sym2 := auxToSym(v_1.Aux)
		base := v_1.Args[0]
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64XORQload)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(val, base, mem)
		return true
	}
	// match: (XORQload x [off] {sym} ptr (MOVSDstore [off] {sym} ptr y _))
	// result: (XORQ x (MOVQf2i y))
	for {
		off := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		x := v_0
		ptr := v_1
		if v_2.Op != OpAMD64MOVSDstore || auxIntToInt32(v_2.AuxInt) != off || auxToSym(v_2.Aux) != sym {
			break
		}
		y := v_2.Args[1]
		if ptr != v_2.Args[0] {
			break
		}
		v.reset(OpAMD64XORQ)
		v0 := b.NewValue0(v_2.Pos, OpAMD64MOVQf2i, typ.UInt64)
		v0.AddArg(y)
		v.AddArg2(x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAMD64XORQmodify(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (XORQmodify [off1] {sym} (ADDQconst [off2] base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2))
	// result: (XORQmodify [off1+off2] {sym} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym := auxToSym(v.Aux)
		if v_0.Op != OpAMD64ADDQconst {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1) + int64(off2))) {
			break
		}
		v.reset(OpAMD64XORQmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(sym)
		v.AddArg3(base, val, mem)
		return true
	}
	// match: (XORQmodify [off1] {sym1} (LEAQ [off2] {sym2} base) val mem)
	// cond: is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)
	// result: (XORQmodify [off1+off2] {mergeSym(sym1,sym2)} base val mem)
	for {
		off1 := auxIntToInt32(v.AuxInt)
		sym1 := auxToSym(v.Aux)
		if v_0.Op != OpAMD64LEAQ {
			break
		}
		off2 := auxIntToInt32(v_0.AuxInt)
		sym2 := auxToSym(v_0.Aux)
		base := v_0.Args[0]
		val := v_1
		mem := v_2
		if !(is32Bit(int64(off1)+int64(off2)) && canMergeSym(sym1, sym2)) {
			break
		}
		v.reset(OpAMD64XORQmodify)
		v.AuxInt = int32ToAuxInt(off1 + off2)
		v.Aux = symToAux(mergeSym(sym1, sym2))
		v.AddArg3(base, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpAddr(v *Value) bool {
	v_0 := v.Args[0]
	// match: (Addr {sym} base)
	// result: (LEAQ {sym} base)
	for {
		sym := auxToSym(v.Aux)
		base := v_0
		v.reset(OpAMD64LEAQ)
		v.Aux = symToAux(sym)
		v.AddArg(base)
		return true
	}
}
func rewriteValueAMD64_OpAtomicAdd32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (AtomicAdd32 ptr val mem)
	// result: (AddTupleFirst32 val (XADDLlock val ptr mem))
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64AddTupleFirst32)
		v0 := b.NewValue0(v.Pos, OpAMD64XADDLlock, types.NewTuple(typ.UInt32, types.TypeMem))
		v0.AddArg3(val, ptr, mem)
		v.AddArg2(val, v0)
		return true
	}
}
func rewriteValueAMD64_OpAtomicAdd64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (AtomicAdd64 ptr val mem)
	// result: (AddTupleFirst64 val (XADDQlock val ptr mem))
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64AddTupleFirst64)
		v0 := b.NewValue0(v.Pos, OpAMD64XADDQlock, types.NewTuple(typ.UInt64, types.TypeMem))
		v0.AddArg3(val, ptr, mem)
		v.AddArg2(val, v0)
		return true
	}
}
func rewriteValueAMD64_OpAtomicAnd32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicAnd32 ptr val mem)
	// result: (ANDLlock ptr val mem)
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64ANDLlock)
		v.AddArg3(ptr, val, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicAnd32value(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicAnd32value ptr val mem)
	// result: (LoweredAtomicAnd32 ptr val mem)
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64LoweredAtomicAnd32)
		v.AddArg3(ptr, val, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicAnd64value(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicAnd64value ptr val mem)
	// result: (LoweredAtomicAnd64 ptr val mem)
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64LoweredAtomicAnd64)
		v.AddArg3(ptr, val, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicAnd8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicAnd8 ptr val mem)
	// result: (ANDBlock ptr val mem)
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64ANDBlock)
		v.AddArg3(ptr, val, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicCompareAndSwap32(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicCompareAndSwap32 ptr old new_ mem)
	// result: (CMPXCHGLlock ptr old new_ mem)
	for {
		ptr := v_0
		old := v_1
		new_ := v_2
		mem := v_3
		v.reset(OpAMD64CMPXCHGLlock)
		v.AddArg4(ptr, old, new_, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicCompareAndSwap64(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicCompareAndSwap64 ptr old new_ mem)
	// result: (CMPXCHGQlock ptr old new_ mem)
	for {
		ptr := v_0
		old := v_1
		new_ := v_2
		mem := v_3
		v.reset(OpAMD64CMPXCHGQlock)
		v.AddArg4(ptr, old, new_, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicExchange32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicExchange32 ptr val mem)
	// result: (XCHGL val ptr mem)
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64XCHGL)
		v.AddArg3(val, ptr, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicExchange64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicExchange64 ptr val mem)
	// result: (XCHGQ val ptr mem)
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64XCHGQ)
		v.AddArg3(val, ptr, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicExchange8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicExchange8 ptr val mem)
	// result: (XCHGB val ptr mem)
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64XCHGB)
		v.AddArg3(val, ptr, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicLoad32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicLoad32 ptr mem)
	// result: (MOVLatomicload ptr mem)
	for {
		ptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVLatomicload)
		v.AddArg2(ptr, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicLoad64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicLoad64 ptr mem)
	// result: (MOVQatomicload ptr mem)
	for {
		ptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVQatomicload)
		v.AddArg2(ptr, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicLoad8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicLoad8 ptr mem)
	// result: (MOVBatomicload ptr mem)
	for {
		ptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVBatomicload)
		v.AddArg2(ptr, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicLoadPtr(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicLoadPtr ptr mem)
	// result: (MOVQatomicload ptr mem)
	for {
		ptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVQatomicload)
		v.AddArg2(ptr, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicOr32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicOr32 ptr val mem)
	// result: (ORLlock ptr val mem)
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64ORLlock)
		v.AddArg3(ptr, val, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicOr32value(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicOr32value ptr val mem)
	// result: (LoweredAtomicOr32 ptr val mem)
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64LoweredAtomicOr32)
		v.AddArg3(ptr, val, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicOr64value(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicOr64value ptr val mem)
	// result: (LoweredAtomicOr64 ptr val mem)
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64LoweredAtomicOr64)
		v.AddArg3(ptr, val, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicOr8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (AtomicOr8 ptr val mem)
	// result: (ORBlock ptr val mem)
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpAMD64ORBlock)
		v.AddArg3(ptr, val, mem)
		return true
	}
}
func rewriteValueAMD64_OpAtomicStore32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (AtomicStore32 ptr val mem)
	// result: (Select1 (XCHGL <types.NewTuple(typ.UInt32,types.TypeMem)> val ptr mem))
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64XCHGL, types.NewTuple(typ.UInt32, types.TypeMem))
		v0.AddArg3(val, ptr, mem)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpAtomicStore64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (AtomicStore64 ptr val mem)
	// result: (Select1 (XCHGQ <types.NewTuple(typ.UInt64,types.TypeMem)> val ptr mem))
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64XCHGQ, types.NewTuple(typ.UInt64, types.TypeMem))
		v0.AddArg3(val, ptr, mem)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpAtomicStore8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (AtomicStore8 ptr val mem)
	// result: (Select1 (XCHGB <types.NewTuple(typ.UInt8,types.TypeMem)> val ptr mem))
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64XCHGB, types.NewTuple(typ.UInt8, types.TypeMem))
		v0.AddArg3(val, ptr, mem)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpAtomicStorePtrNoWB(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (AtomicStorePtrNoWB ptr val mem)
	// result: (Select1 (XCHGQ <types.NewTuple(typ.BytePtr,types.TypeMem)> val ptr mem))
	for {
		ptr := v_0
		val := v_1
		mem := v_2
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64XCHGQ, types.NewTuple(typ.BytePtr, types.TypeMem))
		v0.AddArg3(val, ptr, mem)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpBitLen16(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (BitLen16 x)
	// cond: buildcfg.GOAMD64 < 3
	// result: (BSRL (LEAL1 <typ.UInt32> [1] (MOVWQZX <typ.UInt32> x) (MOVWQZX <typ.UInt32> x)))
	for {
		x := v_0
		if !(buildcfg.GOAMD64 < 3) {
			break
		}
		v.reset(OpAMD64BSRL)
		v0 := b.NewValue0(v.Pos, OpAMD64LEAL1, typ.UInt32)
		v0.AuxInt = int32ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVWQZX, typ.UInt32)
		v1.AddArg(x)
		v0.AddArg2(v1, v1)
		v.AddArg(v0)
		return true
	}
	// match: (BitLen16 <t> x)
	// cond: buildcfg.GOAMD64 >= 3
	// result: (NEGQ (ADDQconst <t> [-32] (LZCNTL (MOVWQZX <x.Type> x))))
	for {
		t := v.Type
		x := v_0
		if !(buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64NEGQ)
		v0 := b.NewValue0(v.Pos, OpAMD64ADDQconst, t)
		v0.AuxInt = int32ToAuxInt(-32)
		v1 := b.NewValue0(v.Pos, OpAMD64LZCNTL, typ.UInt32)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVWQZX, x.Type)
		v2.AddArg(x)
		v1.AddArg(v2)
		v0.AddArg(v1)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpBitLen32(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (BitLen32 x)
	// cond: buildcfg.GOAMD64 < 3
	// result: (Select0 (BSRQ (LEAQ1 <typ.UInt64> [1] (MOVLQZX <typ.UInt64> x) (MOVLQZX <typ.UInt64> x))))
	for {
		x := v_0
		if !(buildcfg.GOAMD64 < 3) {
			break
		}
		v.reset(OpSelect0)
		v0 := b.NewValue0(v.Pos, OpAMD64BSRQ, types.NewTuple(typ.UInt64, types.TypeFlags))
		v1 := b.NewValue0(v.Pos, OpAMD64LEAQ1, typ.UInt64)
		v1.AuxInt = int32ToAuxInt(1)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVLQZX, typ.UInt64)
		v2.AddArg(x)
		v1.AddArg2(v2, v2)
		v0.AddArg(v1)
		v.AddArg(v0)
		return true
	}
	// match: (BitLen32 <t> x)
	// cond: buildcfg.GOAMD64 >= 3
	// result: (NEGQ (ADDQconst <t> [-32] (LZCNTL x)))
	for {
		t := v.Type
		x := v_0
		if !(buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64NEGQ)
		v0 := b.NewValue0(v.Pos, OpAMD64ADDQconst, t)
		v0.AuxInt = int32ToAuxInt(-32)
		v1 := b.NewValue0(v.Pos, OpAMD64LZCNTL, typ.UInt32)
		v1.AddArg(x)
		v0.AddArg(v1)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpBitLen64(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (BitLen64 <t> x)
	// cond: buildcfg.GOAMD64 < 3
	// result: (ADDQconst [1] (CMOVQEQ <t> (Select0 <t> (BSRQ x)) (MOVQconst <t> [-1]) (Select1 <types.TypeFlags> (BSRQ x))))
	for {
		t := v.Type
		x := v_0
		if !(buildcfg.GOAMD64 < 3) {
			break
		}
		v.reset(OpAMD64ADDQconst)
		v.AuxInt = int32ToAuxInt(1)
		v0 := b.NewValue0(v.Pos, OpAMD64CMOVQEQ, t)
		v1 := b.NewValue0(v.Pos, OpSelect0, t)
		v2 := b.NewValue0(v.Pos, OpAMD64BSRQ, types.NewTuple(typ.UInt64, types.TypeFlags))
		v2.AddArg(x)
		v1.AddArg(v2)
		v3 := b.NewValue0(v.Pos, OpAMD64MOVQconst, t)
		v3.AuxInt = int64ToAuxInt(-1)
		v4 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
		v4.AddArg(v2)
		v0.AddArg3(v1, v3, v4)
		v.AddArg(v0)
		return true
	}
	// match: (BitLen64 <t> x)
	// cond: buildcfg.GOAMD64 >= 3
	// result: (NEGQ (ADDQconst <t> [-64] (LZCNTQ x)))
	for {
		t := v.Type
		x := v_0
		if !(buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64NEGQ)
		v0 := b.NewValue0(v.Pos, OpAMD64ADDQconst, t)
		v0.AuxInt = int32ToAuxInt(-64)
		v1 := b.NewValue0(v.Pos, OpAMD64LZCNTQ, typ.UInt64)
		v1.AddArg(x)
		v0.AddArg(v1)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpBitLen8(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (BitLen8 x)
	// cond: buildcfg.GOAMD64 < 3
	// result: (BSRL (LEAL1 <typ.UInt32> [1] (MOVBQZX <typ.UInt32> x) (MOVBQZX <typ.UInt32> x)))
	for {
		x := v_0
		if !(buildcfg.GOAMD64 < 3) {
			break
		}
		v.reset(OpAMD64BSRL)
		v0 := b.NewValue0(v.Pos, OpAMD64LEAL1, typ.UInt32)
		v0.AuxInt = int32ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVBQZX, typ.UInt32)
		v1.AddArg(x)
		v0.AddArg2(v1, v1)
		v.AddArg(v0)
		return true
	}
	// match: (BitLen8 <t> x)
	// cond: buildcfg.GOAMD64 >= 3
	// result: (NEGQ (ADDQconst <t> [-32] (LZCNTL (MOVBQZX <x.Type> x))))
	for {
		t := v.Type
		x := v_0
		if !(buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64NEGQ)
		v0 := b.NewValue0(v.Pos, OpAMD64ADDQconst, t)
		v0.AuxInt = int32ToAuxInt(-32)
		v1 := b.NewValue0(v.Pos, OpAMD64LZCNTL, typ.UInt32)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVBQZX, x.Type)
		v2.AddArg(x)
		v1.AddArg(v2)
		v0.AddArg(v1)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpBswap16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (Bswap16 x)
	// result: (ROLWconst [8] x)
	for {
		x := v_0
		v.reset(OpAMD64ROLWconst)
		v.AuxInt = int8ToAuxInt(8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeil(v *Value) bool {
	v_0 := v.Args[0]
	// match: (Ceil x)
	// result: (ROUNDSD [2] x)
	for {
		x := v_0
		v.reset(OpAMD64ROUNDSD)
		v.AuxInt = int8ToAuxInt(2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilFloat32x4 x)
	// result: (VROUNDPS128 [2] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPS128)
		v.AuxInt = int8ToAuxInt(2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilFloat32x8 x)
	// result: (VROUNDPS256 [2] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPS256)
		v.AuxInt = int8ToAuxInt(2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilFloat64x2 x)
	// result: (VROUNDPD128 [2] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPD128)
		v.AuxInt = int8ToAuxInt(2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilFloat64x4 x)
	// result: (VROUNDPD256 [2] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPD256)
		v.AuxInt = int8ToAuxInt(2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilSuppressExceptionWithPrecisionFloat32x16 [a] x)
	// result: (VRNDSCALEPS512 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS512)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilSuppressExceptionWithPrecisionFloat32x4 [a] x)
	// result: (VRNDSCALEPS128 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS128)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilSuppressExceptionWithPrecisionFloat32x8 [a] x)
	// result: (VRNDSCALEPS256 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS256)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilSuppressExceptionWithPrecisionFloat64x2 [a] x)
	// result: (VRNDSCALEPD128 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD128)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilSuppressExceptionWithPrecisionFloat64x4 [a] x)
	// result: (VRNDSCALEPD256 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD256)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilSuppressExceptionWithPrecisionFloat64x8 [a] x)
	// result: (VRNDSCALEPD512 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD512)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilWithPrecisionFloat32x16 [a] x)
	// result: (VRNDSCALEPS512 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS512)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilWithPrecisionFloat32x4 [a] x)
	// result: (VRNDSCALEPS128 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS128)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilWithPrecisionFloat32x8 [a] x)
	// result: (VRNDSCALEPS256 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS256)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilWithPrecisionFloat64x2 [a] x)
	// result: (VRNDSCALEPD128 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD128)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilWithPrecisionFloat64x4 [a] x)
	// result: (VRNDSCALEPD256 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD256)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCeilWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (CeilWithPrecisionFloat64x8 [a] x)
	// result: (VRNDSCALEPD512 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD512)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpCondSelect(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (CondSelect <t> x y (SETEQ cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQEQ y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETEQ {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQEQ)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETNE cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQNE y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETNE {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQNE)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETL cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQLT y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETL {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQLT)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETG cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQGT y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETG {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQGT)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETLE cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQLE y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETLE {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQLE)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETGE cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQGE y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETGE {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQGE)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETA cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQHI y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETA {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQHI)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETB cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQCS y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETB {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQCS)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETAE cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQCC y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETAE {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQCC)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETBE cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQLS y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETBE {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQLS)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETEQF cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQEQF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETEQF {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQEQF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETNEF cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQNEF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETNEF {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQNEF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETGF cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQGTF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETGF {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQGTF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETGEF cond))
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (CMOVQGEF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETGEF {
			break
		}
		cond := v_2.Args[0]
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64CMOVQGEF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETEQ cond))
	// cond: is32BitInt(t)
	// result: (CMOVLEQ y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETEQ {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLEQ)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETNE cond))
	// cond: is32BitInt(t)
	// result: (CMOVLNE y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETNE {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLNE)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETL cond))
	// cond: is32BitInt(t)
	// result: (CMOVLLT y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETL {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLLT)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETG cond))
	// cond: is32BitInt(t)
	// result: (CMOVLGT y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETG {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLGT)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETLE cond))
	// cond: is32BitInt(t)
	// result: (CMOVLLE y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETLE {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLLE)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETGE cond))
	// cond: is32BitInt(t)
	// result: (CMOVLGE y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETGE {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLGE)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETA cond))
	// cond: is32BitInt(t)
	// result: (CMOVLHI y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETA {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLHI)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETB cond))
	// cond: is32BitInt(t)
	// result: (CMOVLCS y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETB {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLCS)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETAE cond))
	// cond: is32BitInt(t)
	// result: (CMOVLCC y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETAE {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLCC)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETBE cond))
	// cond: is32BitInt(t)
	// result: (CMOVLLS y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETBE {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLLS)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETEQF cond))
	// cond: is32BitInt(t)
	// result: (CMOVLEQF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETEQF {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLEQF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETNEF cond))
	// cond: is32BitInt(t)
	// result: (CMOVLNEF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETNEF {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLNEF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETGF cond))
	// cond: is32BitInt(t)
	// result: (CMOVLGTF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETGF {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLGTF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETGEF cond))
	// cond: is32BitInt(t)
	// result: (CMOVLGEF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETGEF {
			break
		}
		cond := v_2.Args[0]
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLGEF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETEQ cond))
	// cond: is16BitInt(t)
	// result: (CMOVWEQ y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETEQ {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWEQ)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETNE cond))
	// cond: is16BitInt(t)
	// result: (CMOVWNE y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETNE {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWNE)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETL cond))
	// cond: is16BitInt(t)
	// result: (CMOVWLT y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETL {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWLT)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETG cond))
	// cond: is16BitInt(t)
	// result: (CMOVWGT y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETG {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWGT)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETLE cond))
	// cond: is16BitInt(t)
	// result: (CMOVWLE y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETLE {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWLE)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETGE cond))
	// cond: is16BitInt(t)
	// result: (CMOVWGE y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETGE {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWGE)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETA cond))
	// cond: is16BitInt(t)
	// result: (CMOVWHI y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETA {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWHI)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETB cond))
	// cond: is16BitInt(t)
	// result: (CMOVWCS y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETB {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWCS)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETAE cond))
	// cond: is16BitInt(t)
	// result: (CMOVWCC y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETAE {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWCC)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETBE cond))
	// cond: is16BitInt(t)
	// result: (CMOVWLS y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETBE {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWLS)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETEQF cond))
	// cond: is16BitInt(t)
	// result: (CMOVWEQF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETEQF {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWEQF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETNEF cond))
	// cond: is16BitInt(t)
	// result: (CMOVWNEF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETNEF {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWNEF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETGF cond))
	// cond: is16BitInt(t)
	// result: (CMOVWGTF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETGF {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWGTF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y (SETGEF cond))
	// cond: is16BitInt(t)
	// result: (CMOVWGEF y x cond)
	for {
		t := v.Type
		x := v_0
		y := v_1
		if v_2.Op != OpAMD64SETGEF {
			break
		}
		cond := v_2.Args[0]
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWGEF)
		v.AddArg3(y, x, cond)
		return true
	}
	// match: (CondSelect <t> x y check)
	// cond: !check.Type.IsFlags() && check.Type.Size() == 1
	// result: (CondSelect <t> x y (MOVBQZX <typ.UInt64> check))
	for {
		t := v.Type
		x := v_0
		y := v_1
		check := v_2
		if !(!check.Type.IsFlags() && check.Type.Size() == 1) {
			break
		}
		v.reset(OpCondSelect)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64MOVBQZX, typ.UInt64)
		v0.AddArg(check)
		v.AddArg3(x, y, v0)
		return true
	}
	// match: (CondSelect <t> x y check)
	// cond: !check.Type.IsFlags() && check.Type.Size() == 2
	// result: (CondSelect <t> x y (MOVWQZX <typ.UInt64> check))
	for {
		t := v.Type
		x := v_0
		y := v_1
		check := v_2
		if !(!check.Type.IsFlags() && check.Type.Size() == 2) {
			break
		}
		v.reset(OpCondSelect)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64MOVWQZX, typ.UInt64)
		v0.AddArg(check)
		v.AddArg3(x, y, v0)
		return true
	}
	// match: (CondSelect <t> x y check)
	// cond: !check.Type.IsFlags() && check.Type.Size() == 4
	// result: (CondSelect <t> x y (MOVLQZX <typ.UInt64> check))
	for {
		t := v.Type
		x := v_0
		y := v_1
		check := v_2
		if !(!check.Type.IsFlags() && check.Type.Size() == 4) {
			break
		}
		v.reset(OpCondSelect)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLQZX, typ.UInt64)
		v0.AddArg(check)
		v.AddArg3(x, y, v0)
		return true
	}
	// match: (CondSelect <t> x y check)
	// cond: !check.Type.IsFlags() && check.Type.Size() == 8 && (is64BitInt(t) || isPtr(t))
	// result: (CMOVQNE y x (CMPQconst [0] check))
	for {
		t := v.Type
		x := v_0
		y := v_1
		check := v_2
		if !(!check.Type.IsFlags() && check.Type.Size() == 8 && (is64BitInt(t) || isPtr(t))) {
			break
		}
		v.reset(OpAMD64CMOVQNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v0.AddArg(check)
		v.AddArg3(y, x, v0)
		return true
	}
	// match: (CondSelect <t> x y check)
	// cond: !check.Type.IsFlags() && check.Type.Size() == 8 && is32BitInt(t)
	// result: (CMOVLNE y x (CMPQconst [0] check))
	for {
		t := v.Type
		x := v_0
		y := v_1
		check := v_2
		if !(!check.Type.IsFlags() && check.Type.Size() == 8 && is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVLNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v0.AddArg(check)
		v.AddArg3(y, x, v0)
		return true
	}
	// match: (CondSelect <t> x y check)
	// cond: !check.Type.IsFlags() && check.Type.Size() == 8 && is16BitInt(t)
	// result: (CMOVWNE y x (CMPQconst [0] check))
	for {
		t := v.Type
		x := v_0
		y := v_1
		check := v_2
		if !(!check.Type.IsFlags() && check.Type.Size() == 8 && is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64CMOVWNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v0.AddArg(check)
		v.AddArg3(y, x, v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpConst16(v *Value) bool {
	// match: (Const16 [c])
	// result: (MOVLconst [int32(c)])
	for {
		c := auxIntToInt16(v.AuxInt)
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(int32(c))
		return true
	}
}
func rewriteValueAMD64_OpConst8(v *Value) bool {
	// match: (Const8 [c])
	// result: (MOVLconst [int32(c)])
	for {
		c := auxIntToInt8(v.AuxInt)
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(int32(c))
		return true
	}
}
func rewriteValueAMD64_OpConstBool(v *Value) bool {
	// match: (ConstBool [c])
	// result: (MOVLconst [b2i32(c)])
	for {
		c := auxIntToBool(v.AuxInt)
		v.reset(OpAMD64MOVLconst)
		v.AuxInt = int32ToAuxInt(b2i32(c))
		return true
	}
}
func rewriteValueAMD64_OpConstNil(v *Value) bool {
	// match: (ConstNil )
	// result: (MOVQconst [0])
	for {
		v.reset(OpAMD64MOVQconst)
		v.AuxInt = int64ToAuxInt(0)
		return true
	}
}
func rewriteValueAMD64_OpCtz16(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Ctz16 x)
	// result: (BSFL (ORLconst <typ.UInt32> [1<<16] x))
	for {
		x := v_0
		v.reset(OpAMD64BSFL)
		v0 := b.NewValue0(v.Pos, OpAMD64ORLconst, typ.UInt32)
		v0.AuxInt = int32ToAuxInt(1 << 16)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpCtz16NonZero(v *Value) bool {
	v_0 := v.Args[0]
	// match: (Ctz16NonZero x)
	// cond: buildcfg.GOAMD64 >= 3
	// result: (TZCNTL x)
	for {
		x := v_0
		if !(buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64TZCNTL)
		v.AddArg(x)
		return true
	}
	// match: (Ctz16NonZero x)
	// cond: buildcfg.GOAMD64 < 3
	// result: (BSFL x)
	for {
		x := v_0
		if !(buildcfg.GOAMD64 < 3) {
			break
		}
		v.reset(OpAMD64BSFL)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpCtz32(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Ctz32 x)
	// cond: buildcfg.GOAMD64 >= 3
	// result: (TZCNTL x)
	for {
		x := v_0
		if !(buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64TZCNTL)
		v.AddArg(x)
		return true
	}
	// match: (Ctz32 x)
	// cond: buildcfg.GOAMD64 < 3
	// result: (Select0 (BSFQ (BTSQconst <typ.UInt64> [32] x)))
	for {
		x := v_0
		if !(buildcfg.GOAMD64 < 3) {
			break
		}
		v.reset(OpSelect0)
		v0 := b.NewValue0(v.Pos, OpAMD64BSFQ, types.NewTuple(typ.UInt64, types.TypeFlags))
		v1 := b.NewValue0(v.Pos, OpAMD64BTSQconst, typ.UInt64)
		v1.AuxInt = int8ToAuxInt(32)
		v1.AddArg(x)
		v0.AddArg(v1)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpCtz32NonZero(v *Value) bool {
	v_0 := v.Args[0]
	// match: (Ctz32NonZero x)
	// cond: buildcfg.GOAMD64 >= 3
	// result: (TZCNTL x)
	for {
		x := v_0
		if !(buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64TZCNTL)
		v.AddArg(x)
		return true
	}
	// match: (Ctz32NonZero x)
	// cond: buildcfg.GOAMD64 < 3
	// result: (BSFL x)
	for {
		x := v_0
		if !(buildcfg.GOAMD64 < 3) {
			break
		}
		v.reset(OpAMD64BSFL)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpCtz64(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Ctz64 x)
	// cond: buildcfg.GOAMD64 >= 3
	// result: (TZCNTQ x)
	for {
		x := v_0
		if !(buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64TZCNTQ)
		v.AddArg(x)
		return true
	}
	// match: (Ctz64 <t> x)
	// cond: buildcfg.GOAMD64 < 3
	// result: (CMOVQEQ (Select0 <t> (BSFQ x)) (MOVQconst <t> [64]) (Select1 <types.TypeFlags> (BSFQ x)))
	for {
		t := v.Type
		x := v_0
		if !(buildcfg.GOAMD64 < 3) {
			break
		}
		v.reset(OpAMD64CMOVQEQ)
		v0 := b.NewValue0(v.Pos, OpSelect0, t)
		v1 := b.NewValue0(v.Pos, OpAMD64BSFQ, types.NewTuple(typ.UInt64, types.TypeFlags))
		v1.AddArg(x)
		v0.AddArg(v1)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVQconst, t)
		v2.AuxInt = int64ToAuxInt(64)
		v3 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
		v3.AddArg(v1)
		v.AddArg3(v0, v2, v3)
		return true
	}
	return false
}
func rewriteValueAMD64_OpCtz64NonZero(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Ctz64NonZero x)
	// cond: buildcfg.GOAMD64 >= 3
	// result: (TZCNTQ x)
	for {
		x := v_0
		if !(buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64TZCNTQ)
		v.AddArg(x)
		return true
	}
	// match: (Ctz64NonZero x)
	// cond: buildcfg.GOAMD64 < 3
	// result: (Select0 (BSFQ x))
	for {
		x := v_0
		if !(buildcfg.GOAMD64 < 3) {
			break
		}
		v.reset(OpSelect0)
		v0 := b.NewValue0(v.Pos, OpAMD64BSFQ, types.NewTuple(typ.UInt64, types.TypeFlags))
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
	return false
}
func rewriteValueAMD64_OpCtz8(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Ctz8 x)
	// result: (BSFL (ORLconst <typ.UInt32> [1<<8 ] x))
	for {
		x := v_0
		v.reset(OpAMD64BSFL)
		v0 := b.NewValue0(v.Pos, OpAMD64ORLconst, typ.UInt32)
		v0.AuxInt = int32ToAuxInt(1 << 8)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpCtz8NonZero(v *Value) bool {
	v_0 := v.Args[0]
	// match: (Ctz8NonZero x)
	// cond: buildcfg.GOAMD64 >= 3
	// result: (TZCNTL x)
	for {
		x := v_0
		if !(buildcfg.GOAMD64 >= 3) {
			break
		}
		v.reset(OpAMD64TZCNTL)
		v.AddArg(x)
		return true
	}
	// match: (Ctz8NonZero x)
	// cond: buildcfg.GOAMD64 < 3
	// result: (BSFL x)
	for {
		x := v_0
		if !(buildcfg.GOAMD64 < 3) {
			break
		}
		v.reset(OpAMD64BSFL)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilSuppressExceptionWithPrecisionFloat32x16 [a] x)
	// result: (VREDUCEPS512 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS512)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilSuppressExceptionWithPrecisionFloat32x4 [a] x)
	// result: (VREDUCEPS128 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS128)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilSuppressExceptionWithPrecisionFloat32x8 [a] x)
	// result: (VREDUCEPS256 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS256)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilSuppressExceptionWithPrecisionFloat64x2 [a] x)
	// result: (VREDUCEPD128 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD128)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilSuppressExceptionWithPrecisionFloat64x4 [a] x)
	// result: (VREDUCEPD256 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD256)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithCeilSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilSuppressExceptionWithPrecisionFloat64x8 [a] x)
	// result: (VREDUCEPD512 [a+10] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD512)
		v.AuxInt = int8ToAuxInt(a + 10)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilWithPrecisionFloat32x16 [a] x)
	// result: (VREDUCEPS512 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS512)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilWithPrecisionFloat32x4 [a] x)
	// result: (VREDUCEPS128 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS128)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilWithPrecisionFloat32x8 [a] x)
	// result: (VREDUCEPS256 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS256)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilWithPrecisionFloat64x2 [a] x)
	// result: (VREDUCEPD128 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD128)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilWithPrecisionFloat64x4 [a] x)
	// result: (VREDUCEPD256 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD256)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithCeilWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithCeilWithPrecisionFloat64x8 [a] x)
	// result: (VREDUCEPD512 [a+2] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD512)
		v.AuxInt = int8ToAuxInt(a + 2)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorSuppressExceptionWithPrecisionFloat32x16 [a] x)
	// result: (VREDUCEPS512 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS512)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorSuppressExceptionWithPrecisionFloat32x4 [a] x)
	// result: (VREDUCEPS128 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS128)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorSuppressExceptionWithPrecisionFloat32x8 [a] x)
	// result: (VREDUCEPS256 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS256)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorSuppressExceptionWithPrecisionFloat64x2 [a] x)
	// result: (VREDUCEPD128 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD128)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorSuppressExceptionWithPrecisionFloat64x4 [a] x)
	// result: (VREDUCEPD256 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD256)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorSuppressExceptionWithPrecisionFloat64x8 [a] x)
	// result: (VREDUCEPD512 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD512)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorWithPrecisionFloat32x16 [a] x)
	// result: (VREDUCEPS512 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS512)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorWithPrecisionFloat32x4 [a] x)
	// result: (VREDUCEPS128 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS128)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorWithPrecisionFloat32x8 [a] x)
	// result: (VREDUCEPS256 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS256)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorWithPrecisionFloat64x2 [a] x)
	// result: (VREDUCEPD128 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD128)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorWithPrecisionFloat64x4 [a] x)
	// result: (VREDUCEPD256 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD256)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithFloorWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithFloorWithPrecisionFloat64x8 [a] x)
	// result: (VREDUCEPD512 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD512)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundSuppressExceptionWithPrecisionFloat32x16 [a] x)
	// result: (VREDUCEPS512 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS512)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundSuppressExceptionWithPrecisionFloat32x4 [a] x)
	// result: (VREDUCEPS128 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS128)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundSuppressExceptionWithPrecisionFloat32x8 [a] x)
	// result: (VREDUCEPS256 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS256)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundSuppressExceptionWithPrecisionFloat64x2 [a] x)
	// result: (VREDUCEPD128 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD128)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundSuppressExceptionWithPrecisionFloat64x4 [a] x)
	// result: (VREDUCEPD256 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD256)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundSuppressExceptionWithPrecisionFloat64x8 [a] x)
	// result: (VREDUCEPD512 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD512)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundWithPrecisionFloat32x16 [a] x)
	// result: (VREDUCEPS512 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS512)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundWithPrecisionFloat32x4 [a] x)
	// result: (VREDUCEPS128 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS128)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundWithPrecisionFloat32x8 [a] x)
	// result: (VREDUCEPS256 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS256)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundWithPrecisionFloat64x2 [a] x)
	// result: (VREDUCEPD128 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD128)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundWithPrecisionFloat64x4 [a] x)
	// result: (VREDUCEPD256 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD256)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithRoundWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithRoundWithPrecisionFloat64x8 [a] x)
	// result: (VREDUCEPD512 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD512)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncSuppressExceptionWithPrecisionFloat32x16 [a] x)
	// result: (VREDUCEPS512 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS512)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncSuppressExceptionWithPrecisionFloat32x4 [a] x)
	// result: (VREDUCEPS128 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS128)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncSuppressExceptionWithPrecisionFloat32x8 [a] x)
	// result: (VREDUCEPS256 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS256)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncSuppressExceptionWithPrecisionFloat64x2 [a] x)
	// result: (VREDUCEPD128 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD128)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncSuppressExceptionWithPrecisionFloat64x4 [a] x)
	// result: (VREDUCEPD256 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD256)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncSuppressExceptionWithPrecisionFloat64x8 [a] x)
	// result: (VREDUCEPD512 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD512)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncWithPrecisionFloat32x16 [a] x)
	// result: (VREDUCEPS512 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS512)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncWithPrecisionFloat32x4 [a] x)
	// result: (VREDUCEPS128 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS128)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncWithPrecisionFloat32x8 [a] x)
	// result: (VREDUCEPS256 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPS256)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncWithPrecisionFloat64x2 [a] x)
	// result: (VREDUCEPD128 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD128)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncWithPrecisionFloat64x4 [a] x)
	// result: (VREDUCEPD256 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD256)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiffWithTruncWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (DiffWithTruncWithPrecisionFloat64x8 [a] x)
	// result: (VREDUCEPD512 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VREDUCEPD512)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpDiv16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Div16 [a] x y)
	// result: (Select0 (DIVW [a] x y))
	for {
		a := auxIntToBool(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpSelect0)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, types.NewTuple(typ.Int16, typ.Int16))
		v0.AuxInt = boolToAuxInt(a)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpDiv16u(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Div16u x y)
	// result: (Select0 (DIVWU x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpSelect0)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, types.NewTuple(typ.UInt16, typ.UInt16))
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpDiv32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Div32 [a] x y)
	// result: (Select0 (DIVL [a] x y))
	for {
		a := auxIntToBool(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpSelect0)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVL, types.NewTuple(typ.Int32, typ.Int32))
		v0.AuxInt = boolToAuxInt(a)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpDiv32u(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Div32u x y)
	// result: (Select0 (DIVLU x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpSelect0)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVLU, types.NewTuple(typ.UInt32, typ.UInt32))
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpDiv64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Div64 [a] x y)
	// result: (Select0 (DIVQ [a] x y))
	for {
		a := auxIntToBool(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpSelect0)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVQ, types.NewTuple(typ.Int64, typ.Int64))
		v0.AuxInt = boolToAuxInt(a)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpDiv64u(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Div64u x y)
	// result: (Select0 (DIVQU x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpSelect0)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVQU, types.NewTuple(typ.UInt64, typ.UInt64))
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpDiv8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Div8 x y)
	// result: (Select0 (DIVW (SignExt8to16 x) (SignExt8to16 y)))
	for {
		x := v_0
		y := v_1
		v.reset(OpSelect0)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, types.NewTuple(typ.Int16, typ.Int16))
		v1 := b.NewValue0(v.Pos, OpSignExt8to16, typ.Int16)
		v1.AddArg(x)
		v2 := b.NewValue0(v.Pos, OpSignExt8to16, typ.Int16)
		v2.AddArg(y)
		v0.AddArg2(v1, v2)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpDiv8u(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Div8u x y)
	// result: (Select0 (DIVWU (ZeroExt8to16 x) (ZeroExt8to16 y)))
	for {
		x := v_0
		y := v_1
		v.reset(OpSelect0)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, types.NewTuple(typ.UInt16, typ.UInt16))
		v1 := b.NewValue0(v.Pos, OpZeroExt8to16, typ.UInt16)
		v1.AddArg(x)
		v2 := b.NewValue0(v.Pos, OpZeroExt8to16, typ.UInt16)
		v2.AddArg(y)
		v0.AddArg2(v1, v2)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpDotProdBroadcastFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (DotProdBroadcastFloat64x2 x y)
	// result: (VDPPD128 [127] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VDPPD128)
		v.AuxInt = int8ToAuxInt(127)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpEq16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Eq16 x y)
	// result: (SETEQ (CMPW x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETEQ)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPW, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEq32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Eq32 x y)
	// result: (SETEQ (CMPL x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETEQ)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPL, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEq32F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Eq32F x y)
	// result: (SETEQF (UCOMISS x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETEQF)
		v0 := b.NewValue0(v.Pos, OpAMD64UCOMISS, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEq64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Eq64 x y)
	// result: (SETEQ (CMPQ x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETEQ)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEq64F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Eq64F x y)
	// result: (SETEQF (UCOMISD x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETEQF)
		v0 := b.NewValue0(v.Pos, OpAMD64UCOMISD, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEq8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Eq8 x y)
	// result: (SETEQ (CMPB x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETEQ)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPB, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqB(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (EqB x y)
	// result: (SETEQ (CMPB x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETEQ)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPB, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqPtr(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (EqPtr x y)
	// result: (SETEQ (CMPQ x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETEQ)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualFloat32x16 x y)
	// result: (VPMOVMToVec32x16 (VCMPPS512 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPS512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (EqualFloat32x4 x y)
	// result: (VCMPPS128 [0] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS128)
		v.AuxInt = int8ToAuxInt(0)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpEqualFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (EqualFloat32x8 x y)
	// result: (VCMPPS256 [0] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS256)
		v.AuxInt = int8ToAuxInt(0)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpEqualFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (EqualFloat64x2 x y)
	// result: (VCMPPD128 [0] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD128)
		v.AuxInt = int8ToAuxInt(0)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpEqualFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (EqualFloat64x4 x y)
	// result: (VCMPPD256 [0] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD256)
		v.AuxInt = int8ToAuxInt(0)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpEqualFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualFloat64x8 x y)
	// result: (VPMOVMToVec64x8 (VCMPPD512 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualInt16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualInt16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPW512 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualInt32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPD512 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualInt64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPQ512 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualInt8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualInt8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPB512 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint16x16 x y)
	// result: (VPMOVMToVec16x16 (VPCMPUW256 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPUW512 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint16x8 x y)
	// result: (VPMOVMToVec16x8 (VPCMPUW128 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPUD512 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint32x4 x y)
	// result: (VPMOVMToVec32x4 (VPCMPUD128 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint32x8 x y)
	// result: (VPMOVMToVec32x8 (VPCMPUD256 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint64x2 x y)
	// result: (VPMOVMToVec64x2 (VPCMPUQ128 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint64x4 x y)
	// result: (VPMOVMToVec64x4 (VPCMPUQ256 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPUQ512 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint8x16 x y)
	// result: (VPMOVMToVec8x16 (VPCMPUB128 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint8x32 x y)
	// result: (VPMOVMToVec8x32 (VPCMPUB256 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpEqualUint8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (EqualUint8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPUB512 [0] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpFMA(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (FMA x y z)
	// result: (VFMADD231SD z x y)
	for {
		x := v_0
		y := v_1
		z := v_2
		v.reset(OpAMD64VFMADD231SD)
		v.AddArg3(z, x, y)
		return true
	}
}
func rewriteValueAMD64_OpFloor(v *Value) bool {
	v_0 := v.Args[0]
	// match: (Floor x)
	// result: (ROUNDSD [1] x)
	for {
		x := v_0
		v.reset(OpAMD64ROUNDSD)
		v.AuxInt = int8ToAuxInt(1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorFloat32x4 x)
	// result: (VROUNDPS128 [1] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPS128)
		v.AuxInt = int8ToAuxInt(1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorFloat32x8 x)
	// result: (VROUNDPS256 [1] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPS256)
		v.AuxInt = int8ToAuxInt(1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorFloat64x2 x)
	// result: (VROUNDPD128 [1] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPD128)
		v.AuxInt = int8ToAuxInt(1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorFloat64x4 x)
	// result: (VROUNDPD256 [1] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPD256)
		v.AuxInt = int8ToAuxInt(1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorSuppressExceptionWithPrecisionFloat32x16 [a] x)
	// result: (VRNDSCALEPS512 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS512)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorSuppressExceptionWithPrecisionFloat32x4 [a] x)
	// result: (VRNDSCALEPS128 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS128)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorSuppressExceptionWithPrecisionFloat32x8 [a] x)
	// result: (VRNDSCALEPS256 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS256)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorSuppressExceptionWithPrecisionFloat64x2 [a] x)
	// result: (VRNDSCALEPD128 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD128)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorSuppressExceptionWithPrecisionFloat64x4 [a] x)
	// result: (VRNDSCALEPD256 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD256)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorSuppressExceptionWithPrecisionFloat64x8 [a] x)
	// result: (VRNDSCALEPD512 [a+9] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD512)
		v.AuxInt = int8ToAuxInt(a + 9)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorWithPrecisionFloat32x16 [a] x)
	// result: (VRNDSCALEPS512 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS512)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorWithPrecisionFloat32x4 [a] x)
	// result: (VRNDSCALEPS128 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS128)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorWithPrecisionFloat32x8 [a] x)
	// result: (VRNDSCALEPS256 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS256)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorWithPrecisionFloat64x2 [a] x)
	// result: (VRNDSCALEPD128 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD128)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorWithPrecisionFloat64x4 [a] x)
	// result: (VRNDSCALEPD256 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD256)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpFloorWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (FloorWithPrecisionFloat64x8 [a] x)
	// result: (VRNDSCALEPD512 [a+1] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD512)
		v.AuxInt = int8ToAuxInt(a + 1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpGetElemInt16x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (GetElemInt16x8 [a] x)
	// result: (VPEXTRW128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPEXTRW128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpGetElemInt32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (GetElemInt32x4 [a] x)
	// result: (VPEXTRD128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPEXTRD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpGetElemInt64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (GetElemInt64x2 [a] x)
	// result: (VPEXTRQ128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPEXTRQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpGetElemInt8x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (GetElemInt8x16 [a] x)
	// result: (VPEXTRB128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPEXTRB128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpGetElemUint16x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (GetElemUint16x8 [a] x)
	// result: (VPEXTRW128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPEXTRW128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpGetElemUint32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (GetElemUint32x4 [a] x)
	// result: (VPEXTRD128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPEXTRD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpGetElemUint64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (GetElemUint64x2 [a] x)
	// result: (VPEXTRQ128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPEXTRQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpGetElemUint8x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (GetElemUint8x16 [a] x)
	// result: (VPEXTRB128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPEXTRB128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpGetG(v *Value) bool {
	v_0 := v.Args[0]
	// match: (GetG mem)
	// cond: v.Block.Func.OwnAux.Fn.ABI() != obj.ABIInternal
	// result: (LoweredGetG mem)
	for {
		mem := v_0
		if !(v.Block.Func.OwnAux.Fn.ABI() != obj.ABIInternal) {
			break
		}
		v.reset(OpAMD64LoweredGetG)
		v.AddArg(mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpGreaterEqualFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualFloat32x16 x y)
	// result: (VPMOVMToVec32x16 (VCMPPS512 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPS512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (GreaterEqualFloat32x4 x y)
	// result: (VCMPPS128 [5] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS128)
		v.AuxInt = int8ToAuxInt(5)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (GreaterEqualFloat32x8 x y)
	// result: (VCMPPS256 [5] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS256)
		v.AuxInt = int8ToAuxInt(5)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (GreaterEqualFloat64x2 x y)
	// result: (VCMPPD128 [5] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD128)
		v.AuxInt = int8ToAuxInt(5)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (GreaterEqualFloat64x4 x y)
	// result: (VCMPPD256 [5] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD256)
		v.AuxInt = int8ToAuxInt(5)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualFloat64x8 x y)
	// result: (VPMOVMToVec64x8 (VCMPPD512 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt16x16 x y)
	// result: (VPMOVMToVec16x16 (VPCMPW256 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPW512 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt16x8 x y)
	// result: (VPMOVMToVec16x8 (VPCMPW128 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPD512 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt32x4 x y)
	// result: (VPMOVMToVec32x4 (VPCMPD128 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt32x8 x y)
	// result: (VPMOVMToVec32x8 (VPCMPD256 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt64x2 x y)
	// result: (VPMOVMToVec64x2 (VPCMPQ128 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt64x4 x y)
	// result: (VPMOVMToVec64x4 (VPCMPQ256 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPQ512 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt8x16 x y)
	// result: (VPMOVMToVec8x16 (VPCMPB128 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt8x32 x y)
	// result: (VPMOVMToVec8x32 (VPCMPB256 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualInt8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualInt8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPB512 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint16x16 x y)
	// result: (VPMOVMToVec16x16 (VPCMPUW256 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPUW512 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint16x8 x y)
	// result: (VPMOVMToVec16x8 (VPCMPUW128 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPUD512 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint32x4 x y)
	// result: (VPMOVMToVec32x4 (VPCMPUD128 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint32x8 x y)
	// result: (VPMOVMToVec32x8 (VPCMPUD256 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint64x2 x y)
	// result: (VPMOVMToVec64x2 (VPCMPUQ128 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint64x4 x y)
	// result: (VPMOVMToVec64x4 (VPCMPUQ256 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPUQ512 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint8x16 x y)
	// result: (VPMOVMToVec8x16 (VPCMPUB128 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint8x32 x y)
	// result: (VPMOVMToVec8x32 (VPCMPUB256 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterEqualUint8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterEqualUint8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPUB512 [5] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterFloat32x16 x y)
	// result: (VPMOVMToVec32x16 (VCMPPS512 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPS512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (GreaterFloat32x4 x y)
	// result: (VCMPPS128 [6] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS128)
		v.AuxInt = int8ToAuxInt(6)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpGreaterFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (GreaterFloat32x8 x y)
	// result: (VCMPPS256 [6] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS256)
		v.AuxInt = int8ToAuxInt(6)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpGreaterFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (GreaterFloat64x2 x y)
	// result: (VCMPPD128 [6] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD128)
		v.AuxInt = int8ToAuxInt(6)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpGreaterFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (GreaterFloat64x4 x y)
	// result: (VCMPPD256 [6] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD256)
		v.AuxInt = int8ToAuxInt(6)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpGreaterFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterFloat64x8 x y)
	// result: (VPMOVMToVec64x8 (VCMPPD512 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterInt16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterInt16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPW512 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterInt32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPD512 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterInt64x2 x y)
	// result: (VPMOVMToVec64x2 (VPCMPQ128 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterInt64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPQ512 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterInt8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterInt8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPB512 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint16x16 x y)
	// result: (VPMOVMToVec16x16 (VPCMPUW256 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPUW512 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint16x8 x y)
	// result: (VPMOVMToVec16x8 (VPCMPUW128 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPUD512 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint32x4 x y)
	// result: (VPMOVMToVec32x4 (VPCMPUD128 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint32x8 x y)
	// result: (VPMOVMToVec32x8 (VPCMPUD256 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint64x2 x y)
	// result: (VPMOVMToVec64x2 (VPCMPUQ128 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint64x4 x y)
	// result: (VPMOVMToVec64x4 (VPCMPUQ256 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPUQ512 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint8x16 x y)
	// result: (VPMOVMToVec8x16 (VPCMPUB128 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint8x32 x y)
	// result: (VPMOVMToVec8x32 (VPCMPUB256 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpGreaterUint8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (GreaterUint8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPUB512 [6] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpHasCPUFeature(v *Value) bool {
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (HasCPUFeature {s})
	// result: (SETNE (CMPLconst [0] (LoweredHasCPUFeature {s})))
	for {
		s := auxToSym(v.Aux)
		v.reset(OpAMD64SETNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v0.AuxInt = int32ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64LoweredHasCPUFeature, typ.UInt64)
		v1.Aux = symToAux(s)
		v0.AddArg(v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpIsInBounds(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (IsInBounds idx len)
	// result: (SETB (CMPQ idx len))
	for {
		idx := v_0
		len := v_1
		v.reset(OpAMD64SETB)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v0.AddArg2(idx, len)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpIsNanFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (IsNanFloat32x16 x y)
	// result: (VPMOVMToVec32x16 (VCMPPS512 [3] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPS512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(3)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpIsNanFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (IsNanFloat32x4 x y)
	// result: (VCMPPS128 [3] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS128)
		v.AuxInt = int8ToAuxInt(3)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpIsNanFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (IsNanFloat32x8 x y)
	// result: (VCMPPS256 [3] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS256)
		v.AuxInt = int8ToAuxInt(3)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpIsNanFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (IsNanFloat64x2 x y)
	// result: (VCMPPD128 [3] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD128)
		v.AuxInt = int8ToAuxInt(3)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpIsNanFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (IsNanFloat64x4 x y)
	// result: (VCMPPD256 [3] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD256)
		v.AuxInt = int8ToAuxInt(3)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpIsNanFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (IsNanFloat64x8 x y)
	// result: (VPMOVMToVec64x8 (VCMPPD512 [3] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(3)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpIsNonNil(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (IsNonNil p)
	// result: (SETNE (TESTQ p p))
	for {
		p := v_0
		v.reset(OpAMD64SETNE)
		v0 := b.NewValue0(v.Pos, OpAMD64TESTQ, types.TypeFlags)
		v0.AddArg2(p, p)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpIsSliceInBounds(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (IsSliceInBounds idx len)
	// result: (SETBE (CMPQ idx len))
	for {
		idx := v_0
		len := v_1
		v.reset(OpAMD64SETBE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v0.AddArg2(idx, len)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLeq16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Leq16 x y)
	// result: (SETLE (CMPW x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETLE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPW, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLeq16U(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Leq16U x y)
	// result: (SETBE (CMPW x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETBE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPW, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLeq32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Leq32 x y)
	// result: (SETLE (CMPL x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETLE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPL, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLeq32F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Leq32F x y)
	// result: (SETGEF (UCOMISS y x))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETGEF)
		v0 := b.NewValue0(v.Pos, OpAMD64UCOMISS, types.TypeFlags)
		v0.AddArg2(y, x)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLeq32U(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Leq32U x y)
	// result: (SETBE (CMPL x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETBE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPL, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLeq64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Leq64 x y)
	// result: (SETLE (CMPQ x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETLE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLeq64F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Leq64F x y)
	// result: (SETGEF (UCOMISD y x))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETGEF)
		v0 := b.NewValue0(v.Pos, OpAMD64UCOMISD, types.TypeFlags)
		v0.AddArg2(y, x)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLeq64U(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Leq64U x y)
	// result: (SETBE (CMPQ x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETBE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLeq8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Leq8 x y)
	// result: (SETLE (CMPB x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETLE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPB, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLeq8U(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Leq8U x y)
	// result: (SETBE (CMPB x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETBE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPB, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLess16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Less16 x y)
	// result: (SETL (CMPW x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETL)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPW, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLess16U(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Less16U x y)
	// result: (SETB (CMPW x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETB)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPW, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLess32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Less32 x y)
	// result: (SETL (CMPL x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETL)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPL, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLess32F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Less32F x y)
	// result: (SETGF (UCOMISS y x))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETGF)
		v0 := b.NewValue0(v.Pos, OpAMD64UCOMISS, types.TypeFlags)
		v0.AddArg2(y, x)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLess32U(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Less32U x y)
	// result: (SETB (CMPL x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETB)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPL, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLess64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Less64 x y)
	// result: (SETL (CMPQ x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETL)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLess64F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Less64F x y)
	// result: (SETGF (UCOMISD y x))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETGF)
		v0 := b.NewValue0(v.Pos, OpAMD64UCOMISD, types.TypeFlags)
		v0.AddArg2(y, x)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLess64U(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Less64U x y)
	// result: (SETB (CMPQ x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETB)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLess8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Less8 x y)
	// result: (SETL (CMPB x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETL)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPB, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLess8U(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Less8U x y)
	// result: (SETB (CMPB x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETB)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPB, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualFloat32x16 x y)
	// result: (VPMOVMToVec32x16 (VCMPPS512 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPS512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LessEqualFloat32x4 x y)
	// result: (VCMPPS128 [2] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS128)
		v.AuxInt = int8ToAuxInt(2)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LessEqualFloat32x8 x y)
	// result: (VCMPPS256 [2] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS256)
		v.AuxInt = int8ToAuxInt(2)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LessEqualFloat64x2 x y)
	// result: (VCMPPD128 [2] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD128)
		v.AuxInt = int8ToAuxInt(2)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LessEqualFloat64x4 x y)
	// result: (VCMPPD256 [2] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD256)
		v.AuxInt = int8ToAuxInt(2)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualFloat64x8 x y)
	// result: (VPMOVMToVec64x8 (VCMPPD512 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt16x16 x y)
	// result: (VPMOVMToVec16x16 (VPCMPW256 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPW512 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt16x8 x y)
	// result: (VPMOVMToVec16x8 (VPCMPW128 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPD512 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt32x4 x y)
	// result: (VPMOVMToVec32x4 (VPCMPD128 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt32x8 x y)
	// result: (VPMOVMToVec32x8 (VPCMPD256 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt64x2 x y)
	// result: (VPMOVMToVec64x2 (VPCMPQ128 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt64x4 x y)
	// result: (VPMOVMToVec64x4 (VPCMPQ256 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPQ512 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt8x16 x y)
	// result: (VPMOVMToVec8x16 (VPCMPB128 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt8x32 x y)
	// result: (VPMOVMToVec8x32 (VPCMPB256 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualInt8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualInt8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPB512 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint16x16 x y)
	// result: (VPMOVMToVec16x16 (VPCMPUW256 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPUW512 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint16x8 x y)
	// result: (VPMOVMToVec16x8 (VPCMPUW128 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPUD512 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint32x4 x y)
	// result: (VPMOVMToVec32x4 (VPCMPUD128 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint32x8 x y)
	// result: (VPMOVMToVec32x8 (VPCMPUD256 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint64x2 x y)
	// result: (VPMOVMToVec64x2 (VPCMPUQ128 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint64x4 x y)
	// result: (VPMOVMToVec64x4 (VPCMPUQ256 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPUQ512 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint8x16 x y)
	// result: (VPMOVMToVec8x16 (VPCMPUB128 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint8x32 x y)
	// result: (VPMOVMToVec8x32 (VPCMPUB256 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessEqualUint8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessEqualUint8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPUB512 [2] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessFloat32x16 x y)
	// result: (VPMOVMToVec32x16 (VCMPPS512 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPS512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LessFloat32x4 x y)
	// result: (VCMPPS128 [1] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS128)
		v.AuxInt = int8ToAuxInt(1)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpLessFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LessFloat32x8 x y)
	// result: (VCMPPS256 [1] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS256)
		v.AuxInt = int8ToAuxInt(1)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpLessFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LessFloat64x2 x y)
	// result: (VCMPPD128 [1] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD128)
		v.AuxInt = int8ToAuxInt(1)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpLessFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (LessFloat64x4 x y)
	// result: (VCMPPD256 [1] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD256)
		v.AuxInt = int8ToAuxInt(1)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpLessFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessFloat64x8 x y)
	// result: (VPMOVMToVec64x8 (VCMPPD512 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt16x16 x y)
	// result: (VPMOVMToVec16x16 (VPCMPW256 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPW512 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt16x8 x y)
	// result: (VPMOVMToVec16x8 (VPCMPW128 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPD512 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt32x4 x y)
	// result: (VPMOVMToVec32x4 (VPCMPD128 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt32x8 x y)
	// result: (VPMOVMToVec32x8 (VPCMPD256 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt64x2 x y)
	// result: (VPMOVMToVec64x2 (VPCMPQ128 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt64x4 x y)
	// result: (VPMOVMToVec64x4 (VPCMPQ256 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPQ512 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt8x16 x y)
	// result: (VPMOVMToVec8x16 (VPCMPB128 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt8x32 x y)
	// result: (VPMOVMToVec8x32 (VPCMPB256 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessInt8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessInt8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPB512 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint16x16 x y)
	// result: (VPMOVMToVec16x16 (VPCMPUW256 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPUW512 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint16x8 x y)
	// result: (VPMOVMToVec16x8 (VPCMPUW128 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPUD512 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint32x4 x y)
	// result: (VPMOVMToVec32x4 (VPCMPUD128 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint32x8 x y)
	// result: (VPMOVMToVec32x8 (VPCMPUD256 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint64x2 x y)
	// result: (VPMOVMToVec64x2 (VPCMPUQ128 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint64x4 x y)
	// result: (VPMOVMToVec64x4 (VPCMPUQ256 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPUQ512 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint8x16 x y)
	// result: (VPMOVMToVec8x16 (VPCMPUB128 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint8x32 x y)
	// result: (VPMOVMToVec8x32 (VPCMPUB256 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLessUint8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LessUint8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPUB512 [1] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpLoad(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (Load <t> ptr mem)
	// cond: (is64BitInt(t) || isPtr(t))
	// result: (MOVQload ptr mem)
	for {
		t := v.Type
		ptr := v_0
		mem := v_1
		if !(is64BitInt(t) || isPtr(t)) {
			break
		}
		v.reset(OpAMD64MOVQload)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (Load <t> ptr mem)
	// cond: is32BitInt(t)
	// result: (MOVLload ptr mem)
	for {
		t := v.Type
		ptr := v_0
		mem := v_1
		if !(is32BitInt(t)) {
			break
		}
		v.reset(OpAMD64MOVLload)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (Load <t> ptr mem)
	// cond: is16BitInt(t)
	// result: (MOVWload ptr mem)
	for {
		t := v.Type
		ptr := v_0
		mem := v_1
		if !(is16BitInt(t)) {
			break
		}
		v.reset(OpAMD64MOVWload)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (Load <t> ptr mem)
	// cond: (t.IsBoolean() || is8BitInt(t))
	// result: (MOVBload ptr mem)
	for {
		t := v.Type
		ptr := v_0
		mem := v_1
		if !(t.IsBoolean() || is8BitInt(t)) {
			break
		}
		v.reset(OpAMD64MOVBload)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (Load <t> ptr mem)
	// cond: is32BitFloat(t)
	// result: (MOVSSload ptr mem)
	for {
		t := v.Type
		ptr := v_0
		mem := v_1
		if !(is32BitFloat(t)) {
			break
		}
		v.reset(OpAMD64MOVSSload)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (Load <t> ptr mem)
	// cond: is64BitFloat(t)
	// result: (MOVSDload ptr mem)
	for {
		t := v.Type
		ptr := v_0
		mem := v_1
		if !(is64BitFloat(t)) {
			break
		}
		v.reset(OpAMD64MOVSDload)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (Load <t> ptr mem)
	// cond: t.Size() == 16
	// result: (VMOVDQUload128 ptr mem)
	for {
		t := v.Type
		ptr := v_0
		mem := v_1
		if !(t.Size() == 16) {
			break
		}
		v.reset(OpAMD64VMOVDQUload128)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (Load <t> ptr mem)
	// cond: t.Size() == 32
	// result: (VMOVDQUload256 ptr mem)
	for {
		t := v.Type
		ptr := v_0
		mem := v_1
		if !(t.Size() == 32) {
			break
		}
		v.reset(OpAMD64VMOVDQUload256)
		v.AddArg2(ptr, mem)
		return true
	}
	// match: (Load <t> ptr mem)
	// cond: t.Size() == 64
	// result: (VMOVDQUload512 ptr mem)
	for {
		t := v.Type
		ptr := v_0
		mem := v_1
		if !(t.Size() == 64) {
			break
		}
		v.reset(OpAMD64VMOVDQUload512)
		v.AddArg2(ptr, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLocalAddr(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (LocalAddr <t> {sym} base mem)
	// cond: t.Elem().HasPointers()
	// result: (LEAQ {sym} (SPanchored base mem))
	for {
		t := v.Type
		sym := auxToSym(v.Aux)
		base := v_0
		mem := v_1
		if !(t.Elem().HasPointers()) {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.Aux = symToAux(sym)
		v0 := b.NewValue0(v.Pos, OpSPanchored, typ.Uintptr)
		v0.AddArg2(base, mem)
		v.AddArg(v0)
		return true
	}
	// match: (LocalAddr <t> {sym} base _)
	// cond: !t.Elem().HasPointers()
	// result: (LEAQ {sym} base)
	for {
		t := v.Type
		sym := auxToSym(v.Aux)
		base := v_0
		if !(!t.Elem().HasPointers()) {
			break
		}
		v.reset(OpAMD64LEAQ)
		v.Aux = symToAux(sym)
		v.AddArg(base)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh16x16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPWconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v2.AuxInt = int16ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh16x16 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh16x32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPLconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh16x32 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh16x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh16x64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPQconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh16x64 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh16x8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPBconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v2.AuxInt = int8ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh16x8 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh32x16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPWconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v2.AuxInt = int16ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh32x16 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh32x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh32x32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPLconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh32x32 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh32x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh32x64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPQconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh32x64 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh32x8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPBconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v2.AuxInt = int8ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh32x8 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh64x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh64x16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDQ (SHLQ <t> x y) (SBBQcarrymask <t> (CMPWconst y [64])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDQ)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLQ, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v2.AuxInt = int16ToAuxInt(64)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh64x16 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh64x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh64x32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDQ (SHLQ <t> x y) (SBBQcarrymask <t> (CMPLconst y [64])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDQ)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLQ, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(64)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh64x32 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh64x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh64x64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDQ (SHLQ <t> x y) (SBBQcarrymask <t> (CMPQconst y [64])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDQ)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLQ, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(64)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh64x64 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh64x8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDQ (SHLQ <t> x y) (SBBQcarrymask <t> (CMPBconst y [64])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDQ)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLQ, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v2.AuxInt = int8ToAuxInt(64)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh64x8 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh8x16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPWconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v2.AuxInt = int16ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh8x16 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh8x32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPLconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh8x32 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh8x64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPQconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh8x64 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpLsh8x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Lsh8x8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHLL <t> x y) (SBBLcarrymask <t> (CMPBconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHLL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v2.AuxInt = int8ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Lsh8x8 x y)
	// cond: shiftIsBounded(v)
	// result: (SHLL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHLL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpMaskedAbsoluteInt16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt16x16 x mask)
	// result: (VPABSWMasked256 x (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAbsoluteInt16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt16x32 x mask)
	// result: (VPABSWMasked512 x (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAbsoluteInt16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt16x8 x mask)
	// result: (VPABSWMasked128 x (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAbsoluteInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt32x16 x mask)
	// result: (VPABSDMasked512 x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAbsoluteInt32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt32x4 x mask)
	// result: (VPABSDMasked128 x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAbsoluteInt32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt32x8 x mask)
	// result: (VPABSDMasked256 x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAbsoluteInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt64x2 x mask)
	// result: (VPABSQMasked128 x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAbsoluteInt64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt64x4 x mask)
	// result: (VPABSQMasked256 x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAbsoluteInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt64x8 x mask)
	// result: (VPABSQMasked512 x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAbsoluteInt8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt8x16 x mask)
	// result: (VPABSBMasked128 x (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAbsoluteInt8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt8x32 x mask)
	// result: (VPABSBMasked256 x (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAbsoluteInt8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAbsoluteInt8x64 x mask)
	// result: (VPABSBMasked512 x (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPABSBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddFloat32x16 x y mask)
	// result: (VADDPSMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VADDPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddFloat32x4 x y mask)
	// result: (VADDPSMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VADDPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddFloat32x8 x y mask)
	// result: (VADDPSMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VADDPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddFloat64x2 x y mask)
	// result: (VADDPDMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VADDPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddFloat64x4 x y mask)
	// result: (VADDPDMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VADDPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddFloat64x8 x y mask)
	// result: (VADDPDMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VADDPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt16x16 x y mask)
	// result: (VPADDWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt16x32 x y mask)
	// result: (VPADDWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt16x8 x y mask)
	// result: (VPADDWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt32x16 x y mask)
	// result: (VPADDDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt32x4 x y mask)
	// result: (VPADDDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt32x8 x y mask)
	// result: (VPADDDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt64x2 x y mask)
	// result: (VPADDQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt64x4 x y mask)
	// result: (VPADDQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt64x8 x y mask)
	// result: (VPADDQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt8x16 x y mask)
	// result: (VPADDBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt8x32 x y mask)
	// result: (VPADDBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddInt8x64 x y mask)
	// result: (VPADDBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint16x16 x y mask)
	// result: (VPADDWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint16x32 x y mask)
	// result: (VPADDWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint16x8 x y mask)
	// result: (VPADDWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint32x16 x y mask)
	// result: (VPADDDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint32x4 x y mask)
	// result: (VPADDDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint32x8 x y mask)
	// result: (VPADDDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint64x2 x y mask)
	// result: (VPADDQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint64x4 x y mask)
	// result: (VPADDQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint64x8 x y mask)
	// result: (VPADDQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint8x16 x y mask)
	// result: (VPADDBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint8x32 x y mask)
	// result: (VPADDBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAddUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAddUint8x64 x y mask)
	// result: (VPADDBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndFloat32x16 x y mask)
	// result: (VANDPSMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndFloat32x4 x y mask)
	// result: (VANDPSMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndFloat32x8 x y mask)
	// result: (VANDPSMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndFloat64x2 x y mask)
	// result: (VANDPDMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndFloat64x4 x y mask)
	// result: (VANDPDMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndFloat64x8 x y mask)
	// result: (VANDPDMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndInt32x16 x y mask)
	// result: (VPANDDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndInt32x4 x y mask)
	// result: (VPANDDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndInt32x8 x y mask)
	// result: (VPANDDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndInt64x2 x y mask)
	// result: (VPANDQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndInt64x4 x y mask)
	// result: (VPANDQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndInt64x8 x y mask)
	// result: (VPANDQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotFloat32x16 x y mask)
	// result: (VANDNPSMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDNPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotFloat32x4 x y mask)
	// result: (VANDNPSMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDNPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotFloat32x8 x y mask)
	// result: (VANDNPSMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDNPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotFloat64x2 x y mask)
	// result: (VANDNPDMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDNPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotFloat64x4 x y mask)
	// result: (VANDNPDMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDNPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotFloat64x8 x y mask)
	// result: (VANDNPDMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VANDNPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotInt32x16 x y mask)
	// result: (VPANDNDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotInt32x4 x y mask)
	// result: (VPANDNDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotInt32x8 x y mask)
	// result: (VPANDNDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotInt64x2 x y mask)
	// result: (VPANDNQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotInt64x4 x y mask)
	// result: (VPANDNQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotInt64x8 x y mask)
	// result: (VPANDNQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotUint32x16 x y mask)
	// result: (VPANDNDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotUint32x4 x y mask)
	// result: (VPANDNDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotUint32x8 x y mask)
	// result: (VPANDNDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotUint64x2 x y mask)
	// result: (VPANDNQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotUint64x4 x y mask)
	// result: (VPANDNQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndNotUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndNotUint64x8 x y mask)
	// result: (VPANDNQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDNQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndUint32x16 x y mask)
	// result: (VPANDDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndUint32x4 x y mask)
	// result: (VPANDDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndUint32x8 x y mask)
	// result: (VPANDDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndUint64x2 x y mask)
	// result: (VPANDQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndUint64x4 x y mask)
	// result: (VPANDQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAndUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAndUint64x8 x y mask)
	// result: (VPANDQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPANDQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalFloat32x16 x mask)
	// result: (VRCP14PSMasked512 x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRCP14PSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalFloat32x4 x mask)
	// result: (VRCP14PSMasked128 x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRCP14PSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalFloat32x8 x mask)
	// result: (VRCP14PSMasked256 x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRCP14PSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalFloat64x2 x mask)
	// result: (VRCP14PDMasked128 x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRCP14PDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalFloat64x4 x mask)
	// result: (VRCP14PDMasked256 x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRCP14PDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalFloat64x8 x mask)
	// result: (VRCP14PDMasked512 x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRCP14PDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalOfSqrtFloat32x16 x mask)
	// result: (VRSQRT14PSMasked512 x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRSQRT14PSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalOfSqrtFloat32x4 x mask)
	// result: (VRSQRT14PSMasked128 x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRSQRT14PSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalOfSqrtFloat32x8 x mask)
	// result: (VRSQRT14PSMasked256 x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRSQRT14PSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalOfSqrtFloat64x2 x mask)
	// result: (VRSQRT14PDMasked128 x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRSQRT14PDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalOfSqrtFloat64x4 x mask)
	// result: (VRSQRT14PDMasked256 x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRSQRT14PDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedApproximateReciprocalOfSqrtFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedApproximateReciprocalOfSqrtFloat64x8 x mask)
	// result: (VRSQRT14PDMasked512 x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRSQRT14PDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAverageUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAverageUint16x16 x y mask)
	// result: (VPAVGWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPAVGWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAverageUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAverageUint16x32 x y mask)
	// result: (VPAVGWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPAVGWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAverageUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAverageUint16x8 x y mask)
	// result: (VPAVGWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPAVGWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAverageUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAverageUint8x16 x y mask)
	// result: (VPAVGBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPAVGBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAverageUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAverageUint8x32 x y mask)
	// result: (VPAVGBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPAVGBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedAverageUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedAverageUint8x64 x y mask)
	// result: (VPAVGBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPAVGBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilSuppressExceptionWithPrecisionFloat32x16 [a] x mask)
	// result: (VRNDSCALEPSMasked512 [a+10] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilSuppressExceptionWithPrecisionFloat32x4 [a] x mask)
	// result: (VRNDSCALEPSMasked128 [a+10] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilSuppressExceptionWithPrecisionFloat32x8 [a] x mask)
	// result: (VRNDSCALEPSMasked256 [a+10] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilSuppressExceptionWithPrecisionFloat64x2 [a] x mask)
	// result: (VRNDSCALEPDMasked128 [a+10] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilSuppressExceptionWithPrecisionFloat64x4 [a] x mask)
	// result: (VRNDSCALEPDMasked256 [a+10] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilSuppressExceptionWithPrecisionFloat64x8 [a] x mask)
	// result: (VRNDSCALEPDMasked512 [a+10] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilWithPrecisionFloat32x16 [a] x mask)
	// result: (VRNDSCALEPSMasked512 [a+2] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilWithPrecisionFloat32x4 [a] x mask)
	// result: (VRNDSCALEPSMasked128 [a+2] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilWithPrecisionFloat32x8 [a] x mask)
	// result: (VRNDSCALEPSMasked256 [a+2] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilWithPrecisionFloat64x2 [a] x mask)
	// result: (VRNDSCALEPDMasked128 [a+2] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilWithPrecisionFloat64x4 [a] x mask)
	// result: (VRNDSCALEPDMasked256 [a+2] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedCeilWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedCeilWithPrecisionFloat64x8 [a] x mask)
	// result: (VRNDSCALEPDMasked512 [a+2] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x16 [a] x mask)
	// result: (VREDUCEPSMasked512 [a+10] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x4 [a] x mask)
	// result: (VREDUCEPSMasked128 [a+10] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilSuppressExceptionWithPrecisionFloat32x8 [a] x mask)
	// result: (VREDUCEPSMasked256 [a+10] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x2 [a] x mask)
	// result: (VREDUCEPDMasked128 [a+10] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x4 [a] x mask)
	// result: (VREDUCEPDMasked256 [a+10] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilSuppressExceptionWithPrecisionFloat64x8 [a] x mask)
	// result: (VREDUCEPDMasked512 [a+10] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 10)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilWithPrecisionFloat32x16 [a] x mask)
	// result: (VREDUCEPSMasked512 [a+2] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilWithPrecisionFloat32x4 [a] x mask)
	// result: (VREDUCEPSMasked128 [a+2] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilWithPrecisionFloat32x8 [a] x mask)
	// result: (VREDUCEPSMasked256 [a+2] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilWithPrecisionFloat64x2 [a] x mask)
	// result: (VREDUCEPDMasked128 [a+2] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilWithPrecisionFloat64x4 [a] x mask)
	// result: (VREDUCEPDMasked256 [a+2] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithCeilWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithCeilWithPrecisionFloat64x8 [a] x mask)
	// result: (VREDUCEPDMasked512 [a+2] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x16 [a] x mask)
	// result: (VREDUCEPSMasked512 [a+9] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x4 [a] x mask)
	// result: (VREDUCEPSMasked128 [a+9] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorSuppressExceptionWithPrecisionFloat32x8 [a] x mask)
	// result: (VREDUCEPSMasked256 [a+9] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x2 [a] x mask)
	// result: (VREDUCEPDMasked128 [a+9] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x4 [a] x mask)
	// result: (VREDUCEPDMasked256 [a+9] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorSuppressExceptionWithPrecisionFloat64x8 [a] x mask)
	// result: (VREDUCEPDMasked512 [a+9] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorWithPrecisionFloat32x16 [a] x mask)
	// result: (VREDUCEPSMasked512 [a+1] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorWithPrecisionFloat32x4 [a] x mask)
	// result: (VREDUCEPSMasked128 [a+1] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorWithPrecisionFloat32x8 [a] x mask)
	// result: (VREDUCEPSMasked256 [a+1] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorWithPrecisionFloat64x2 [a] x mask)
	// result: (VREDUCEPDMasked128 [a+1] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorWithPrecisionFloat64x4 [a] x mask)
	// result: (VREDUCEPDMasked256 [a+1] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithFloorWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithFloorWithPrecisionFloat64x8 [a] x mask)
	// result: (VREDUCEPDMasked512 [a+1] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x16 [a] x mask)
	// result: (VREDUCEPSMasked512 [a+8] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x4 [a] x mask)
	// result: (VREDUCEPSMasked128 [a+8] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundSuppressExceptionWithPrecisionFloat32x8 [a] x mask)
	// result: (VREDUCEPSMasked256 [a+8] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x2 [a] x mask)
	// result: (VREDUCEPDMasked128 [a+8] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x4 [a] x mask)
	// result: (VREDUCEPDMasked256 [a+8] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundSuppressExceptionWithPrecisionFloat64x8 [a] x mask)
	// result: (VREDUCEPDMasked512 [a+8] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundWithPrecisionFloat32x16 [a] x mask)
	// result: (VREDUCEPSMasked512 [a+0] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundWithPrecisionFloat32x4 [a] x mask)
	// result: (VREDUCEPSMasked128 [a+0] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundWithPrecisionFloat32x8 [a] x mask)
	// result: (VREDUCEPSMasked256 [a+0] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundWithPrecisionFloat64x2 [a] x mask)
	// result: (VREDUCEPDMasked128 [a+0] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundWithPrecisionFloat64x4 [a] x mask)
	// result: (VREDUCEPDMasked256 [a+0] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithRoundWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithRoundWithPrecisionFloat64x8 [a] x mask)
	// result: (VREDUCEPDMasked512 [a+0] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x16 [a] x mask)
	// result: (VREDUCEPSMasked512 [a+11] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x4 [a] x mask)
	// result: (VREDUCEPSMasked128 [a+11] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncSuppressExceptionWithPrecisionFloat32x8 [a] x mask)
	// result: (VREDUCEPSMasked256 [a+11] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x2 [a] x mask)
	// result: (VREDUCEPDMasked128 [a+11] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x4 [a] x mask)
	// result: (VREDUCEPDMasked256 [a+11] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncSuppressExceptionWithPrecisionFloat64x8 [a] x mask)
	// result: (VREDUCEPDMasked512 [a+11] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncWithPrecisionFloat32x16 [a] x mask)
	// result: (VREDUCEPSMasked512 [a+3] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncWithPrecisionFloat32x4 [a] x mask)
	// result: (VREDUCEPSMasked128 [a+3] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncWithPrecisionFloat32x8 [a] x mask)
	// result: (VREDUCEPSMasked256 [a+3] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncWithPrecisionFloat64x2 [a] x mask)
	// result: (VREDUCEPDMasked128 [a+3] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncWithPrecisionFloat64x4 [a] x mask)
	// result: (VREDUCEPDMasked256 [a+3] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDiffWithTruncWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDiffWithTruncWithPrecisionFloat64x8 [a] x mask)
	// result: (VREDUCEPDMasked512 [a+3] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VREDUCEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDivFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDivFloat32x16 x y mask)
	// result: (VDIVPSMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VDIVPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDivFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDivFloat32x4 x y mask)
	// result: (VDIVPSMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VDIVPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDivFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDivFloat32x8 x y mask)
	// result: (VDIVPSMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VDIVPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDivFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDivFloat64x2 x y mask)
	// result: (VDIVPDMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VDIVPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDivFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDivFloat64x4 x y mask)
	// result: (VDIVPDMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VDIVPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedDivFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedDivFloat64x8 x y mask)
	// result: (VDIVPDMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VDIVPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualFloat32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VCMPPSMasked512 [0] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualFloat32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VCMPPSMasked128 [0] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualFloat32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VCMPPSMasked256 [0] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualFloat64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VCMPPDMasked128 [0] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualFloat64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VCMPPDMasked256 [0] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualFloat64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VCMPPDMasked512 [0] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPWMasked256 [0] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPWMasked512 [0] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPWMasked128 [0] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPDMasked512 [0] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPDMasked128 [0] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPDMasked256 [0] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPQMasked128 [0] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPQMasked256 [0] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPQMasked512 [0] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPBMasked128 [0] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPBMasked256 [0] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualInt8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPBMasked512 [0] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPUWMasked256 [0] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPUWMasked512 [0] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPUWMasked128 [0] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPUDMasked512 [0] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPUDMasked128 [0] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPUDMasked256 [0] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPUQMasked128 [0] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPUQMasked256 [0] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPUQMasked512 [0] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPUBMasked128 [0] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPUBMasked256 [0] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedEqualUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedEqualUint8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPUBMasked512 [0] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorSuppressExceptionWithPrecisionFloat32x16 [a] x mask)
	// result: (VRNDSCALEPSMasked512 [a+9] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorSuppressExceptionWithPrecisionFloat32x4 [a] x mask)
	// result: (VRNDSCALEPSMasked128 [a+9] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorSuppressExceptionWithPrecisionFloat32x8 [a] x mask)
	// result: (VRNDSCALEPSMasked256 [a+9] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorSuppressExceptionWithPrecisionFloat64x2 [a] x mask)
	// result: (VRNDSCALEPDMasked128 [a+9] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorSuppressExceptionWithPrecisionFloat64x4 [a] x mask)
	// result: (VRNDSCALEPDMasked256 [a+9] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorSuppressExceptionWithPrecisionFloat64x8 [a] x mask)
	// result: (VRNDSCALEPDMasked512 [a+9] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 9)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorWithPrecisionFloat32x16 [a] x mask)
	// result: (VRNDSCALEPSMasked512 [a+1] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorWithPrecisionFloat32x4 [a] x mask)
	// result: (VRNDSCALEPSMasked128 [a+1] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorWithPrecisionFloat32x8 [a] x mask)
	// result: (VRNDSCALEPSMasked256 [a+1] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorWithPrecisionFloat64x2 [a] x mask)
	// result: (VRNDSCALEPDMasked128 [a+1] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorWithPrecisionFloat64x4 [a] x mask)
	// result: (VRNDSCALEPDMasked256 [a+1] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFloorWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFloorWithPrecisionFloat64x8 [a] x mask)
	// result: (VRNDSCALEPDMasked512 [a+1] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 1)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddFloat32x16 x y z mask)
	// result: (VFMADD213PSMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADD213PSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddFloat32x4 x y z mask)
	// result: (VFMADD213PSMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADD213PSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddFloat32x8 x y z mask)
	// result: (VFMADD213PSMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADD213PSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat64x2(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddFloat64x2 x y z mask)
	// result: (VFMADD213PDMasked128 x y z (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADD213PDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat64x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddFloat64x4 x y z mask)
	// result: (VFMADD213PDMasked256 x y z (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADD213PDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddFloat64x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddFloat64x8 x y z mask)
	// result: (VFMADD213PDMasked512 x y z (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADD213PDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddSubFloat32x16 x y z mask)
	// result: (VFMADDSUB213PSMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADDSUB213PSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddSubFloat32x4 x y z mask)
	// result: (VFMADDSUB213PSMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADDSUB213PSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddSubFloat32x8 x y z mask)
	// result: (VFMADDSUB213PSMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADDSUB213PSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat64x2(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddSubFloat64x2 x y z mask)
	// result: (VFMADDSUB213PDMasked128 x y z (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADDSUB213PDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat64x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddSubFloat64x4 x y z mask)
	// result: (VFMADDSUB213PDMasked256 x y z (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADDSUB213PDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplyAddSubFloat64x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplyAddSubFloat64x8 x y z mask)
	// result: (VFMADDSUB213PDMasked512 x y z (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMADDSUB213PDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplySubAddFloat32x16 x y z mask)
	// result: (VFMSUBADD213PSMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMSUBADD213PSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplySubAddFloat32x4 x y z mask)
	// result: (VFMSUBADD213PSMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMSUBADD213PSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplySubAddFloat32x8 x y z mask)
	// result: (VFMSUBADD213PSMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMSUBADD213PSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat64x2(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplySubAddFloat64x2 x y z mask)
	// result: (VFMSUBADD213PDMasked128 x y z (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMSUBADD213PDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat64x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplySubAddFloat64x4 x y z mask)
	// result: (VFMSUBADD213PDMasked256 x y z (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMSUBADD213PDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedFusedMultiplySubAddFloat64x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedFusedMultiplySubAddFloat64x8 x y z mask)
	// result: (VFMSUBADD213PDMasked512 x y z (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VFMSUBADD213PDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualFloat32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VCMPPSMasked512 [5] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualFloat32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VCMPPSMasked128 [5] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualFloat32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VCMPPSMasked256 [5] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualFloat64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VCMPPDMasked128 [5] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualFloat64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VCMPPDMasked256 [5] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualFloat64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VCMPPDMasked512 [5] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPWMasked256 [5] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPWMasked512 [5] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPWMasked128 [5] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPDMasked512 [5] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPDMasked128 [5] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPDMasked256 [5] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPQMasked128 [5] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPQMasked256 [5] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPQMasked512 [5] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPBMasked128 [5] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPBMasked256 [5] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualInt8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPBMasked512 [5] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPUWMasked256 [5] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPUWMasked512 [5] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPUWMasked128 [5] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPUDMasked512 [5] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPUDMasked128 [5] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPUDMasked256 [5] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPUQMasked128 [5] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPUQMasked256 [5] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPUQMasked512 [5] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPUBMasked128 [5] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPUBMasked256 [5] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterEqualUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterEqualUint8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPUBMasked512 [5] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(5)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterFloat32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VCMPPSMasked512 [6] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterFloat32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VCMPPSMasked128 [6] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterFloat32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VCMPPSMasked256 [6] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterFloat64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VCMPPDMasked128 [6] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterFloat64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VCMPPDMasked256 [6] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterFloat64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VCMPPDMasked512 [6] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPWMasked256 [6] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPWMasked512 [6] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPWMasked128 [6] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPDMasked512 [6] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPDMasked128 [6] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPDMasked256 [6] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPQMasked128 [6] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPQMasked256 [6] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPQMasked512 [6] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPBMasked128 [6] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPBMasked256 [6] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterInt8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPBMasked512 [6] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPUWMasked256 [6] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPUWMasked512 [6] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPUWMasked128 [6] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPUDMasked512 [6] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPUDMasked128 [6] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPUDMasked256 [6] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPUQMasked128 [6] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPUQMasked256 [6] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPUQMasked512 [6] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPUBMasked128 [6] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPUBMasked256 [6] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedGreaterUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedGreaterUint8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPUBMasked512 [6] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(6)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedIsNanFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedIsNanFloat32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VCMPPSMasked512 [3] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(3)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedIsNanFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedIsNanFloat32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VCMPPSMasked128 [3] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(3)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedIsNanFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedIsNanFloat32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VCMPPSMasked256 [3] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(3)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedIsNanFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedIsNanFloat64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VCMPPDMasked128 [3] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(3)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedIsNanFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedIsNanFloat64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VCMPPDMasked256 [3] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(3)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedIsNanFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedIsNanFloat64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VCMPPDMasked512 [3] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(3)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualFloat32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VCMPPSMasked512 [2] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualFloat32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VCMPPSMasked128 [2] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualFloat32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VCMPPSMasked256 [2] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualFloat64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VCMPPDMasked128 [2] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualFloat64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VCMPPDMasked256 [2] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualFloat64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VCMPPDMasked512 [2] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPWMasked256 [2] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPWMasked512 [2] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPWMasked128 [2] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPDMasked512 [2] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPDMasked128 [2] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPDMasked256 [2] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPQMasked128 [2] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPQMasked256 [2] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPQMasked512 [2] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPBMasked128 [2] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPBMasked256 [2] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualInt8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPBMasked512 [2] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPUWMasked256 [2] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPUWMasked512 [2] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPUWMasked128 [2] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPUDMasked512 [2] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPUDMasked128 [2] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPUDMasked256 [2] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPUQMasked128 [2] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPUQMasked256 [2] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPUQMasked512 [2] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPUBMasked128 [2] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPUBMasked256 [2] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessEqualUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessEqualUint8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPUBMasked512 [2] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(2)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessFloat32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VCMPPSMasked512 [1] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessFloat32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VCMPPSMasked128 [1] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessFloat32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VCMPPSMasked256 [1] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessFloat64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VCMPPDMasked128 [1] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessFloat64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VCMPPDMasked256 [1] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessFloat64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VCMPPDMasked512 [1] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPWMasked256 [1] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPWMasked512 [1] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPWMasked128 [1] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPDMasked512 [1] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPDMasked128 [1] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPDMasked256 [1] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPQMasked128 [1] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPQMasked256 [1] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPQMasked512 [1] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPBMasked128 [1] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPBMasked256 [1] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessInt8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPBMasked512 [1] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPUWMasked256 [1] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPUWMasked512 [1] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPUWMasked128 [1] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPUDMasked512 [1] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPUDMasked128 [1] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPUDMasked256 [1] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPUQMasked128 [1] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPUQMasked256 [1] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPUQMasked512 [1] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPUBMasked128 [1] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPUBMasked256 [1] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedLessUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedLessUint8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPUBMasked512 [1] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(1)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxFloat32x16 x y mask)
	// result: (VMAXPSMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMAXPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxFloat32x4 x y mask)
	// result: (VMAXPSMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMAXPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxFloat32x8 x y mask)
	// result: (VMAXPSMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMAXPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxFloat64x2 x y mask)
	// result: (VMAXPDMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMAXPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxFloat64x4 x y mask)
	// result: (VMAXPDMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMAXPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxFloat64x8 x y mask)
	// result: (VMAXPDMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMAXPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt16x16 x y mask)
	// result: (VPMAXSWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt16x32 x y mask)
	// result: (VPMAXSWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt16x8 x y mask)
	// result: (VPMAXSWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt32x16 x y mask)
	// result: (VPMAXSDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt32x4 x y mask)
	// result: (VPMAXSDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt32x8 x y mask)
	// result: (VPMAXSDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt64x2 x y mask)
	// result: (VPMAXSQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt64x4 x y mask)
	// result: (VPMAXSQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt64x8 x y mask)
	// result: (VPMAXSQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt8x16 x y mask)
	// result: (VPMAXSBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt8x32 x y mask)
	// result: (VPMAXSBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxInt8x64 x y mask)
	// result: (VPMAXSBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXSBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint16x16 x y mask)
	// result: (VPMAXUWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint16x32 x y mask)
	// result: (VPMAXUWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint16x8 x y mask)
	// result: (VPMAXUWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint32x16 x y mask)
	// result: (VPMAXUDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint32x4 x y mask)
	// result: (VPMAXUDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint32x8 x y mask)
	// result: (VPMAXUDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint64x2 x y mask)
	// result: (VPMAXUQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint64x4 x y mask)
	// result: (VPMAXUQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint64x8 x y mask)
	// result: (VPMAXUQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint8x16 x y mask)
	// result: (VPMAXUBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint8x32 x y mask)
	// result: (VPMAXUBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMaxUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMaxUint8x64 x y mask)
	// result: (VPMAXUBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMAXUBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinFloat32x16 x y mask)
	// result: (VMINPSMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMINPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinFloat32x4 x y mask)
	// result: (VMINPSMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMINPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinFloat32x8 x y mask)
	// result: (VMINPSMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMINPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinFloat64x2 x y mask)
	// result: (VMINPDMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMINPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinFloat64x4 x y mask)
	// result: (VMINPDMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMINPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinFloat64x8 x y mask)
	// result: (VMINPDMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMINPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt16x16 x y mask)
	// result: (VPMINSWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt16x32 x y mask)
	// result: (VPMINSWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt16x8 x y mask)
	// result: (VPMINSWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt32x16 x y mask)
	// result: (VPMINSDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt32x4 x y mask)
	// result: (VPMINSDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt32x8 x y mask)
	// result: (VPMINSDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt64x2 x y mask)
	// result: (VPMINSQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt64x4 x y mask)
	// result: (VPMINSQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt64x8 x y mask)
	// result: (VPMINSQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt8x16 x y mask)
	// result: (VPMINSBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt8x32 x y mask)
	// result: (VPMINSBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinInt8x64 x y mask)
	// result: (VPMINSBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINSBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint16x16 x y mask)
	// result: (VPMINUWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint16x32 x y mask)
	// result: (VPMINUWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint16x8 x y mask)
	// result: (VPMINUWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint32x16 x y mask)
	// result: (VPMINUDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint32x4 x y mask)
	// result: (VPMINUDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint32x8 x y mask)
	// result: (VPMINUDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint64x2 x y mask)
	// result: (VPMINUQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint64x4 x y mask)
	// result: (VPMINUQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint64x8 x y mask)
	// result: (VPMINUQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint8x16 x y mask)
	// result: (VPMINUBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint8x32 x y mask)
	// result: (VPMINUBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMinUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMinUint8x64 x y mask)
	// result: (VPMINUBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMINUBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulByPowOf2Float32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulByPowOf2Float32x16 x y mask)
	// result: (VSCALEFPSMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSCALEFPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulByPowOf2Float32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulByPowOf2Float32x4 x y mask)
	// result: (VSCALEFPSMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSCALEFPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulByPowOf2Float32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulByPowOf2Float32x8 x y mask)
	// result: (VSCALEFPSMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSCALEFPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulByPowOf2Float64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulByPowOf2Float64x2 x y mask)
	// result: (VSCALEFPDMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSCALEFPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulByPowOf2Float64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulByPowOf2Float64x4 x y mask)
	// result: (VSCALEFPDMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSCALEFPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulByPowOf2Float64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulByPowOf2Float64x8 x y mask)
	// result: (VSCALEFPDMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSCALEFPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulEvenWidenInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulEvenWidenInt64x2 x y mask)
	// result: (VPMULDQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULDQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulEvenWidenInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulEvenWidenInt64x4 x y mask)
	// result: (VPMULDQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULDQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulEvenWidenInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulEvenWidenInt64x8 x y mask)
	// result: (VPMULDQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULDQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulEvenWidenUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulEvenWidenUint64x2 x y mask)
	// result: (VPMULUDQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULUDQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulEvenWidenUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulEvenWidenUint64x4 x y mask)
	// result: (VPMULUDQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULUDQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulEvenWidenUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulEvenWidenUint64x8 x y mask)
	// result: (VPMULUDQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULUDQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulFloat32x16 x y mask)
	// result: (VMULPSMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMULPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulFloat32x4 x y mask)
	// result: (VMULPSMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMULPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulFloat32x8 x y mask)
	// result: (VMULPSMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMULPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulFloat64x2 x y mask)
	// result: (VMULPDMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMULPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulFloat64x4 x y mask)
	// result: (VMULPDMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMULPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulFloat64x8 x y mask)
	// result: (VMULPDMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VMULPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulHighInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulHighInt16x16 x y mask)
	// result: (VPMULHWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULHWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulHighInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulHighInt16x32 x y mask)
	// result: (VPMULHWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULHWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulHighInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulHighInt16x8 x y mask)
	// result: (VPMULHWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULHWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulHighUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulHighUint16x16 x y mask)
	// result: (VPMULHUWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULHUWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulHighUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulHighUint16x32 x y mask)
	// result: (VPMULHUWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULHUWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulHighUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulHighUint16x8 x y mask)
	// result: (VPMULHUWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULHUWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulLowInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulLowInt16x16 x y mask)
	// result: (VPMULLWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULLWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulLowInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulLowInt16x32 x y mask)
	// result: (VPMULLWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULLWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulLowInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulLowInt16x8 x y mask)
	// result: (VPMULLWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULLWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulLowInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulLowInt32x16 x y mask)
	// result: (VPMULLDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULLDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulLowInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulLowInt32x4 x y mask)
	// result: (VPMULLDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULLDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulLowInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulLowInt32x8 x y mask)
	// result: (VPMULLDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULLDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulLowInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulLowInt64x2 x y mask)
	// result: (VPMULLQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULLQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulLowInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulLowInt64x4 x y mask)
	// result: (VPMULLQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULLQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedMulLowInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedMulLowInt64x8 x y mask)
	// result: (VPMULLQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMULLQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualFloat32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VCMPPSMasked512 [4] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualFloat32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VCMPPSMasked128 [4] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualFloat32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VCMPPSMasked256 [4] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPSMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualFloat64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VCMPPDMasked128 [4] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualFloat64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VCMPPDMasked256 [4] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualFloat64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VCMPPDMasked512 [4] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPWMasked256 [4] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPWMasked512 [4] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPWMasked128 [4] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPDMasked512 [4] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPDMasked128 [4] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPDMasked256 [4] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPQMasked128 [4] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPQMasked256 [4] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPQMasked512 [4] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPBMasked128 [4] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPBMasked256 [4] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualInt8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPBMasked512 [4] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint16x16 x y mask)
	// result: (VPMOVMToVec16x16 (VPCMPUWMasked256 [4] x y (VPMOVVec16x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint16x32 x y mask)
	// result: (VPMOVMToVec16x32 (VPCMPUWMasked512 [4] x y (VPMOVVec16x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint16x8 x y mask)
	// result: (VPMOVMToVec16x8 (VPCMPUWMasked128 [4] x y (VPMOVVec16x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUWMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint32x16 x y mask)
	// result: (VPMOVMToVec32x16 (VPCMPUDMasked512 [4] x y (VPMOVVec32x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint32x4 x y mask)
	// result: (VPMOVMToVec32x4 (VPCMPUDMasked128 [4] x y (VPMOVVec32x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint32x8 x y mask)
	// result: (VPMOVMToVec32x8 (VPCMPUDMasked256 [4] x y (VPMOVVec32x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUDMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint64x2 x y mask)
	// result: (VPMOVMToVec64x2 (VPCMPUQMasked128 [4] x y (VPMOVVec64x2ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint64x4 x y mask)
	// result: (VPMOVMToVec64x4 (VPCMPUQMasked256 [4] x y (VPMOVVec64x4ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint64x8 x y mask)
	// result: (VPMOVMToVec64x8 (VPCMPUQMasked512 [4] x y (VPMOVVec64x8ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint8x16 x y mask)
	// result: (VPMOVMToVec8x16 (VPCMPUBMasked128 [4] x y (VPMOVVec8x16ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint8x32 x y mask)
	// result: (VPMOVMToVec8x32 (VPCMPUBMasked256 [4] x y (VPMOVVec8x32ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedNotEqualUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (MaskedNotEqualUint8x64 x y mask)
	// result: (VPMOVMToVec8x64 (VPCMPUBMasked512 [4] x y (VPMOVVec8x64ToM <types.TypeMask> mask)))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUBMasked512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v1 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v1.AddArg(mask)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrFloat32x16 x y mask)
	// result: (VORPSMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VORPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrFloat32x4 x y mask)
	// result: (VORPSMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VORPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrFloat32x8 x y mask)
	// result: (VORPSMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VORPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrFloat64x2 x y mask)
	// result: (VORPDMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VORPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrFloat64x4 x y mask)
	// result: (VORPDMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VORPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrFloat64x8 x y mask)
	// result: (VORPDMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VORPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrInt32x16 x y mask)
	// result: (VPORDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrInt32x4 x y mask)
	// result: (VPORDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrInt32x8 x y mask)
	// result: (VPORDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrInt64x2 x y mask)
	// result: (VPORQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrInt64x4 x y mask)
	// result: (VPORQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrInt64x8 x y mask)
	// result: (VPORQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrUint32x16 x y mask)
	// result: (VPORDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrUint32x4 x y mask)
	// result: (VPORDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrUint32x8 x y mask)
	// result: (VPORDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrUint64x2 x y mask)
	// result: (VPORQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrUint64x4 x y mask)
	// result: (VPORQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedOrUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedOrUint64x8 x y mask)
	// result: (VPORQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPORQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPairDotProdAccumulateInt32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPairDotProdAccumulateInt32x16 x y z mask)
	// result: (VPDPWSSDMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPWSSDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPairDotProdAccumulateInt32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPairDotProdAccumulateInt32x4 x y z mask)
	// result: (VPDPWSSDMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPWSSDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPairDotProdAccumulateInt32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPairDotProdAccumulateInt32x8 x y z mask)
	// result: (VPDPWSSDMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPWSSDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPairDotProdInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPairDotProdInt16x16 x y mask)
	// result: (VPMADDWDMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMADDWDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPairDotProdInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPairDotProdInt16x32 x y mask)
	// result: (VPMADDWDMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMADDWDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPairDotProdInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPairDotProdInt16x8 x y mask)
	// result: (VPMADDWDMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMADDWDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt16x16 x mask)
	// result: (VPOPCNTWMasked256 x (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt16x32 x mask)
	// result: (VPOPCNTWMasked512 x (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt16x8 x mask)
	// result: (VPOPCNTWMasked128 x (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt32x16 x mask)
	// result: (VPOPCNTDMasked512 x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt32x4 x mask)
	// result: (VPOPCNTDMasked128 x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt32x8 x mask)
	// result: (VPOPCNTDMasked256 x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt64x2 x mask)
	// result: (VPOPCNTQMasked128 x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt64x4 x mask)
	// result: (VPOPCNTQMasked256 x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt64x8 x mask)
	// result: (VPOPCNTQMasked512 x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt8x16 x mask)
	// result: (VPOPCNTBMasked128 x (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt8x32 x mask)
	// result: (VPOPCNTBMasked256 x (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountInt8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountInt8x64 x mask)
	// result: (VPOPCNTBMasked512 x (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint16x16 x mask)
	// result: (VPOPCNTWMasked256 x (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint16x32 x mask)
	// result: (VPOPCNTWMasked512 x (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint16x8 x mask)
	// result: (VPOPCNTWMasked128 x (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint32x16 x mask)
	// result: (VPOPCNTDMasked512 x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint32x4 x mask)
	// result: (VPOPCNTDMasked128 x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint32x8 x mask)
	// result: (VPOPCNTDMasked256 x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint64x2 x mask)
	// result: (VPOPCNTQMasked128 x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint64x4 x mask)
	// result: (VPOPCNTQMasked256 x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint64x8 x mask)
	// result: (VPOPCNTQMasked512 x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint8x16 x mask)
	// result: (VPOPCNTBMasked128 x (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint8x32 x mask)
	// result: (VPOPCNTBMasked256 x (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedPopCountUint8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedPopCountUint8x64 x mask)
	// result: (VPOPCNTBMasked512 x (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPOPCNTBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftInt32x16 [a] x mask)
	// result: (VPROLDMasked512 [a] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLDMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftInt32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftInt32x4 [a] x mask)
	// result: (VPROLDMasked128 [a] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLDMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftInt32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftInt32x8 [a] x mask)
	// result: (VPROLDMasked256 [a] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLDMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftInt64x2 [a] x mask)
	// result: (VPROLQMasked128 [a] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLQMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftInt64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftInt64x4 [a] x mask)
	// result: (VPROLQMasked256 [a] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLQMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftInt64x8 [a] x mask)
	// result: (VPROLQMasked512 [a] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLQMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftUint32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftUint32x16 [a] x mask)
	// result: (VPROLDMasked512 [a] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLDMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftUint32x4 [a] x mask)
	// result: (VPROLDMasked128 [a] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLDMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftUint32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftUint32x8 [a] x mask)
	// result: (VPROLDMasked256 [a] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLDMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftUint64x2 [a] x mask)
	// result: (VPROLQMasked128 [a] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLQMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftUint64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftUint64x4 [a] x mask)
	// result: (VPROLQMasked256 [a] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLQMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllLeftUint64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllLeftUint64x8 [a] x mask)
	// result: (VPROLQMasked512 [a] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPROLQMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightInt32x16 [a] x mask)
	// result: (VPRORDMasked512 [a] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORDMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightInt32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightInt32x4 [a] x mask)
	// result: (VPRORDMasked128 [a] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORDMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightInt32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightInt32x8 [a] x mask)
	// result: (VPRORDMasked256 [a] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORDMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightInt64x2 [a] x mask)
	// result: (VPRORQMasked128 [a] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORQMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightInt64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightInt64x4 [a] x mask)
	// result: (VPRORQMasked256 [a] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORQMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightInt64x8 [a] x mask)
	// result: (VPRORQMasked512 [a] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORQMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightUint32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightUint32x16 [a] x mask)
	// result: (VPRORDMasked512 [a] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORDMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightUint32x4 [a] x mask)
	// result: (VPRORDMasked128 [a] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORDMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightUint32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightUint32x8 [a] x mask)
	// result: (VPRORDMasked256 [a] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORDMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightUint64x2 [a] x mask)
	// result: (VPRORQMasked128 [a] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORQMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightUint64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightUint64x4 [a] x mask)
	// result: (VPRORQMasked256 [a] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORQMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateAllRightUint64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateAllRightUint64x8 [a] x mask)
	// result: (VPRORQMasked512 [a] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VPRORQMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftInt32x16 x y mask)
	// result: (VPROLVDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftInt32x4 x y mask)
	// result: (VPROLVDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftInt32x8 x y mask)
	// result: (VPROLVDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftInt64x2 x y mask)
	// result: (VPROLVQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftInt64x4 x y mask)
	// result: (VPROLVQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftInt64x8 x y mask)
	// result: (VPROLVQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftUint32x16 x y mask)
	// result: (VPROLVDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftUint32x4 x y mask)
	// result: (VPROLVDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftUint32x8 x y mask)
	// result: (VPROLVDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftUint64x2 x y mask)
	// result: (VPROLVQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftUint64x4 x y mask)
	// result: (VPROLVQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateLeftUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateLeftUint64x8 x y mask)
	// result: (VPROLVQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPROLVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightInt32x16 x y mask)
	// result: (VPRORVDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightInt32x4 x y mask)
	// result: (VPRORVDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightInt32x8 x y mask)
	// result: (VPRORVDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightInt64x2 x y mask)
	// result: (VPRORVQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightInt64x4 x y mask)
	// result: (VPRORVQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightInt64x8 x y mask)
	// result: (VPRORVQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightUint32x16 x y mask)
	// result: (VPRORVDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightUint32x4 x y mask)
	// result: (VPRORVDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightUint32x8 x y mask)
	// result: (VPRORVDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightUint64x2 x y mask)
	// result: (VPRORVQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightUint64x4 x y mask)
	// result: (VPRORVQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRotateRightUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRotateRightUint64x8 x y mask)
	// result: (VPRORVQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPRORVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundSuppressExceptionWithPrecisionFloat32x16 [a] x mask)
	// result: (VRNDSCALEPSMasked512 [a+8] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundSuppressExceptionWithPrecisionFloat32x4 [a] x mask)
	// result: (VRNDSCALEPSMasked128 [a+8] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundSuppressExceptionWithPrecisionFloat32x8 [a] x mask)
	// result: (VRNDSCALEPSMasked256 [a+8] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundSuppressExceptionWithPrecisionFloat64x2 [a] x mask)
	// result: (VRNDSCALEPDMasked128 [a+8] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundSuppressExceptionWithPrecisionFloat64x4 [a] x mask)
	// result: (VRNDSCALEPDMasked256 [a+8] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundSuppressExceptionWithPrecisionFloat64x8 [a] x mask)
	// result: (VRNDSCALEPDMasked512 [a+8] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundWithPrecisionFloat32x16 [a] x mask)
	// result: (VRNDSCALEPSMasked512 [a+0] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundWithPrecisionFloat32x4 [a] x mask)
	// result: (VRNDSCALEPSMasked128 [a+0] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundWithPrecisionFloat32x8 [a] x mask)
	// result: (VRNDSCALEPSMasked256 [a+0] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundWithPrecisionFloat64x2 [a] x mask)
	// result: (VRNDSCALEPDMasked128 [a+0] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundWithPrecisionFloat64x4 [a] x mask)
	// result: (VRNDSCALEPDMasked256 [a+0] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedRoundWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedRoundWithPrecisionFloat64x8 [a] x mask)
	// result: (VRNDSCALEPDMasked512 [a+0] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 0)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddInt16x16 x y mask)
	// result: (VPADDSWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddInt16x32 x y mask)
	// result: (VPADDSWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddInt16x8 x y mask)
	// result: (VPADDSWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddInt8x16 x y mask)
	// result: (VPADDSBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddInt8x32 x y mask)
	// result: (VPADDSBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddInt8x64 x y mask)
	// result: (VPADDSBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddUint16x16 x y mask)
	// result: (VPADDSWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddUint16x32 x y mask)
	// result: (VPADDSWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddUint16x8 x y mask)
	// result: (VPADDSWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddUint8x16 x y mask)
	// result: (VPADDSBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddUint8x32 x y mask)
	// result: (VPADDSBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedAddUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedAddUint8x64 x y mask)
	// result: (VPADDSBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPADDSBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedPairDotProdAccumulateInt32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedPairDotProdAccumulateInt32x16 x y z mask)
	// result: (VPDPWSSDSMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPWSSDSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedPairDotProdAccumulateInt32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedPairDotProdAccumulateInt32x4 x y z mask)
	// result: (VPDPWSSDSMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPWSSDSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedPairDotProdAccumulateInt32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedPairDotProdAccumulateInt32x8 x y z mask)
	// result: (VPDPWSSDSMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPWSSDSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubInt16x16 x y mask)
	// result: (VPSUBSWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubInt16x32 x y mask)
	// result: (VPSUBSWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubInt16x8 x y mask)
	// result: (VPSUBSWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubInt8x16 x y mask)
	// result: (VPSUBSBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubInt8x32 x y mask)
	// result: (VPSUBSBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubInt8x64 x y mask)
	// result: (VPSUBSBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubUint16x16 x y mask)
	// result: (VPSUBSWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubUint16x32 x y mask)
	// result: (VPSUBSWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubUint16x8 x y mask)
	// result: (VPSUBSWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubUint8x16 x y mask)
	// result: (VPSUBSBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubUint8x32 x y mask)
	// result: (VPSUBSBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedSubUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedSubUint8x64 x y mask)
	// result: (VPSUBSBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBSBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedPairDotProdUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedUnsignedSignedPairDotProdUint8x16 x y mask)
	// result: (VPMADDUBSWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMADDUBSWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedPairDotProdUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedUnsignedSignedPairDotProdUint8x32 x y mask)
	// result: (VPMADDUBSWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMADDUBSWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedPairDotProdUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedUnsignedSignedPairDotProdUint8x64 x y mask)
	// result: (VPMADDUBSWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPMADDUBSWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x16 x y z mask)
	// result: (VPDPBUSDSMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x4 x y z mask)
	// result: (VPDPBUSDSMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedUnsignedSignedQuadDotProdAccumulateInt32x8 x y z mask)
	// result: (VPDPBUSDSMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x16 x y z mask)
	// result: (VPDPBUSDSMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x4 x y z mask)
	// result: (VPDPBUSDSMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSaturatedUnsignedSignedQuadDotProdAccumulateUint32x8 x y z mask)
	// result: (VPDPBUSDSMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromInt16x16 [a] x y mask)
	// result: (VPSHLDWMasked256 [a] x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDWMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromInt16x32 [a] x y mask)
	// result: (VPSHLDWMasked512 [a] x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDWMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromInt16x8 [a] x y mask)
	// result: (VPSHLDWMasked128 [a] x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDWMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromInt32x16 [a] x y mask)
	// result: (VPSHLDDMasked512 [a] x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDDMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromInt32x4 [a] x y mask)
	// result: (VPSHLDDMasked128 [a] x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDDMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromInt32x8 [a] x y mask)
	// result: (VPSHLDDMasked256 [a] x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDDMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromInt64x2 [a] x y mask)
	// result: (VPSHLDQMasked128 [a] x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDQMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromInt64x4 [a] x y mask)
	// result: (VPSHLDQMasked256 [a] x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDQMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromInt64x8 [a] x y mask)
	// result: (VPSHLDQMasked512 [a] x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDQMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromUint16x16 [a] x y mask)
	// result: (VPSHLDWMasked256 [a] x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDWMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromUint16x32 [a] x y mask)
	// result: (VPSHLDWMasked512 [a] x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDWMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromUint16x8 [a] x y mask)
	// result: (VPSHLDWMasked128 [a] x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDWMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromUint32x16 [a] x y mask)
	// result: (VPSHLDDMasked512 [a] x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDDMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromUint32x4 [a] x y mask)
	// result: (VPSHLDDMasked128 [a] x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDDMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromUint32x8 [a] x y mask)
	// result: (VPSHLDDMasked256 [a] x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDDMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromUint64x2 [a] x y mask)
	// result: (VPSHLDQMasked128 [a] x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDQMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromUint64x4 [a] x y mask)
	// result: (VPSHLDQMasked256 [a] x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDQMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftAndFillUpperFromUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftAndFillUpperFromUint64x8 [a] x y mask)
	// result: (VPSHLDQMasked512 [a] x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHLDQMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftInt64x2 x y mask)
	// result: (VPSLLQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftInt64x4 x y mask)
	// result: (VPSLLQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftInt64x8 x y mask)
	// result: (VPSLLQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftUint64x2 x y mask)
	// result: (VPSLLQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftUint64x4 x y mask)
	// result: (VPSLLQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllLeftUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllLeftUint64x8 x y mask)
	// result: (VPSLLQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromInt16x16 [a] x y mask)
	// result: (VPSHRDWMasked256 [a] x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDWMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromInt16x32 [a] x y mask)
	// result: (VPSHRDWMasked512 [a] x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDWMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromInt16x8 [a] x y mask)
	// result: (VPSHRDWMasked128 [a] x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDWMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromInt32x16 [a] x y mask)
	// result: (VPSHRDDMasked512 [a] x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDDMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromInt32x4 [a] x y mask)
	// result: (VPSHRDDMasked128 [a] x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDDMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromInt32x8 [a] x y mask)
	// result: (VPSHRDDMasked256 [a] x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDDMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromInt64x2 [a] x y mask)
	// result: (VPSHRDQMasked128 [a] x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDQMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromInt64x4 [a] x y mask)
	// result: (VPSHRDQMasked256 [a] x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDQMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromInt64x8 [a] x y mask)
	// result: (VPSHRDQMasked512 [a] x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDQMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromUint16x16 [a] x y mask)
	// result: (VPSHRDWMasked256 [a] x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDWMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromUint16x32 [a] x y mask)
	// result: (VPSHRDWMasked512 [a] x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDWMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromUint16x8 [a] x y mask)
	// result: (VPSHRDWMasked128 [a] x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDWMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromUint32x16 [a] x y mask)
	// result: (VPSHRDDMasked512 [a] x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDDMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromUint32x4 [a] x y mask)
	// result: (VPSHRDDMasked128 [a] x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDDMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromUint32x8 [a] x y mask)
	// result: (VPSHRDDMasked256 [a] x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDDMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromUint64x2 [a] x y mask)
	// result: (VPSHRDQMasked128 [a] x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDQMasked128)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromUint64x4 [a] x y mask)
	// result: (VPSHRDQMasked256 [a] x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDQMasked256)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightAndFillUpperFromUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightAndFillUpperFromUint64x8 [a] x y mask)
	// result: (VPSHRDQMasked512 [a] x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSHRDQMasked512)
		v.AuxInt = int8ToAuxInt(a)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightInt64x2 x y mask)
	// result: (VPSRLQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightInt64x4 x y mask)
	// result: (VPSRLQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightInt64x8 x y mask)
	// result: (VPSRLQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightSignExtendedInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightSignExtendedInt64x2 x y mask)
	// result: (VPSRAQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightSignExtendedInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightSignExtendedInt64x4 x y mask)
	// result: (VPSRAQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightSignExtendedInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightSignExtendedInt64x8 x y mask)
	// result: (VPSRAQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightUint64x2 x y mask)
	// result: (VPSRLQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightUint64x4 x y mask)
	// result: (VPSRLQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftAllRightUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftAllRightUint64x8 x y mask)
	// result: (VPSRLQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt16x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromInt16x16 x y z mask)
	// result: (VPSHLDVWMasked256 x y z (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt16x32(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromInt16x32 x y z mask)
	// result: (VPSHLDVWMasked512 x y z (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt16x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromInt16x8 x y z mask)
	// result: (VPSHLDVWMasked128 x y z (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromInt32x16 x y z mask)
	// result: (VPSHLDVDMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromInt32x4 x y z mask)
	// result: (VPSHLDVDMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromInt32x8 x y z mask)
	// result: (VPSHLDVDMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt64x2(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromInt64x2 x y z mask)
	// result: (VPSHLDVQMasked128 x y z (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt64x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromInt64x4 x y z mask)
	// result: (VPSHLDVQMasked256 x y z (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromInt64x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromInt64x8 x y z mask)
	// result: (VPSHLDVQMasked512 x y z (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint16x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromUint16x16 x y z mask)
	// result: (VPSHLDVWMasked256 x y z (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint16x32(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromUint16x32 x y z mask)
	// result: (VPSHLDVWMasked512 x y z (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint16x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromUint16x8 x y z mask)
	// result: (VPSHLDVWMasked128 x y z (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromUint32x16 x y z mask)
	// result: (VPSHLDVDMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromUint32x4 x y z mask)
	// result: (VPSHLDVDMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromUint32x8 x y z mask)
	// result: (VPSHLDVDMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint64x2(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromUint64x2 x y z mask)
	// result: (VPSHLDVQMasked128 x y z (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint64x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromUint64x4 x y z mask)
	// result: (VPSHLDVQMasked256 x y z (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftAndFillUpperFromUint64x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftAndFillUpperFromUint64x8 x y z mask)
	// result: (VPSHLDVQMasked512 x y z (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHLDVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftInt16x16 x y mask)
	// result: (VPSLLVWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftInt16x32 x y mask)
	// result: (VPSLLVWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftInt16x8 x y mask)
	// result: (VPSLLVWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftInt32x16 x y mask)
	// result: (VPSLLVDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftInt32x4 x y mask)
	// result: (VPSLLVDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftInt32x8 x y mask)
	// result: (VPSLLVDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftInt64x2 x y mask)
	// result: (VPSLLVQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftInt64x4 x y mask)
	// result: (VPSLLVQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftInt64x8 x y mask)
	// result: (VPSLLVQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftUint16x16 x y mask)
	// result: (VPSLLVWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftUint16x32 x y mask)
	// result: (VPSLLVWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftUint16x8 x y mask)
	// result: (VPSLLVWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftUint32x16 x y mask)
	// result: (VPSLLVDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftUint32x4 x y mask)
	// result: (VPSLLVDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftUint32x8 x y mask)
	// result: (VPSLLVDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftUint64x2 x y mask)
	// result: (VPSLLVQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftUint64x4 x y mask)
	// result: (VPSLLVQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftLeftUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftLeftUint64x8 x y mask)
	// result: (VPSLLVQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSLLVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt16x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromInt16x16 x y z mask)
	// result: (VPSHRDVWMasked256 x y z (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt16x32(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromInt16x32 x y z mask)
	// result: (VPSHRDVWMasked512 x y z (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt16x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromInt16x8 x y z mask)
	// result: (VPSHRDVWMasked128 x y z (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromInt32x16 x y z mask)
	// result: (VPSHRDVDMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromInt32x4 x y z mask)
	// result: (VPSHRDVDMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromInt32x8 x y z mask)
	// result: (VPSHRDVDMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt64x2(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromInt64x2 x y z mask)
	// result: (VPSHRDVQMasked128 x y z (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt64x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromInt64x4 x y z mask)
	// result: (VPSHRDVQMasked256 x y z (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromInt64x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromInt64x8 x y z mask)
	// result: (VPSHRDVQMasked512 x y z (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint16x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromUint16x16 x y z mask)
	// result: (VPSHRDVWMasked256 x y z (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint16x32(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromUint16x32 x y z mask)
	// result: (VPSHRDVWMasked512 x y z (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint16x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromUint16x8 x y z mask)
	// result: (VPSHRDVWMasked128 x y z (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromUint32x16 x y z mask)
	// result: (VPSHRDVDMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromUint32x4 x y z mask)
	// result: (VPSHRDVDMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromUint32x8 x y z mask)
	// result: (VPSHRDVDMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint64x2(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromUint64x2 x y z mask)
	// result: (VPSHRDVQMasked128 x y z (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint64x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromUint64x4 x y z mask)
	// result: (VPSHRDVQMasked256 x y z (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightAndFillUpperFromUint64x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightAndFillUpperFromUint64x8 x y z mask)
	// result: (VPSHRDVQMasked512 x y z (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPSHRDVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightInt16x16 x y mask)
	// result: (VPSRLVWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightInt16x32 x y mask)
	// result: (VPSRLVWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightInt16x8 x y mask)
	// result: (VPSRLVWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightInt32x16 x y mask)
	// result: (VPSRLVDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightInt32x4 x y mask)
	// result: (VPSRLVDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightInt32x8 x y mask)
	// result: (VPSRLVDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightInt64x2 x y mask)
	// result: (VPSRLVQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightInt64x4 x y mask)
	// result: (VPSRLVQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightInt64x8 x y mask)
	// result: (VPSRLVQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedInt16x16 x y mask)
	// result: (VPSRAVWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedInt16x32 x y mask)
	// result: (VPSRAVWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedInt16x8 x y mask)
	// result: (VPSRAVWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedInt32x16 x y mask)
	// result: (VPSRAVDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedInt32x4 x y mask)
	// result: (VPSRAVDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedInt32x8 x y mask)
	// result: (VPSRAVDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedInt64x2 x y mask)
	// result: (VPSRAVQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedInt64x4 x y mask)
	// result: (VPSRAVQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedInt64x8 x y mask)
	// result: (VPSRAVQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedUint16x16 x y mask)
	// result: (VPSRAVWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedUint16x32 x y mask)
	// result: (VPSRAVWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedUint16x8 x y mask)
	// result: (VPSRAVWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedUint32x16 x y mask)
	// result: (VPSRAVDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedUint32x4 x y mask)
	// result: (VPSRAVDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedUint32x8 x y mask)
	// result: (VPSRAVDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedUint64x2 x y mask)
	// result: (VPSRAVQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedUint64x4 x y mask)
	// result: (VPSRAVQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightSignExtendedUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightSignExtendedUint64x8 x y mask)
	// result: (VPSRAVQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRAVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightUint16x16 x y mask)
	// result: (VPSRLVWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightUint16x32 x y mask)
	// result: (VPSRLVWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightUint16x8 x y mask)
	// result: (VPSRLVWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightUint32x16 x y mask)
	// result: (VPSRLVDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightUint32x4 x y mask)
	// result: (VPSRLVDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightUint32x8 x y mask)
	// result: (VPSRLVDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightUint64x2 x y mask)
	// result: (VPSRLVQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightUint64x4 x y mask)
	// result: (VPSRLVQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedShiftRightUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedShiftRightUint64x8 x y mask)
	// result: (VPSRLVQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSRLVQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSqrtFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSqrtFloat32x16 x mask)
	// result: (VSQRTPSMasked512 x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VSQRTPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSqrtFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSqrtFloat32x4 x mask)
	// result: (VSQRTPSMasked128 x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VSQRTPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSqrtFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSqrtFloat32x8 x mask)
	// result: (VSQRTPSMasked256 x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VSQRTPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSqrtFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSqrtFloat64x2 x mask)
	// result: (VSQRTPDMasked128 x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VSQRTPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSqrtFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSqrtFloat64x4 x mask)
	// result: (VSQRTPDMasked256 x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VSQRTPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSqrtFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSqrtFloat64x8 x mask)
	// result: (VSQRTPDMasked512 x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		mask := v_1
		v.reset(OpAMD64VSQRTPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubFloat32x16 x y mask)
	// result: (VSUBPSMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSUBPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubFloat32x4 x y mask)
	// result: (VSUBPSMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSUBPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubFloat32x8 x y mask)
	// result: (VSUBPSMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSUBPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubFloat64x2 x y mask)
	// result: (VSUBPDMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSUBPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubFloat64x4 x y mask)
	// result: (VSUBPDMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSUBPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubFloat64x8 x y mask)
	// result: (VSUBPDMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VSUBPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt16x16 x y mask)
	// result: (VPSUBWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt16x32 x y mask)
	// result: (VPSUBWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt16x8 x y mask)
	// result: (VPSUBWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt32x16 x y mask)
	// result: (VPSUBDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt32x4 x y mask)
	// result: (VPSUBDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt32x8 x y mask)
	// result: (VPSUBDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt64x2 x y mask)
	// result: (VPSUBQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt64x4 x y mask)
	// result: (VPSUBQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt64x8 x y mask)
	// result: (VPSUBQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt8x16 x y mask)
	// result: (VPSUBBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt8x32 x y mask)
	// result: (VPSUBBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubInt8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubInt8x64 x y mask)
	// result: (VPSUBBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint16x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint16x16 x y mask)
	// result: (VPSUBWMasked256 x y (VPMOVVec16x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBWMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint16x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint16x32 x y mask)
	// result: (VPSUBWMasked512 x y (VPMOVVec16x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBWMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint16x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint16x8 x y mask)
	// result: (VPSUBWMasked128 x y (VPMOVVec16x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBWMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec16x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint32x16 x y mask)
	// result: (VPSUBDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint32x4 x y mask)
	// result: (VPSUBDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint32x8 x y mask)
	// result: (VPSUBDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint64x2 x y mask)
	// result: (VPSUBQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint64x4 x y mask)
	// result: (VPSUBQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint64x8 x y mask)
	// result: (VPSUBQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint8x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint8x16 x y mask)
	// result: (VPSUBBMasked128 x y (VPMOVVec8x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBBMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint8x32(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint8x32 x y mask)
	// result: (VPSUBBMasked256 x y (VPMOVVec8x32ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBBMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x32ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedSubUint8x64(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedSubUint8x64 x y mask)
	// result: (VPSUBBMasked512 x y (VPMOVVec8x64ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPSUBBMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec8x64ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncSuppressExceptionWithPrecisionFloat32x16 [a] x mask)
	// result: (VRNDSCALEPSMasked512 [a+11] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncSuppressExceptionWithPrecisionFloat32x4 [a] x mask)
	// result: (VRNDSCALEPSMasked128 [a+11] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncSuppressExceptionWithPrecisionFloat32x8 [a] x mask)
	// result: (VRNDSCALEPSMasked256 [a+11] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncSuppressExceptionWithPrecisionFloat64x2 [a] x mask)
	// result: (VRNDSCALEPDMasked128 [a+11] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncSuppressExceptionWithPrecisionFloat64x4 [a] x mask)
	// result: (VRNDSCALEPDMasked256 [a+11] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncSuppressExceptionWithPrecisionFloat64x8 [a] x mask)
	// result: (VRNDSCALEPDMasked512 [a+11] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 11)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncWithPrecisionFloat32x16 [a] x mask)
	// result: (VRNDSCALEPSMasked512 [a+3] x (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked512)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncWithPrecisionFloat32x4 [a] x mask)
	// result: (VRNDSCALEPSMasked128 [a+3] x (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked128)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncWithPrecisionFloat32x8 [a] x mask)
	// result: (VRNDSCALEPSMasked256 [a+3] x (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPSMasked256)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncWithPrecisionFloat64x2 [a] x mask)
	// result: (VRNDSCALEPDMasked128 [a+3] x (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked128)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncWithPrecisionFloat64x4 [a] x mask)
	// result: (VRNDSCALEPDMasked256 [a+3] x (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked256)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedTruncWithPrecisionFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedTruncWithPrecisionFloat64x8 [a] x mask)
	// result: (VRNDSCALEPDMasked512 [a+3] x (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		mask := v_1
		v.reset(OpAMD64VRNDSCALEPDMasked512)
		v.AuxInt = int8ToAuxInt(a + 3)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateInt32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedUnsignedSignedQuadDotProdAccumulateInt32x16 x y z mask)
	// result: (VPDPBUSDMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateInt32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedUnsignedSignedQuadDotProdAccumulateInt32x4 x y z mask)
	// result: (VPDPBUSDMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateInt32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedUnsignedSignedQuadDotProdAccumulateInt32x8 x y z mask)
	// result: (VPDPBUSDMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateUint32x16(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedUnsignedSignedQuadDotProdAccumulateUint32x16 x y z mask)
	// result: (VPDPBUSDMasked512 x y z (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateUint32x4(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedUnsignedSignedQuadDotProdAccumulateUint32x4 x y z mask)
	// result: (VPDPBUSDMasked128 x y z (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedUnsignedSignedQuadDotProdAccumulateUint32x8(v *Value) bool {
	v_3 := v.Args[3]
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedUnsignedSignedQuadDotProdAccumulateUint32x8 x y z mask)
	// result: (VPDPBUSDMasked256 x y z (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		z := v_2
		mask := v_3
		v.reset(OpAMD64VPDPBUSDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg4(x, y, z, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorFloat32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorFloat32x16 x y mask)
	// result: (VXORPSMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VXORPSMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorFloat32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorFloat32x4 x y mask)
	// result: (VXORPSMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VXORPSMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorFloat32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorFloat32x8 x y mask)
	// result: (VXORPSMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VXORPSMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorFloat64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorFloat64x2 x y mask)
	// result: (VXORPDMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VXORPDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorFloat64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorFloat64x4 x y mask)
	// result: (VXORPDMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VXORPDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorFloat64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorFloat64x8 x y mask)
	// result: (VXORPDMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VXORPDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorInt32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorInt32x16 x y mask)
	// result: (VPXORDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorInt32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorInt32x4 x y mask)
	// result: (VPXORDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorInt32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorInt32x8 x y mask)
	// result: (VPXORDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorInt64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorInt64x2 x y mask)
	// result: (VPXORQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorInt64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorInt64x4 x y mask)
	// result: (VPXORQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorInt64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorInt64x8 x y mask)
	// result: (VPXORQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorUint32x16(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorUint32x16 x y mask)
	// result: (VPXORDMasked512 x y (VPMOVVec32x16ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORDMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x16ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorUint32x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorUint32x4 x y mask)
	// result: (VPXORDMasked128 x y (VPMOVVec32x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORDMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorUint32x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorUint32x8 x y mask)
	// result: (VPXORDMasked256 x y (VPMOVVec32x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORDMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec32x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorUint64x2(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorUint64x2 x y mask)
	// result: (VPXORQMasked128 x y (VPMOVVec64x2ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORQMasked128)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x2ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorUint64x4(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorUint64x4 x y mask)
	// result: (VPXORQMasked256 x y (VPMOVVec64x4ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORQMasked256)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x4ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMaskedXorUint64x8(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (MaskedXorUint64x8 x y mask)
	// result: (VPXORQMasked512 x y (VPMOVVec64x8ToM <types.TypeMask> mask))
	for {
		x := v_0
		y := v_1
		mask := v_2
		v.reset(OpAMD64VPXORQMasked512)
		v0 := b.NewValue0(v.Pos, OpAMD64VPMOVVec64x8ToM, types.TypeMask)
		v0.AddArg(mask)
		v.AddArg3(x, y, v0)
		return true
	}
}
func rewriteValueAMD64_OpMax32F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Max32F <t> x y)
	// result: (Neg32F <t> (Min32F <t> (Neg32F <t> x) (Neg32F <t> y)))
	for {
		t := v.Type
		x := v_0
		y := v_1
		v.reset(OpNeg32F)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpMin32F, t)
		v1 := b.NewValue0(v.Pos, OpNeg32F, t)
		v1.AddArg(x)
		v2 := b.NewValue0(v.Pos, OpNeg32F, t)
		v2.AddArg(y)
		v0.AddArg2(v1, v2)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMax64F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Max64F <t> x y)
	// result: (Neg64F <t> (Min64F <t> (Neg64F <t> x) (Neg64F <t> y)))
	for {
		t := v.Type
		x := v_0
		y := v_1
		v.reset(OpNeg64F)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpMin64F, t)
		v1 := b.NewValue0(v.Pos, OpNeg64F, t)
		v1.AddArg(x)
		v2 := b.NewValue0(v.Pos, OpNeg64F, t)
		v2.AddArg(y)
		v0.AddArg2(v1, v2)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMin32F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Min32F <t> x y)
	// result: (POR (MINSS <t> (MINSS <t> x y) x) (MINSS <t> x y))
	for {
		t := v.Type
		x := v_0
		y := v_1
		v.reset(OpAMD64POR)
		v0 := b.NewValue0(v.Pos, OpAMD64MINSS, t)
		v1 := b.NewValue0(v.Pos, OpAMD64MINSS, t)
		v1.AddArg2(x, y)
		v0.AddArg2(v1, x)
		v.AddArg2(v0, v1)
		return true
	}
}
func rewriteValueAMD64_OpMin64F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Min64F <t> x y)
	// result: (POR (MINSD <t> (MINSD <t> x y) x) (MINSD <t> x y))
	for {
		t := v.Type
		x := v_0
		y := v_1
		v.reset(OpAMD64POR)
		v0 := b.NewValue0(v.Pos, OpAMD64MINSD, t)
		v1 := b.NewValue0(v.Pos, OpAMD64MINSD, t)
		v1.AddArg2(x, y)
		v0.AddArg2(v1, x)
		v.AddArg2(v0, v1)
		return true
	}
}
func rewriteValueAMD64_OpMod16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Mod16 [a] x y)
	// result: (Select1 (DIVW [a] x y))
	for {
		a := auxIntToBool(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, types.NewTuple(typ.Int16, typ.Int16))
		v0.AuxInt = boolToAuxInt(a)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMod16u(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Mod16u x y)
	// result: (Select1 (DIVWU x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, types.NewTuple(typ.UInt16, typ.UInt16))
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMod32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Mod32 [a] x y)
	// result: (Select1 (DIVL [a] x y))
	for {
		a := auxIntToBool(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVL, types.NewTuple(typ.Int32, typ.Int32))
		v0.AuxInt = boolToAuxInt(a)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMod32u(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Mod32u x y)
	// result: (Select1 (DIVLU x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVLU, types.NewTuple(typ.UInt32, typ.UInt32))
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMod64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Mod64 [a] x y)
	// result: (Select1 (DIVQ [a] x y))
	for {
		a := auxIntToBool(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVQ, types.NewTuple(typ.Int64, typ.Int64))
		v0.AuxInt = boolToAuxInt(a)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMod64u(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Mod64u x y)
	// result: (Select1 (DIVQU x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVQU, types.NewTuple(typ.UInt64, typ.UInt64))
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMod8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Mod8 x y)
	// result: (Select1 (DIVW (SignExt8to16 x) (SignExt8to16 y)))
	for {
		x := v_0
		y := v_1
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVW, types.NewTuple(typ.Int16, typ.Int16))
		v1 := b.NewValue0(v.Pos, OpSignExt8to16, typ.Int16)
		v1.AddArg(x)
		v2 := b.NewValue0(v.Pos, OpSignExt8to16, typ.Int16)
		v2.AddArg(y)
		v0.AddArg2(v1, v2)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMod8u(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Mod8u x y)
	// result: (Select1 (DIVWU (ZeroExt8to16 x) (ZeroExt8to16 y)))
	for {
		x := v_0
		y := v_1
		v.reset(OpSelect1)
		v0 := b.NewValue0(v.Pos, OpAMD64DIVWU, types.NewTuple(typ.UInt16, typ.UInt16))
		v1 := b.NewValue0(v.Pos, OpZeroExt8to16, typ.UInt16)
		v1.AddArg(x)
		v2 := b.NewValue0(v.Pos, OpZeroExt8to16, typ.UInt16)
		v2.AddArg(y)
		v0.AddArg2(v1, v2)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpMove(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Move [0] _ _ mem)
	// result: mem
	for {
		if auxIntToInt64(v.AuxInt) != 0 {
			break
		}
		mem := v_2
		v.copyOf(mem)
		return true
	}
	// match: (Move [1] dst src mem)
	// result: (MOVBstore dst (MOVBload src mem) mem)
	for {
		if auxIntToInt64(v.AuxInt) != 1 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVBload, typ.UInt8)
		v0.AddArg2(src, mem)
		v.AddArg3(dst, v0, mem)
		return true
	}
	// match: (Move [2] dst src mem)
	// result: (MOVWstore dst (MOVWload src mem) mem)
	for {
		if auxIntToInt64(v.AuxInt) != 2 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVWstore)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVWload, typ.UInt16)
		v0.AddArg2(src, mem)
		v.AddArg3(dst, v0, mem)
		return true
	}
	// match: (Move [4] dst src mem)
	// result: (MOVLstore dst (MOVLload src mem) mem)
	for {
		if auxIntToInt64(v.AuxInt) != 4 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVLstore)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, typ.UInt32)
		v0.AddArg2(src, mem)
		v.AddArg3(dst, v0, mem)
		return true
	}
	// match: (Move [8] dst src mem)
	// result: (MOVQstore dst (MOVQload src mem) mem)
	for {
		if auxIntToInt64(v.AuxInt) != 8 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVQstore)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v0.AddArg2(src, mem)
		v.AddArg3(dst, v0, mem)
		return true
	}
	// match: (Move [16] dst src mem)
	// result: (MOVOstore dst (MOVOload src mem) mem)
	for {
		if auxIntToInt64(v.AuxInt) != 16 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVOstore)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVOload, types.TypeInt128)
		v0.AddArg2(src, mem)
		v.AddArg3(dst, v0, mem)
		return true
	}
	// match: (Move [32] dst src mem)
	// result: (Move [16] (OffPtr <dst.Type> dst [16]) (OffPtr <src.Type> src [16]) (Move [16] dst src mem))
	for {
		if auxIntToInt64(v.AuxInt) != 32 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpMove)
		v.AuxInt = int64ToAuxInt(16)
		v0 := b.NewValue0(v.Pos, OpOffPtr, dst.Type)
		v0.AuxInt = int64ToAuxInt(16)
		v0.AddArg(dst)
		v1 := b.NewValue0(v.Pos, OpOffPtr, src.Type)
		v1.AuxInt = int64ToAuxInt(16)
		v1.AddArg(src)
		v2 := b.NewValue0(v.Pos, OpMove, types.TypeMem)
		v2.AuxInt = int64ToAuxInt(16)
		v2.AddArg3(dst, src, mem)
		v.AddArg3(v0, v1, v2)
		return true
	}
	// match: (Move [48] dst src mem)
	// result: (Move [32] (OffPtr <dst.Type> dst [16]) (OffPtr <src.Type> src [16]) (Move [16] dst src mem))
	for {
		if auxIntToInt64(v.AuxInt) != 48 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpMove)
		v.AuxInt = int64ToAuxInt(32)
		v0 := b.NewValue0(v.Pos, OpOffPtr, dst.Type)
		v0.AuxInt = int64ToAuxInt(16)
		v0.AddArg(dst)
		v1 := b.NewValue0(v.Pos, OpOffPtr, src.Type)
		v1.AuxInt = int64ToAuxInt(16)
		v1.AddArg(src)
		v2 := b.NewValue0(v.Pos, OpMove, types.TypeMem)
		v2.AuxInt = int64ToAuxInt(16)
		v2.AddArg3(dst, src, mem)
		v.AddArg3(v0, v1, v2)
		return true
	}
	// match: (Move [64] dst src mem)
	// result: (Move [32] (OffPtr <dst.Type> dst [32]) (OffPtr <src.Type> src [32]) (Move [32] dst src mem))
	for {
		if auxIntToInt64(v.AuxInt) != 64 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpMove)
		v.AuxInt = int64ToAuxInt(32)
		v0 := b.NewValue0(v.Pos, OpOffPtr, dst.Type)
		v0.AuxInt = int64ToAuxInt(32)
		v0.AddArg(dst)
		v1 := b.NewValue0(v.Pos, OpOffPtr, src.Type)
		v1.AuxInt = int64ToAuxInt(32)
		v1.AddArg(src)
		v2 := b.NewValue0(v.Pos, OpMove, types.TypeMem)
		v2.AuxInt = int64ToAuxInt(32)
		v2.AddArg3(dst, src, mem)
		v.AddArg3(v0, v1, v2)
		return true
	}
	// match: (Move [3] dst src mem)
	// result: (MOVBstore [2] dst (MOVBload [2] src mem) (MOVWstore dst (MOVWload src mem) mem))
	for {
		if auxIntToInt64(v.AuxInt) != 3 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(2)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVBload, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(2)
		v0.AddArg2(src, mem)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVWstore, types.TypeMem)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVWload, typ.UInt16)
		v2.AddArg2(src, mem)
		v1.AddArg3(dst, v2, mem)
		v.AddArg3(dst, v0, v1)
		return true
	}
	// match: (Move [5] dst src mem)
	// result: (MOVBstore [4] dst (MOVBload [4] src mem) (MOVLstore dst (MOVLload src mem) mem))
	for {
		if auxIntToInt64(v.AuxInt) != 5 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(4)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVBload, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(4)
		v0.AddArg2(src, mem)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVLstore, types.TypeMem)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVLload, typ.UInt32)
		v2.AddArg2(src, mem)
		v1.AddArg3(dst, v2, mem)
		v.AddArg3(dst, v0, v1)
		return true
	}
	// match: (Move [6] dst src mem)
	// result: (MOVWstore [4] dst (MOVWload [4] src mem) (MOVLstore dst (MOVLload src mem) mem))
	for {
		if auxIntToInt64(v.AuxInt) != 6 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVWstore)
		v.AuxInt = int32ToAuxInt(4)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVWload, typ.UInt16)
		v0.AuxInt = int32ToAuxInt(4)
		v0.AddArg2(src, mem)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVLstore, types.TypeMem)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVLload, typ.UInt32)
		v2.AddArg2(src, mem)
		v1.AddArg3(dst, v2, mem)
		v.AddArg3(dst, v0, v1)
		return true
	}
	// match: (Move [7] dst src mem)
	// result: (MOVLstore [3] dst (MOVLload [3] src mem) (MOVLstore dst (MOVLload src mem) mem))
	for {
		if auxIntToInt64(v.AuxInt) != 7 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVLstore)
		v.AuxInt = int32ToAuxInt(3)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, typ.UInt32)
		v0.AuxInt = int32ToAuxInt(3)
		v0.AddArg2(src, mem)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVLstore, types.TypeMem)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVLload, typ.UInt32)
		v2.AddArg2(src, mem)
		v1.AddArg3(dst, v2, mem)
		v.AddArg3(dst, v0, v1)
		return true
	}
	// match: (Move [9] dst src mem)
	// result: (MOVBstore [8] dst (MOVBload [8] src mem) (MOVQstore dst (MOVQload src mem) mem))
	for {
		if auxIntToInt64(v.AuxInt) != 9 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVBstore)
		v.AuxInt = int32ToAuxInt(8)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVBload, typ.UInt8)
		v0.AuxInt = int32ToAuxInt(8)
		v0.AddArg2(src, mem)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVQstore, types.TypeMem)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v2.AddArg2(src, mem)
		v1.AddArg3(dst, v2, mem)
		v.AddArg3(dst, v0, v1)
		return true
	}
	// match: (Move [10] dst src mem)
	// result: (MOVWstore [8] dst (MOVWload [8] src mem) (MOVQstore dst (MOVQload src mem) mem))
	for {
		if auxIntToInt64(v.AuxInt) != 10 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVWstore)
		v.AuxInt = int32ToAuxInt(8)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVWload, typ.UInt16)
		v0.AuxInt = int32ToAuxInt(8)
		v0.AddArg2(src, mem)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVQstore, types.TypeMem)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v2.AddArg2(src, mem)
		v1.AddArg3(dst, v2, mem)
		v.AddArg3(dst, v0, v1)
		return true
	}
	// match: (Move [11] dst src mem)
	// result: (MOVLstore [7] dst (MOVLload [7] src mem) (MOVQstore dst (MOVQload src mem) mem))
	for {
		if auxIntToInt64(v.AuxInt) != 11 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVLstore)
		v.AuxInt = int32ToAuxInt(7)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, typ.UInt32)
		v0.AuxInt = int32ToAuxInt(7)
		v0.AddArg2(src, mem)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVQstore, types.TypeMem)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v2.AddArg2(src, mem)
		v1.AddArg3(dst, v2, mem)
		v.AddArg3(dst, v0, v1)
		return true
	}
	// match: (Move [12] dst src mem)
	// result: (MOVLstore [8] dst (MOVLload [8] src mem) (MOVQstore dst (MOVQload src mem) mem))
	for {
		if auxIntToInt64(v.AuxInt) != 12 {
			break
		}
		dst := v_0
		src := v_1
		mem := v_2
		v.reset(OpAMD64MOVLstore)
		v.AuxInt = int32ToAuxInt(8)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLload, typ.UInt32)
		v0.AuxInt = int32ToAuxInt(8)
		v0.AddArg2(src, mem)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVQstore, types.TypeMem)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v2.AddArg2(src, mem)
		v1.AddArg3(dst, v2, mem)
		v.AddArg3(dst, v0, v1)
		return true
	}
	// match: (Move [s] dst src mem)
	// cond: s >= 13 && s <= 15
	// result: (MOVQstore [int32(s-8)] dst (MOVQload [int32(s-8)] src mem) (MOVQstore dst (MOVQload src mem) mem))
	for {
		s := auxIntToInt64(v.AuxInt)
		dst := v_0
		src := v_1
		mem := v_2
		if !(s >= 13 && s <= 15) {
			break
		}
		v.reset(OpAMD64MOVQstore)
		v.AuxInt = int32ToAuxInt(int32(s - 8))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v0.AuxInt = int32ToAuxInt(int32(s - 8))
		v0.AddArg2(src, mem)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVQstore, types.TypeMem)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v2.AddArg2(src, mem)
		v1.AddArg3(dst, v2, mem)
		v.AddArg3(dst, v0, v1)
		return true
	}
	// match: (Move [s] dst src mem)
	// cond: s > 16 && s%16 != 0 && s%16 <= 8
	// result: (Move [s-s%16] (OffPtr <dst.Type> dst [s%16]) (OffPtr <src.Type> src [s%16]) (MOVQstore dst (MOVQload src mem) mem))
	for {
		s := auxIntToInt64(v.AuxInt)
		dst := v_0
		src := v_1
		mem := v_2
		if !(s > 16 && s%16 != 0 && s%16 <= 8) {
			break
		}
		v.reset(OpMove)
		v.AuxInt = int64ToAuxInt(s - s%16)
		v0 := b.NewValue0(v.Pos, OpOffPtr, dst.Type)
		v0.AuxInt = int64ToAuxInt(s % 16)
		v0.AddArg(dst)
		v1 := b.NewValue0(v.Pos, OpOffPtr, src.Type)
		v1.AuxInt = int64ToAuxInt(s % 16)
		v1.AddArg(src)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVQstore, types.TypeMem)
		v3 := b.NewValue0(v.Pos, OpAMD64MOVQload, typ.UInt64)
		v3.AddArg2(src, mem)
		v2.AddArg3(dst, v3, mem)
		v.AddArg3(v0, v1, v2)
		return true
	}
	// match: (Move [s] dst src mem)
	// cond: s > 16 && s%16 != 0 && s%16 > 8
	// result: (Move [s-s%16] (OffPtr <dst.Type> dst [s%16]) (OffPtr <src.Type> src [s%16]) (MOVOstore dst (MOVOload src mem) mem))
	for {
		s := auxIntToInt64(v.AuxInt)
		dst := v_0
		src := v_1
		mem := v_2
		if !(s > 16 && s%16 != 0 && s%16 > 8) {
			break
		}
		v.reset(OpMove)
		v.AuxInt = int64ToAuxInt(s - s%16)
		v0 := b.NewValue0(v.Pos, OpOffPtr, dst.Type)
		v0.AuxInt = int64ToAuxInt(s % 16)
		v0.AddArg(dst)
		v1 := b.NewValue0(v.Pos, OpOffPtr, src.Type)
		v1.AuxInt = int64ToAuxInt(s % 16)
		v1.AddArg(src)
		v2 := b.NewValue0(v.Pos, OpAMD64MOVOstore, types.TypeMem)
		v3 := b.NewValue0(v.Pos, OpAMD64MOVOload, types.TypeInt128)
		v3.AddArg2(src, mem)
		v2.AddArg3(dst, v3, mem)
		v.AddArg3(v0, v1, v2)
		return true
	}
	// match: (Move [s] dst src mem)
	// cond: s > 64 && s <= 16*64 && s%16 == 0 && logLargeCopy(v, s)
	// result: (DUFFCOPY [s] dst src mem)
	for {
		s := auxIntToInt64(v.AuxInt)
		dst := v_0
		src := v_1
		mem := v_2
		if !(s > 64 && s <= 16*64 && s%16 == 0 && logLargeCopy(v, s)) {
			break
		}
		v.reset(OpAMD64DUFFCOPY)
		v.AuxInt = int64ToAuxInt(s)
		v.AddArg3(dst, src, mem)
		return true
	}
	// match: (Move [s] dst src mem)
	// cond: s > 16*64 && s%8 == 0 && logLargeCopy(v, s)
	// result: (REPMOVSQ dst src (MOVQconst [s/8]) mem)
	for {
		s := auxIntToInt64(v.AuxInt)
		dst := v_0
		src := v_1
		mem := v_2
		if !(s > 16*64 && s%8 == 0 && logLargeCopy(v, s)) {
			break
		}
		v.reset(OpAMD64REPMOVSQ)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, typ.UInt64)
		v0.AuxInt = int64ToAuxInt(s / 8)
		v.AddArg4(dst, src, v0, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpNeg32F(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Neg32F x)
	// result: (PXOR x (MOVSSconst <typ.Float32> [float32(math.Copysign(0, -1))]))
	for {
		x := v_0
		v.reset(OpAMD64PXOR)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVSSconst, typ.Float32)
		v0.AuxInt = float32ToAuxInt(float32(math.Copysign(0, -1)))
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpNeg64F(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Neg64F x)
	// result: (PXOR x (MOVSDconst <typ.Float64> [math.Copysign(0, -1)]))
	for {
		x := v_0
		v.reset(OpAMD64PXOR)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVSDconst, typ.Float64)
		v0.AuxInt = float64ToAuxInt(math.Copysign(0, -1))
		v.AddArg2(x, v0)
		return true
	}
}
func rewriteValueAMD64_OpNeq16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Neq16 x y)
	// result: (SETNE (CMPW x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPW, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNeq32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Neq32 x y)
	// result: (SETNE (CMPL x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPL, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNeq32F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Neq32F x y)
	// result: (SETNEF (UCOMISS x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETNEF)
		v0 := b.NewValue0(v.Pos, OpAMD64UCOMISS, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNeq64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Neq64 x y)
	// result: (SETNE (CMPQ x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNeq64F(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Neq64F x y)
	// result: (SETNEF (UCOMISD x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETNEF)
		v0 := b.NewValue0(v.Pos, OpAMD64UCOMISD, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNeq8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Neq8 x y)
	// result: (SETNE (CMPB x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPB, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNeqB(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (NeqB x y)
	// result: (SETNE (CMPB x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPB, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNeqPtr(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (NeqPtr x y)
	// result: (SETNE (CMPQ x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64SETNE)
		v0 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNot(v *Value) bool {
	v_0 := v.Args[0]
	// match: (Not x)
	// result: (XORLconst [1] x)
	for {
		x := v_0
		v.reset(OpAMD64XORLconst)
		v.AuxInt = int32ToAuxInt(1)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualFloat32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualFloat32x16 x y)
	// result: (VPMOVMToVec32x16 (VCMPPS512 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPS512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualFloat32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (NotEqualFloat32x4 x y)
	// result: (VCMPPS128 [4] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS128)
		v.AuxInt = int8ToAuxInt(4)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualFloat32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (NotEqualFloat32x8 x y)
	// result: (VCMPPS256 [4] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPS256)
		v.AuxInt = int8ToAuxInt(4)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualFloat64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (NotEqualFloat64x2 x y)
	// result: (VCMPPD128 [4] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD128)
		v.AuxInt = int8ToAuxInt(4)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualFloat64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (NotEqualFloat64x4 x y)
	// result: (VCMPPD256 [4] x y)
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VCMPPD256)
		v.AuxInt = int8ToAuxInt(4)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualFloat64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualFloat64x8 x y)
	// result: (VPMOVMToVec64x8 (VCMPPD512 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VCMPPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt16x16 x y)
	// result: (VPMOVMToVec16x16 (VPCMPW256 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPW512 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt16x8 x y)
	// result: (VPMOVMToVec16x8 (VPCMPW128 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPW128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPD512 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt32x4 x y)
	// result: (VPMOVMToVec32x4 (VPCMPD128 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt32x8 x y)
	// result: (VPMOVMToVec32x8 (VPCMPD256 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPD256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt64x2 x y)
	// result: (VPMOVMToVec64x2 (VPCMPQ128 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt64x4 x y)
	// result: (VPMOVMToVec64x4 (VPCMPQ256 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPQ512 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt8x16 x y)
	// result: (VPMOVMToVec8x16 (VPCMPB128 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt8x32 x y)
	// result: (VPMOVMToVec8x32 (VPCMPB256 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualInt8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualInt8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPB512 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint16x16 x y)
	// result: (VPMOVMToVec16x16 (VPCMPUW256 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint16x32 x y)
	// result: (VPMOVMToVec16x32 (VPCMPUW512 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint16x8 x y)
	// result: (VPMOVMToVec16x8 (VPCMPUW128 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec16x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUW128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint32x16 x y)
	// result: (VPMOVMToVec32x16 (VPCMPUD512 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint32x4 x y)
	// result: (VPMOVMToVec32x4 (VPCMPUD128 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint32x8 x y)
	// result: (VPMOVMToVec32x8 (VPCMPUD256 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec32x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUD256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint64x2 x y)
	// result: (VPMOVMToVec64x2 (VPCMPUQ128 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x2)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint64x4 x y)
	// result: (VPMOVMToVec64x4 (VPCMPUQ256 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x4)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint64x8 x y)
	// result: (VPMOVMToVec64x8 (VPCMPUQ512 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec64x8)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUQ512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint8x16 x y)
	// result: (VPMOVMToVec8x16 (VPCMPUB128 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x16)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB128, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint8x32 x y)
	// result: (VPMOVMToVec8x32 (VPCMPUB256 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x32)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB256, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpNotEqualUint8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (NotEqualUint8x64 x y)
	// result: (VPMOVMToVec8x64 (VPCMPUB512 [4] x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64VPMOVMToVec8x64)
		v0 := b.NewValue0(v.Pos, OpAMD64VPCMPUB512, typ.Mask)
		v0.AuxInt = int8ToAuxInt(4)
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpOffPtr(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (OffPtr [off] ptr)
	// cond: is32Bit(off)
	// result: (ADDQconst [int32(off)] ptr)
	for {
		off := auxIntToInt64(v.AuxInt)
		ptr := v_0
		if !(is32Bit(off)) {
			break
		}
		v.reset(OpAMD64ADDQconst)
		v.AuxInt = int32ToAuxInt(int32(off))
		v.AddArg(ptr)
		return true
	}
	// match: (OffPtr [off] ptr)
	// result: (ADDQ (MOVQconst [off]) ptr)
	for {
		off := auxIntToInt64(v.AuxInt)
		ptr := v_0
		v.reset(OpAMD64ADDQ)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, typ.UInt64)
		v0.AuxInt = int64ToAuxInt(off)
		v.AddArg2(v0, ptr)
		return true
	}
}
func rewriteValueAMD64_OpPanicBounds(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (PanicBounds [kind] x y mem)
	// cond: boundsABI(kind) == 0
	// result: (LoweredPanicBoundsA [kind] x y mem)
	for {
		kind := auxIntToInt64(v.AuxInt)
		x := v_0
		y := v_1
		mem := v_2
		if !(boundsABI(kind) == 0) {
			break
		}
		v.reset(OpAMD64LoweredPanicBoundsA)
		v.AuxInt = int64ToAuxInt(kind)
		v.AddArg3(x, y, mem)
		return true
	}
	// match: (PanicBounds [kind] x y mem)
	// cond: boundsABI(kind) == 1
	// result: (LoweredPanicBoundsB [kind] x y mem)
	for {
		kind := auxIntToInt64(v.AuxInt)
		x := v_0
		y := v_1
		mem := v_2
		if !(boundsABI(kind) == 1) {
			break
		}
		v.reset(OpAMD64LoweredPanicBoundsB)
		v.AuxInt = int64ToAuxInt(kind)
		v.AddArg3(x, y, mem)
		return true
	}
	// match: (PanicBounds [kind] x y mem)
	// cond: boundsABI(kind) == 2
	// result: (LoweredPanicBoundsC [kind] x y mem)
	for {
		kind := auxIntToInt64(v.AuxInt)
		x := v_0
		y := v_1
		mem := v_2
		if !(boundsABI(kind) == 2) {
			break
		}
		v.reset(OpAMD64LoweredPanicBoundsC)
		v.AuxInt = int64ToAuxInt(kind)
		v.AddArg3(x, y, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpPopCount16(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (PopCount16 x)
	// result: (POPCNTL (MOVWQZX <typ.UInt32> x))
	for {
		x := v_0
		v.reset(OpAMD64POPCNTL)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVWQZX, typ.UInt32)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpPopCount8(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (PopCount8 x)
	// result: (POPCNTL (MOVBQZX <typ.UInt32> x))
	for {
		x := v_0
		v.reset(OpAMD64POPCNTL)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVBQZX, typ.UInt32)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftInt32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftInt32x16 [a] x)
	// result: (VPROLD512 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLD512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftInt32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftInt32x4 [a] x)
	// result: (VPROLD128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftInt32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftInt32x8 [a] x)
	// result: (VPROLD256 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLD256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftInt64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftInt64x2 [a] x)
	// result: (VPROLQ128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftInt64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftInt64x4 [a] x)
	// result: (VPROLQ256 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLQ256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftInt64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftInt64x8 [a] x)
	// result: (VPROLQ512 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLQ512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftUint32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftUint32x16 [a] x)
	// result: (VPROLD512 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLD512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftUint32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftUint32x4 [a] x)
	// result: (VPROLD128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftUint32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftUint32x8 [a] x)
	// result: (VPROLD256 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLD256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftUint64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftUint64x2 [a] x)
	// result: (VPROLQ128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftUint64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftUint64x4 [a] x)
	// result: (VPROLQ256 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLQ256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllLeftUint64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllLeftUint64x8 [a] x)
	// result: (VPROLQ512 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPROLQ512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightInt32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightInt32x16 [a] x)
	// result: (VPRORD512 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORD512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightInt32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightInt32x4 [a] x)
	// result: (VPRORD128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightInt32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightInt32x8 [a] x)
	// result: (VPRORD256 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORD256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightInt64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightInt64x2 [a] x)
	// result: (VPRORQ128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightInt64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightInt64x4 [a] x)
	// result: (VPRORQ256 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORQ256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightInt64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightInt64x8 [a] x)
	// result: (VPRORQ512 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORQ512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightUint32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightUint32x16 [a] x)
	// result: (VPRORD512 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORD512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightUint32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightUint32x4 [a] x)
	// result: (VPRORD128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightUint32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightUint32x8 [a] x)
	// result: (VPRORD256 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORD256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightUint64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightUint64x2 [a] x)
	// result: (VPRORQ128 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightUint64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightUint64x4 [a] x)
	// result: (VPRORQ256 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORQ256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRotateAllRightUint64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RotateAllRightUint64x8 [a] x)
	// result: (VPRORQ512 [a] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VPRORQ512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundFloat32x4 x)
	// result: (VROUNDPS128 [0] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPS128)
		v.AuxInt = int8ToAuxInt(0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundFloat32x8 x)
	// result: (VROUNDPS256 [0] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPS256)
		v.AuxInt = int8ToAuxInt(0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundFloat64x2 x)
	// result: (VROUNDPD128 [0] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPD128)
		v.AuxInt = int8ToAuxInt(0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundFloat64x4 x)
	// result: (VROUNDPD256 [0] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPD256)
		v.AuxInt = int8ToAuxInt(0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundSuppressExceptionWithPrecisionFloat32x16 [a] x)
	// result: (VRNDSCALEPS512 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS512)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundSuppressExceptionWithPrecisionFloat32x4 [a] x)
	// result: (VRNDSCALEPS128 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS128)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundSuppressExceptionWithPrecisionFloat32x8 [a] x)
	// result: (VRNDSCALEPS256 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS256)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundSuppressExceptionWithPrecisionFloat64x2 [a] x)
	// result: (VRNDSCALEPD128 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD128)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundSuppressExceptionWithPrecisionFloat64x4 [a] x)
	// result: (VRNDSCALEPD256 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD256)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundSuppressExceptionWithPrecisionFloat64x8 [a] x)
	// result: (VRNDSCALEPD512 [a+8] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD512)
		v.AuxInt = int8ToAuxInt(a + 8)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundToEven(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundToEven x)
	// result: (ROUNDSD [0] x)
	for {
		x := v_0
		v.reset(OpAMD64ROUNDSD)
		v.AuxInt = int8ToAuxInt(0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundWithPrecisionFloat32x16 [a] x)
	// result: (VRNDSCALEPS512 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS512)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundWithPrecisionFloat32x4 [a] x)
	// result: (VRNDSCALEPS128 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS128)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundWithPrecisionFloat32x8 [a] x)
	// result: (VRNDSCALEPS256 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS256)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundWithPrecisionFloat64x2 [a] x)
	// result: (VRNDSCALEPD128 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD128)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundWithPrecisionFloat64x4 [a] x)
	// result: (VRNDSCALEPD256 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD256)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRoundWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (RoundWithPrecisionFloat64x8 [a] x)
	// result: (VRNDSCALEPD512 [a+0] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD512)
		v.AuxInt = int8ToAuxInt(a + 0)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpRsh16Ux16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh16Ux16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRW <t> x y) (SBBLcarrymask <t> (CMPWconst y [16])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRW, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v2.AuxInt = int16ToAuxInt(16)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh16Ux16 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRW x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRW)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh16Ux32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh16Ux32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRW <t> x y) (SBBLcarrymask <t> (CMPLconst y [16])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRW, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(16)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh16Ux32 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRW x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRW)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh16Ux64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh16Ux64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRW <t> x y) (SBBLcarrymask <t> (CMPQconst y [16])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRW, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(16)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh16Ux64 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRW x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRW)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh16Ux8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh16Ux8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRW <t> x y) (SBBLcarrymask <t> (CMPBconst y [16])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRW, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v2.AuxInt = int8ToAuxInt(16)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh16Ux8 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRW x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRW)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh16x16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARW <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPWconst y [16])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARW)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v3.AuxInt = int16ToAuxInt(16)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh16x16 x y)
	// cond: shiftIsBounded(v)
	// result: (SARW x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARW)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh16x32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARW <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPLconst y [16])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARW)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v3.AuxInt = int32ToAuxInt(16)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh16x32 x y)
	// cond: shiftIsBounded(v)
	// result: (SARW x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARW)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh16x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh16x64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARW <t> x (ORQ <y.Type> y (NOTQ <y.Type> (SBBQcarrymask <y.Type> (CMPQconst y [16])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARW)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORQ, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTQ, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v3.AuxInt = int32ToAuxInt(16)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh16x64 x y)
	// cond: shiftIsBounded(v)
	// result: (SARW x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARW)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh16x8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARW <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPBconst y [16])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARW)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v3.AuxInt = int8ToAuxInt(16)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh16x8 x y)
	// cond: shiftIsBounded(v)
	// result: (SARW x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARW)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh32Ux16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh32Ux16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRL <t> x y) (SBBLcarrymask <t> (CMPWconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v2.AuxInt = int16ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh32Ux16 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh32Ux32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh32Ux32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRL <t> x y) (SBBLcarrymask <t> (CMPLconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh32Ux32 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh32Ux64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh32Ux64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRL <t> x y) (SBBLcarrymask <t> (CMPQconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh32Ux64 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh32Ux8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh32Ux8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRL <t> x y) (SBBLcarrymask <t> (CMPBconst y [32])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRL, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v2.AuxInt = int8ToAuxInt(32)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh32Ux8 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh32x16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARL <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPWconst y [32])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARL)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v3.AuxInt = int16ToAuxInt(32)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh32x16 x y)
	// cond: shiftIsBounded(v)
	// result: (SARL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh32x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh32x32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARL <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPLconst y [32])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARL)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v3.AuxInt = int32ToAuxInt(32)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh32x32 x y)
	// cond: shiftIsBounded(v)
	// result: (SARL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh32x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh32x64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARL <t> x (ORQ <y.Type> y (NOTQ <y.Type> (SBBQcarrymask <y.Type> (CMPQconst y [32])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARL)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORQ, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTQ, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v3.AuxInt = int32ToAuxInt(32)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh32x64 x y)
	// cond: shiftIsBounded(v)
	// result: (SARL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh32x8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARL <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPBconst y [32])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARL)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v3.AuxInt = int8ToAuxInt(32)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh32x8 x y)
	// cond: shiftIsBounded(v)
	// result: (SARL x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARL)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh64Ux16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh64Ux16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDQ (SHRQ <t> x y) (SBBQcarrymask <t> (CMPWconst y [64])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDQ)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRQ, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v2.AuxInt = int16ToAuxInt(64)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh64Ux16 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh64Ux32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh64Ux32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDQ (SHRQ <t> x y) (SBBQcarrymask <t> (CMPLconst y [64])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDQ)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRQ, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(64)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh64Ux32 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh64Ux64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh64Ux64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDQ (SHRQ <t> x y) (SBBQcarrymask <t> (CMPQconst y [64])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDQ)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRQ, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(64)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh64Ux64 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh64Ux8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh64Ux8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDQ (SHRQ <t> x y) (SBBQcarrymask <t> (CMPBconst y [64])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDQ)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRQ, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v2.AuxInt = int8ToAuxInt(64)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh64Ux8 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh64x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh64x16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARQ <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPWconst y [64])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v3.AuxInt = int16ToAuxInt(64)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh64x16 x y)
	// cond: shiftIsBounded(v)
	// result: (SARQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh64x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh64x32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARQ <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPLconst y [64])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v3.AuxInt = int32ToAuxInt(64)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh64x32 x y)
	// cond: shiftIsBounded(v)
	// result: (SARQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh64x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh64x64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARQ <t> x (ORQ <y.Type> y (NOTQ <y.Type> (SBBQcarrymask <y.Type> (CMPQconst y [64])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORQ, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTQ, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v3.AuxInt = int32ToAuxInt(64)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh64x64 x y)
	// cond: shiftIsBounded(v)
	// result: (SARQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh64x8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARQ <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPBconst y [64])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v3.AuxInt = int8ToAuxInt(64)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh64x8 x y)
	// cond: shiftIsBounded(v)
	// result: (SARQ x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARQ)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh8Ux16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh8Ux16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRB <t> x y) (SBBLcarrymask <t> (CMPWconst y [8])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRB, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v2.AuxInt = int16ToAuxInt(8)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh8Ux16 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRB x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRB)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh8Ux32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh8Ux32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRB <t> x y) (SBBLcarrymask <t> (CMPLconst y [8])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRB, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(8)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh8Ux32 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRB x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRB)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh8Ux64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh8Ux64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRB <t> x y) (SBBLcarrymask <t> (CMPQconst y [8])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRB, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v2.AuxInt = int32ToAuxInt(8)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh8Ux64 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRB x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRB)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh8Ux8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh8Ux8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (ANDL (SHRB <t> x y) (SBBLcarrymask <t> (CMPBconst y [8])))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64ANDL)
		v0 := b.NewValue0(v.Pos, OpAMD64SHRB, t)
		v0.AddArg2(x, y)
		v1 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, t)
		v2 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v2.AuxInt = int8ToAuxInt(8)
		v2.AddArg(y)
		v1.AddArg(v2)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Rsh8Ux8 x y)
	// cond: shiftIsBounded(v)
	// result: (SHRB x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SHRB)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh8x16 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARB <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPWconst y [8])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARB)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPWconst, types.TypeFlags)
		v3.AuxInt = int16ToAuxInt(8)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh8x16 x y)
	// cond: shiftIsBounded(v)
	// result: (SARB x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARB)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh8x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh8x32 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARB <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPLconst y [8])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARB)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPLconst, types.TypeFlags)
		v3.AuxInt = int32ToAuxInt(8)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh8x32 x y)
	// cond: shiftIsBounded(v)
	// result: (SARB x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARB)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh8x64(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh8x64 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARB <t> x (ORQ <y.Type> y (NOTQ <y.Type> (SBBQcarrymask <y.Type> (CMPQconst y [8])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARB)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORQ, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTQ, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPQconst, types.TypeFlags)
		v3.AuxInt = int32ToAuxInt(8)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh8x64 x y)
	// cond: shiftIsBounded(v)
	// result: (SARB x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARB)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpRsh8x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	// match: (Rsh8x8 <t> x y)
	// cond: !shiftIsBounded(v)
	// result: (SARB <t> x (ORL <y.Type> y (NOTL <y.Type> (SBBLcarrymask <y.Type> (CMPBconst y [8])))))
	for {
		t := v.Type
		x := v_0
		y := v_1
		if !(!shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARB)
		v.Type = t
		v0 := b.NewValue0(v.Pos, OpAMD64ORL, y.Type)
		v1 := b.NewValue0(v.Pos, OpAMD64NOTL, y.Type)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBLcarrymask, y.Type)
		v3 := b.NewValue0(v.Pos, OpAMD64CMPBconst, types.TypeFlags)
		v3.AuxInt = int8ToAuxInt(8)
		v3.AddArg(y)
		v2.AddArg(v3)
		v1.AddArg(v2)
		v0.AddArg2(y, v1)
		v.AddArg2(x, v0)
		return true
	}
	// match: (Rsh8x8 x y)
	// cond: shiftIsBounded(v)
	// result: (SARB x y)
	for {
		x := v_0
		y := v_1
		if !(shiftIsBounded(v)) {
			break
		}
		v.reset(OpAMD64SARB)
		v.AddArg2(x, y)
		return true
	}
	return false
}
func rewriteValueAMD64_OpSelect0(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Select0 (Mul64uover x y))
	// result: (Select0 <typ.UInt64> (MULQU x y))
	for {
		if v_0.Op != OpMul64uover {
			break
		}
		y := v_0.Args[1]
		x := v_0.Args[0]
		v.reset(OpSelect0)
		v.Type = typ.UInt64
		v0 := b.NewValue0(v.Pos, OpAMD64MULQU, types.NewTuple(typ.UInt64, types.TypeFlags))
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
	// match: (Select0 (Mul32uover x y))
	// result: (Select0 <typ.UInt32> (MULLU x y))
	for {
		if v_0.Op != OpMul32uover {
			break
		}
		y := v_0.Args[1]
		x := v_0.Args[0]
		v.reset(OpSelect0)
		v.Type = typ.UInt32
		v0 := b.NewValue0(v.Pos, OpAMD64MULLU, types.NewTuple(typ.UInt32, types.TypeFlags))
		v0.AddArg2(x, y)
		v.AddArg(v0)
		return true
	}
	// match: (Select0 (Add64carry x y c))
	// result: (Select0 <typ.UInt64> (ADCQ x y (Select1 <types.TypeFlags> (NEGLflags c))))
	for {
		if v_0.Op != OpAdd64carry {
			break
		}
		c := v_0.Args[2]
		x := v_0.Args[0]
		y := v_0.Args[1]
		v.reset(OpSelect0)
		v.Type = typ.UInt64
		v0 := b.NewValue0(v.Pos, OpAMD64ADCQ, types.NewTuple(typ.UInt64, types.TypeFlags))
		v1 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
		v2 := b.NewValue0(v.Pos, OpAMD64NEGLflags, types.NewTuple(typ.UInt32, types.TypeFlags))
		v2.AddArg(c)
		v1.AddArg(v2)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
	// match: (Select0 (Sub64borrow x y c))
	// result: (Select0 <typ.UInt64> (SBBQ x y (Select1 <types.TypeFlags> (NEGLflags c))))
	for {
		if v_0.Op != OpSub64borrow {
			break
		}
		c := v_0.Args[2]
		x := v_0.Args[0]
		y := v_0.Args[1]
		v.reset(OpSelect0)
		v.Type = typ.UInt64
		v0 := b.NewValue0(v.Pos, OpAMD64SBBQ, types.NewTuple(typ.UInt64, types.TypeFlags))
		v1 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
		v2 := b.NewValue0(v.Pos, OpAMD64NEGLflags, types.NewTuple(typ.UInt32, types.TypeFlags))
		v2.AddArg(c)
		v1.AddArg(v2)
		v0.AddArg3(x, y, v1)
		v.AddArg(v0)
		return true
	}
	// match: (Select0 <t> (AddTupleFirst32 val tuple))
	// result: (ADDL val (Select0 <t> tuple))
	for {
		t := v.Type
		if v_0.Op != OpAMD64AddTupleFirst32 {
			break
		}
		tuple := v_0.Args[1]
		val := v_0.Args[0]
		v.reset(OpAMD64ADDL)
		v0 := b.NewValue0(v.Pos, OpSelect0, t)
		v0.AddArg(tuple)
		v.AddArg2(val, v0)
		return true
	}
	// match: (Select0 <t> (AddTupleFirst64 val tuple))
	// result: (ADDQ val (Select0 <t> tuple))
	for {
		t := v.Type
		if v_0.Op != OpAMD64AddTupleFirst64 {
			break
		}
		tuple := v_0.Args[1]
		val := v_0.Args[0]
		v.reset(OpAMD64ADDQ)
		v0 := b.NewValue0(v.Pos, OpSelect0, t)
		v0.AddArg(tuple)
		v.AddArg2(val, v0)
		return true
	}
	// match: (Select0 a:(ADDQconstflags [c] x))
	// cond: a.Uses == 1
	// result: (ADDQconst [c] x)
	for {
		a := v_0
		if a.Op != OpAMD64ADDQconstflags {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		x := a.Args[0]
		if !(a.Uses == 1) {
			break
		}
		v.reset(OpAMD64ADDQconst)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	// match: (Select0 a:(ADDLconstflags [c] x))
	// cond: a.Uses == 1
	// result: (ADDLconst [c] x)
	for {
		a := v_0
		if a.Op != OpAMD64ADDLconstflags {
			break
		}
		c := auxIntToInt32(a.AuxInt)
		x := a.Args[0]
		if !(a.Uses == 1) {
			break
		}
		v.reset(OpAMD64ADDLconst)
		v.AuxInt = int32ToAuxInt(c)
		v.AddArg(x)
		return true
	}
	return false
}
func rewriteValueAMD64_OpSelect1(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Select1 (Mul64uover x y))
	// result: (SETO (Select1 <types.TypeFlags> (MULQU x y)))
	for {
		if v_0.Op != OpMul64uover {
			break
		}
		y := v_0.Args[1]
		x := v_0.Args[0]
		v.reset(OpAMD64SETO)
		v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
		v1 := b.NewValue0(v.Pos, OpAMD64MULQU, types.NewTuple(typ.UInt64, types.TypeFlags))
		v1.AddArg2(x, y)
		v0.AddArg(v1)
		v.AddArg(v0)
		return true
	}
	// match: (Select1 (Mul32uover x y))
	// result: (SETO (Select1 <types.TypeFlags> (MULLU x y)))
	for {
		if v_0.Op != OpMul32uover {
			break
		}
		y := v_0.Args[1]
		x := v_0.Args[0]
		v.reset(OpAMD64SETO)
		v0 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
		v1 := b.NewValue0(v.Pos, OpAMD64MULLU, types.NewTuple(typ.UInt32, types.TypeFlags))
		v1.AddArg2(x, y)
		v0.AddArg(v1)
		v.AddArg(v0)
		return true
	}
	// match: (Select1 (Add64carry x y c))
	// result: (NEGQ <typ.UInt64> (SBBQcarrymask <typ.UInt64> (Select1 <types.TypeFlags> (ADCQ x y (Select1 <types.TypeFlags> (NEGLflags c))))))
	for {
		if v_0.Op != OpAdd64carry {
			break
		}
		c := v_0.Args[2]
		x := v_0.Args[0]
		y := v_0.Args[1]
		v.reset(OpAMD64NEGQ)
		v.Type = typ.UInt64
		v0 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, typ.UInt64)
		v1 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
		v2 := b.NewValue0(v.Pos, OpAMD64ADCQ, types.NewTuple(typ.UInt64, types.TypeFlags))
		v3 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
		v4 := b.NewValue0(v.Pos, OpAMD64NEGLflags, types.NewTuple(typ.UInt32, types.TypeFlags))
		v4.AddArg(c)
		v3.AddArg(v4)
		v2.AddArg3(x, y, v3)
		v1.AddArg(v2)
		v0.AddArg(v1)
		v.AddArg(v0)
		return true
	}
	// match: (Select1 (Sub64borrow x y c))
	// result: (NEGQ <typ.UInt64> (SBBQcarrymask <typ.UInt64> (Select1 <types.TypeFlags> (SBBQ x y (Select1 <types.TypeFlags> (NEGLflags c))))))
	for {
		if v_0.Op != OpSub64borrow {
			break
		}
		c := v_0.Args[2]
		x := v_0.Args[0]
		y := v_0.Args[1]
		v.reset(OpAMD64NEGQ)
		v.Type = typ.UInt64
		v0 := b.NewValue0(v.Pos, OpAMD64SBBQcarrymask, typ.UInt64)
		v1 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
		v2 := b.NewValue0(v.Pos, OpAMD64SBBQ, types.NewTuple(typ.UInt64, types.TypeFlags))
		v3 := b.NewValue0(v.Pos, OpSelect1, types.TypeFlags)
		v4 := b.NewValue0(v.Pos, OpAMD64NEGLflags, types.NewTuple(typ.UInt32, types.TypeFlags))
		v4.AddArg(c)
		v3.AddArg(v4)
		v2.AddArg3(x, y, v3)
		v1.AddArg(v2)
		v0.AddArg(v1)
		v.AddArg(v0)
		return true
	}
	// match: (Select1 (NEGLflags (MOVQconst [0])))
	// result: (FlagEQ)
	for {
		if v_0.Op != OpAMD64NEGLflags {
			break
		}
		v_0_0 := v_0.Args[0]
		if v_0_0.Op != OpAMD64MOVQconst || auxIntToInt64(v_0_0.AuxInt) != 0 {
			break
		}
		v.reset(OpAMD64FlagEQ)
		return true
	}
	// match: (Select1 (NEGLflags (NEGQ (SBBQcarrymask x))))
	// result: x
	for {
		if v_0.Op != OpAMD64NEGLflags {
			break
		}
		v_0_0 := v_0.Args[0]
		if v_0_0.Op != OpAMD64NEGQ {
			break
		}
		v_0_0_0 := v_0_0.Args[0]
		if v_0_0_0.Op != OpAMD64SBBQcarrymask {
			break
		}
		x := v_0_0_0.Args[0]
		v.copyOf(x)
		return true
	}
	// match: (Select1 (AddTupleFirst32 _ tuple))
	// result: (Select1 tuple)
	for {
		if v_0.Op != OpAMD64AddTupleFirst32 {
			break
		}
		tuple := v_0.Args[1]
		v.reset(OpSelect1)
		v.AddArg(tuple)
		return true
	}
	// match: (Select1 (AddTupleFirst64 _ tuple))
	// result: (Select1 tuple)
	for {
		if v_0.Op != OpAMD64AddTupleFirst64 {
			break
		}
		tuple := v_0.Args[1]
		v.reset(OpSelect1)
		v.AddArg(tuple)
		return true
	}
	// match: (Select1 a:(LoweredAtomicAnd64 ptr val mem))
	// cond: a.Uses == 1 && clobber(a)
	// result: (ANDQlock ptr val mem)
	for {
		a := v_0
		if a.Op != OpAMD64LoweredAtomicAnd64 {
			break
		}
		mem := a.Args[2]
		ptr := a.Args[0]
		val := a.Args[1]
		if !(a.Uses == 1 && clobber(a)) {
			break
		}
		v.reset(OpAMD64ANDQlock)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (Select1 a:(LoweredAtomicAnd32 ptr val mem))
	// cond: a.Uses == 1 && clobber(a)
	// result: (ANDLlock ptr val mem)
	for {
		a := v_0
		if a.Op != OpAMD64LoweredAtomicAnd32 {
			break
		}
		mem := a.Args[2]
		ptr := a.Args[0]
		val := a.Args[1]
		if !(a.Uses == 1 && clobber(a)) {
			break
		}
		v.reset(OpAMD64ANDLlock)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (Select1 a:(LoweredAtomicOr64 ptr val mem))
	// cond: a.Uses == 1 && clobber(a)
	// result: (ORQlock ptr val mem)
	for {
		a := v_0
		if a.Op != OpAMD64LoweredAtomicOr64 {
			break
		}
		mem := a.Args[2]
		ptr := a.Args[0]
		val := a.Args[1]
		if !(a.Uses == 1 && clobber(a)) {
			break
		}
		v.reset(OpAMD64ORQlock)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (Select1 a:(LoweredAtomicOr32 ptr val mem))
	// cond: a.Uses == 1 && clobber(a)
	// result: (ORLlock ptr val mem)
	for {
		a := v_0
		if a.Op != OpAMD64LoweredAtomicOr32 {
			break
		}
		mem := a.Args[2]
		ptr := a.Args[0]
		val := a.Args[1]
		if !(a.Uses == 1 && clobber(a)) {
			break
		}
		v.reset(OpAMD64ORLlock)
		v.AddArg3(ptr, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpSelectN(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	config := b.Func.Config
	// match: (SelectN [0] call:(CALLstatic {sym} s1:(MOVQstoreconst _ [sc] s2:(MOVQstore _ src s3:(MOVQstore _ dst mem)))))
	// cond: sc.Val64() >= 0 && isSameCall(sym, "runtime.memmove") && s1.Uses == 1 && s2.Uses == 1 && s3.Uses == 1 && isInlinableMemmove(dst, src, sc.Val64(), config) && clobber(s1, s2, s3, call)
	// result: (Move [sc.Val64()] dst src mem)
	for {
		if auxIntToInt64(v.AuxInt) != 0 {
			break
		}
		call := v_0
		if call.Op != OpAMD64CALLstatic || len(call.Args) != 1 {
			break
		}
		sym := auxToCall(call.Aux)
		s1 := call.Args[0]
		if s1.Op != OpAMD64MOVQstoreconst {
			break
		}
		sc := auxIntToValAndOff(s1.AuxInt)
		_ = s1.Args[1]
		s2 := s1.Args[1]
		if s2.Op != OpAMD64MOVQstore {
			break
		}
		_ = s2.Args[2]
		src := s2.Args[1]
		s3 := s2.Args[2]
		if s3.Op != OpAMD64MOVQstore {
			break
		}
		mem := s3.Args[2]
		dst := s3.Args[1]
		if !(sc.Val64() >= 0 && isSameCall(sym, "runtime.memmove") && s1.Uses == 1 && s2.Uses == 1 && s3.Uses == 1 && isInlinableMemmove(dst, src, sc.Val64(), config) && clobber(s1, s2, s3, call)) {
			break
		}
		v.reset(OpMove)
		v.AuxInt = int64ToAuxInt(sc.Val64())
		v.AddArg3(dst, src, mem)
		return true
	}
	// match: (SelectN [0] call:(CALLstatic {sym} dst src (MOVQconst [sz]) mem))
	// cond: sz >= 0 && isSameCall(sym, "runtime.memmove") && call.Uses == 1 && isInlinableMemmove(dst, src, sz, config) && clobber(call)
	// result: (Move [sz] dst src mem)
	for {
		if auxIntToInt64(v.AuxInt) != 0 {
			break
		}
		call := v_0
		if call.Op != OpAMD64CALLstatic || len(call.Args) != 4 {
			break
		}
		sym := auxToCall(call.Aux)
		mem := call.Args[3]
		dst := call.Args[0]
		src := call.Args[1]
		call_2 := call.Args[2]
		if call_2.Op != OpAMD64MOVQconst {
			break
		}
		sz := auxIntToInt64(call_2.AuxInt)
		if !(sz >= 0 && isSameCall(sym, "runtime.memmove") && call.Uses == 1 && isInlinableMemmove(dst, src, sz, config) && clobber(call)) {
			break
		}
		v.reset(OpMove)
		v.AuxInt = int64ToAuxInt(sz)
		v.AddArg3(dst, src, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpSetElemInt16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SetElemInt16x8 [a] x y)
	// result: (VPINSRW128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPINSRW128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpSetElemInt32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SetElemInt32x4 [a] x y)
	// result: (VPINSRD128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPINSRD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpSetElemInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SetElemInt64x2 [a] x y)
	// result: (VPINSRQ128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPINSRQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpSetElemInt8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SetElemInt8x16 [a] x y)
	// result: (VPINSRB128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPINSRB128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpSetElemUint16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SetElemUint16x8 [a] x y)
	// result: (VPINSRW128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPINSRW128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpSetElemUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SetElemUint32x4 [a] x y)
	// result: (VPINSRD128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPINSRD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpSetElemUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SetElemUint64x2 [a] x y)
	// result: (VPINSRQ128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPINSRQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpSetElemUint8x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (SetElemUint8x16 [a] x y)
	// result: (VPINSRB128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPINSRB128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromInt16x16 [a] x y)
	// result: (VPSHLDW256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDW256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromInt16x32 [a] x y)
	// result: (VPSHLDW512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDW512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromInt16x8 [a] x y)
	// result: (VPSHLDW128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDW128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromInt32x16 [a] x y)
	// result: (VPSHLDD512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDD512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromInt32x4 [a] x y)
	// result: (VPSHLDD128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromInt32x8 [a] x y)
	// result: (VPSHLDD256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDD256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromInt64x2 [a] x y)
	// result: (VPSHLDQ128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromInt64x4 [a] x y)
	// result: (VPSHLDQ256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDQ256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromInt64x8 [a] x y)
	// result: (VPSHLDQ512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDQ512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromUint16x16 [a] x y)
	// result: (VPSHLDW256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDW256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromUint16x32 [a] x y)
	// result: (VPSHLDW512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDW512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromUint16x8 [a] x y)
	// result: (VPSHLDW128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDW128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromUint32x16 [a] x y)
	// result: (VPSHLDD512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDD512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromUint32x4 [a] x y)
	// result: (VPSHLDD128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromUint32x8 [a] x y)
	// result: (VPSHLDD256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDD256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromUint64x2 [a] x y)
	// result: (VPSHLDQ128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromUint64x4 [a] x y)
	// result: (VPSHLDQ256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDQ256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllLeftAndFillUpperFromUint64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllLeftAndFillUpperFromUint64x8 [a] x y)
	// result: (VPSHLDQ512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHLDQ512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromInt16x16 [a] x y)
	// result: (VPSHRDW256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDW256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromInt16x32 [a] x y)
	// result: (VPSHRDW512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDW512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromInt16x8 [a] x y)
	// result: (VPSHRDW128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDW128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromInt32x16 [a] x y)
	// result: (VPSHRDD512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDD512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromInt32x4 [a] x y)
	// result: (VPSHRDD128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromInt32x8 [a] x y)
	// result: (VPSHRDD256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDD256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromInt64x2 [a] x y)
	// result: (VPSHRDQ128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromInt64x4 [a] x y)
	// result: (VPSHRDQ256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDQ256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromInt64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromInt64x8 [a] x y)
	// result: (VPSHRDQ512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDQ512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint16x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromUint16x16 [a] x y)
	// result: (VPSHRDW256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDW256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint16x32(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromUint16x32 [a] x y)
	// result: (VPSHRDW512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDW512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint16x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromUint16x8 [a] x y)
	// result: (VPSHRDW128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDW128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint32x16(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromUint32x16 [a] x y)
	// result: (VPSHRDD512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDD512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint32x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromUint32x4 [a] x y)
	// result: (VPSHRDD128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDD128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint32x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromUint32x8 [a] x y)
	// result: (VPSHRDD256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDD256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint64x2(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromUint64x2 [a] x y)
	// result: (VPSHRDQ128 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDQ128)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint64x4(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromUint64x4 [a] x y)
	// result: (VPSHRDQ256 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDQ256)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpShiftAllRightAndFillUpperFromUint64x8(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (ShiftAllRightAndFillUpperFromUint64x8 [a] x y)
	// result: (VPSHRDQ512 [a] x y)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		y := v_1
		v.reset(OpAMD64VPSHRDQ512)
		v.AuxInt = int8ToAuxInt(a)
		v.AddArg2(x, y)
		return true
	}
}
func rewriteValueAMD64_OpSlicemask(v *Value) bool {
	v_0 := v.Args[0]
	b := v.Block
	// match: (Slicemask <t> x)
	// result: (SARQconst (NEGQ <t> x) [63])
	for {
		t := v.Type
		x := v_0
		v.reset(OpAMD64SARQconst)
		v.AuxInt = int8ToAuxInt(63)
		v0 := b.NewValue0(v.Pos, OpAMD64NEGQ, t)
		v0.AddArg(x)
		v.AddArg(v0)
		return true
	}
}
func rewriteValueAMD64_OpSpectreIndex(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SpectreIndex <t> x y)
	// result: (CMOVQCC x (MOVQconst [0]) (CMPQ x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64CMOVQCC)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, typ.UInt64)
		v0.AuxInt = int64ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v1.AddArg2(x, y)
		v.AddArg3(x, v0, v1)
		return true
	}
}
func rewriteValueAMD64_OpSpectreSliceIndex(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (SpectreSliceIndex <t> x y)
	// result: (CMOVQHI x (MOVQconst [0]) (CMPQ x y))
	for {
		x := v_0
		y := v_1
		v.reset(OpAMD64CMOVQHI)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, typ.UInt64)
		v0.AuxInt = int64ToAuxInt(0)
		v1 := b.NewValue0(v.Pos, OpAMD64CMPQ, types.TypeFlags)
		v1.AddArg2(x, y)
		v.AddArg3(x, v0, v1)
		return true
	}
}
func rewriteValueAMD64_OpStore(v *Value) bool {
	v_2 := v.Args[2]
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	// match: (Store {t} ptr val mem)
	// cond: t.Size() == 8 && t.IsFloat()
	// result: (MOVSDstore ptr val mem)
	for {
		t := auxToType(v.Aux)
		ptr := v_0
		val := v_1
		mem := v_2
		if !(t.Size() == 8 && t.IsFloat()) {
			break
		}
		v.reset(OpAMD64MOVSDstore)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (Store {t} ptr val mem)
	// cond: t.Size() == 4 && t.IsFloat()
	// result: (MOVSSstore ptr val mem)
	for {
		t := auxToType(v.Aux)
		ptr := v_0
		val := v_1
		mem := v_2
		if !(t.Size() == 4 && t.IsFloat()) {
			break
		}
		v.reset(OpAMD64MOVSSstore)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (Store {t} ptr val mem)
	// cond: t.Size() == 8 && !t.IsFloat()
	// result: (MOVQstore ptr val mem)
	for {
		t := auxToType(v.Aux)
		ptr := v_0
		val := v_1
		mem := v_2
		if !(t.Size() == 8 && !t.IsFloat()) {
			break
		}
		v.reset(OpAMD64MOVQstore)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (Store {t} ptr val mem)
	// cond: t.Size() == 4 && !t.IsFloat()
	// result: (MOVLstore ptr val mem)
	for {
		t := auxToType(v.Aux)
		ptr := v_0
		val := v_1
		mem := v_2
		if !(t.Size() == 4 && !t.IsFloat()) {
			break
		}
		v.reset(OpAMD64MOVLstore)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (Store {t} ptr val mem)
	// cond: t.Size() == 2
	// result: (MOVWstore ptr val mem)
	for {
		t := auxToType(v.Aux)
		ptr := v_0
		val := v_1
		mem := v_2
		if !(t.Size() == 2) {
			break
		}
		v.reset(OpAMD64MOVWstore)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (Store {t} ptr val mem)
	// cond: t.Size() == 1
	// result: (MOVBstore ptr val mem)
	for {
		t := auxToType(v.Aux)
		ptr := v_0
		val := v_1
		mem := v_2
		if !(t.Size() == 1) {
			break
		}
		v.reset(OpAMD64MOVBstore)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (Store {t} ptr val mem)
	// cond: t.Size() == 16
	// result: (VMOVDQUstore128 ptr val mem)
	for {
		t := auxToType(v.Aux)
		ptr := v_0
		val := v_1
		mem := v_2
		if !(t.Size() == 16) {
			break
		}
		v.reset(OpAMD64VMOVDQUstore128)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (Store {t} ptr val mem)
	// cond: t.Size() == 32
	// result: (VMOVDQUstore256 ptr val mem)
	for {
		t := auxToType(v.Aux)
		ptr := v_0
		val := v_1
		mem := v_2
		if !(t.Size() == 32) {
			break
		}
		v.reset(OpAMD64VMOVDQUstore256)
		v.AddArg3(ptr, val, mem)
		return true
	}
	// match: (Store {t} ptr val mem)
	// cond: t.Size() == 64
	// result: (VMOVDQUstore512 ptr val mem)
	for {
		t := auxToType(v.Aux)
		ptr := v_0
		val := v_1
		mem := v_2
		if !(t.Size() == 64) {
			break
		}
		v.reset(OpAMD64VMOVDQUstore512)
		v.AddArg3(ptr, val, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpTrunc(v *Value) bool {
	v_0 := v.Args[0]
	// match: (Trunc x)
	// result: (ROUNDSD [3] x)
	for {
		x := v_0
		v.reset(OpAMD64ROUNDSD)
		v.AuxInt = int8ToAuxInt(3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncFloat32x4 x)
	// result: (VROUNDPS128 [3] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPS128)
		v.AuxInt = int8ToAuxInt(3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncFloat32x8 x)
	// result: (VROUNDPS256 [3] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPS256)
		v.AuxInt = int8ToAuxInt(3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncFloat64x2 x)
	// result: (VROUNDPD128 [3] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPD128)
		v.AuxInt = int8ToAuxInt(3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncFloat64x4 x)
	// result: (VROUNDPD256 [3] x)
	for {
		x := v_0
		v.reset(OpAMD64VROUNDPD256)
		v.AuxInt = int8ToAuxInt(3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncSuppressExceptionWithPrecisionFloat32x16 [a] x)
	// result: (VRNDSCALEPS512 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS512)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncSuppressExceptionWithPrecisionFloat32x4 [a] x)
	// result: (VRNDSCALEPS128 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS128)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncSuppressExceptionWithPrecisionFloat32x8 [a] x)
	// result: (VRNDSCALEPS256 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS256)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncSuppressExceptionWithPrecisionFloat64x2 [a] x)
	// result: (VRNDSCALEPD128 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD128)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncSuppressExceptionWithPrecisionFloat64x4 [a] x)
	// result: (VRNDSCALEPD256 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD256)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncSuppressExceptionWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncSuppressExceptionWithPrecisionFloat64x8 [a] x)
	// result: (VRNDSCALEPD512 [a+11] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD512)
		v.AuxInt = int8ToAuxInt(a + 11)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncWithPrecisionFloat32x16(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncWithPrecisionFloat32x16 [a] x)
	// result: (VRNDSCALEPS512 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS512)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncWithPrecisionFloat32x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncWithPrecisionFloat32x4 [a] x)
	// result: (VRNDSCALEPS128 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS128)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncWithPrecisionFloat32x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncWithPrecisionFloat32x8 [a] x)
	// result: (VRNDSCALEPS256 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPS256)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncWithPrecisionFloat64x2(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncWithPrecisionFloat64x2 [a] x)
	// result: (VRNDSCALEPD128 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD128)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncWithPrecisionFloat64x4(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncWithPrecisionFloat64x4 [a] x)
	// result: (VRNDSCALEPD256 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD256)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpTruncWithPrecisionFloat64x8(v *Value) bool {
	v_0 := v.Args[0]
	// match: (TruncWithPrecisionFloat64x8 [a] x)
	// result: (VRNDSCALEPD512 [a+3] x)
	for {
		a := auxIntToInt8(v.AuxInt)
		x := v_0
		v.reset(OpAMD64VRNDSCALEPD512)
		v.AuxInt = int8ToAuxInt(a + 3)
		v.AddArg(x)
		return true
	}
}
func rewriteValueAMD64_OpZero(v *Value) bool {
	v_1 := v.Args[1]
	v_0 := v.Args[0]
	b := v.Block
	typ := &b.Func.Config.Types
	// match: (Zero [0] _ mem)
	// result: mem
	for {
		if auxIntToInt64(v.AuxInt) != 0 {
			break
		}
		mem := v_1
		v.copyOf(mem)
		return true
	}
	// match: (Zero [1] destptr mem)
	// result: (MOVBstoreconst [makeValAndOff(0,0)] destptr mem)
	for {
		if auxIntToInt64(v.AuxInt) != 1 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVBstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v.AddArg2(destptr, mem)
		return true
	}
	// match: (Zero [2] destptr mem)
	// result: (MOVWstoreconst [makeValAndOff(0,0)] destptr mem)
	for {
		if auxIntToInt64(v.AuxInt) != 2 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVWstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v.AddArg2(destptr, mem)
		return true
	}
	// match: (Zero [4] destptr mem)
	// result: (MOVLstoreconst [makeValAndOff(0,0)] destptr mem)
	for {
		if auxIntToInt64(v.AuxInt) != 4 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVLstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v.AddArg2(destptr, mem)
		return true
	}
	// match: (Zero [8] destptr mem)
	// result: (MOVQstoreconst [makeValAndOff(0,0)] destptr mem)
	for {
		if auxIntToInt64(v.AuxInt) != 8 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVQstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v.AddArg2(destptr, mem)
		return true
	}
	// match: (Zero [3] destptr mem)
	// result: (MOVBstoreconst [makeValAndOff(0,2)] destptr (MOVWstoreconst [makeValAndOff(0,0)] destptr mem))
	for {
		if auxIntToInt64(v.AuxInt) != 3 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVBstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 2))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVWstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v0.AddArg2(destptr, mem)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [5] destptr mem)
	// result: (MOVBstoreconst [makeValAndOff(0,4)] destptr (MOVLstoreconst [makeValAndOff(0,0)] destptr mem))
	for {
		if auxIntToInt64(v.AuxInt) != 5 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVBstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 4))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v0.AddArg2(destptr, mem)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [6] destptr mem)
	// result: (MOVWstoreconst [makeValAndOff(0,4)] destptr (MOVLstoreconst [makeValAndOff(0,0)] destptr mem))
	for {
		if auxIntToInt64(v.AuxInt) != 6 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVWstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 4))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v0.AddArg2(destptr, mem)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [7] destptr mem)
	// result: (MOVLstoreconst [makeValAndOff(0,3)] destptr (MOVLstoreconst [makeValAndOff(0,0)] destptr mem))
	for {
		if auxIntToInt64(v.AuxInt) != 7 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVLstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 3))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVLstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v0.AddArg2(destptr, mem)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [9] destptr mem)
	// result: (MOVBstoreconst [makeValAndOff(0,8)] destptr (MOVQstoreconst [makeValAndOff(0,0)] destptr mem))
	for {
		if auxIntToInt64(v.AuxInt) != 9 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVBstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 8))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v0.AddArg2(destptr, mem)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [10] destptr mem)
	// result: (MOVWstoreconst [makeValAndOff(0,8)] destptr (MOVQstoreconst [makeValAndOff(0,0)] destptr mem))
	for {
		if auxIntToInt64(v.AuxInt) != 10 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVWstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 8))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v0.AddArg2(destptr, mem)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [11] destptr mem)
	// result: (MOVLstoreconst [makeValAndOff(0,7)] destptr (MOVQstoreconst [makeValAndOff(0,0)] destptr mem))
	for {
		if auxIntToInt64(v.AuxInt) != 11 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVLstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 7))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v0.AddArg2(destptr, mem)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [12] destptr mem)
	// result: (MOVLstoreconst [makeValAndOff(0,8)] destptr (MOVQstoreconst [makeValAndOff(0,0)] destptr mem))
	for {
		if auxIntToInt64(v.AuxInt) != 12 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVLstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 8))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v0.AddArg2(destptr, mem)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [s] destptr mem)
	// cond: s > 12 && s < 16
	// result: (MOVQstoreconst [makeValAndOff(0,int32(s-8))] destptr (MOVQstoreconst [makeValAndOff(0,0)] destptr mem))
	for {
		s := auxIntToInt64(v.AuxInt)
		destptr := v_0
		mem := v_1
		if !(s > 12 && s < 16) {
			break
		}
		v.reset(OpAMD64MOVQstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, int32(s-8)))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v0.AddArg2(destptr, mem)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [s] destptr mem)
	// cond: s%16 != 0 && s > 16
	// result: (Zero [s-s%16] (OffPtr <destptr.Type> destptr [s%16]) (MOVOstoreconst [makeValAndOff(0,0)] destptr mem))
	for {
		s := auxIntToInt64(v.AuxInt)
		destptr := v_0
		mem := v_1
		if !(s%16 != 0 && s > 16) {
			break
		}
		v.reset(OpZero)
		v.AuxInt = int64ToAuxInt(s - s%16)
		v0 := b.NewValue0(v.Pos, OpOffPtr, destptr.Type)
		v0.AuxInt = int64ToAuxInt(s % 16)
		v0.AddArg(destptr)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVOstoreconst, types.TypeMem)
		v1.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v1.AddArg2(destptr, mem)
		v.AddArg2(v0, v1)
		return true
	}
	// match: (Zero [16] destptr mem)
	// result: (MOVOstoreconst [makeValAndOff(0,0)] destptr mem)
	for {
		if auxIntToInt64(v.AuxInt) != 16 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVOstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v.AddArg2(destptr, mem)
		return true
	}
	// match: (Zero [32] destptr mem)
	// result: (MOVOstoreconst [makeValAndOff(0,16)] destptr (MOVOstoreconst [makeValAndOff(0,0)] destptr mem))
	for {
		if auxIntToInt64(v.AuxInt) != 32 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVOstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 16))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVOstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v0.AddArg2(destptr, mem)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [48] destptr mem)
	// result: (MOVOstoreconst [makeValAndOff(0,32)] destptr (MOVOstoreconst [makeValAndOff(0,16)] destptr (MOVOstoreconst [makeValAndOff(0,0)] destptr mem)))
	for {
		if auxIntToInt64(v.AuxInt) != 48 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVOstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 32))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVOstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 16))
		v1 := b.NewValue0(v.Pos, OpAMD64MOVOstoreconst, types.TypeMem)
		v1.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v1.AddArg2(destptr, mem)
		v0.AddArg2(destptr, v1)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [64] destptr mem)
	// result: (MOVOstoreconst [makeValAndOff(0,48)] destptr (MOVOstoreconst [makeValAndOff(0,32)] destptr (MOVOstoreconst [makeValAndOff(0,16)] destptr (MOVOstoreconst [makeValAndOff(0,0)] destptr mem))))
	for {
		if auxIntToInt64(v.AuxInt) != 64 {
			break
		}
		destptr := v_0
		mem := v_1
		v.reset(OpAMD64MOVOstoreconst)
		v.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 48))
		v0 := b.NewValue0(v.Pos, OpAMD64MOVOstoreconst, types.TypeMem)
		v0.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 32))
		v1 := b.NewValue0(v.Pos, OpAMD64MOVOstoreconst, types.TypeMem)
		v1.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 16))
		v2 := b.NewValue0(v.Pos, OpAMD64MOVOstoreconst, types.TypeMem)
		v2.AuxInt = valAndOffToAuxInt(makeValAndOff(0, 0))
		v2.AddArg2(destptr, mem)
		v1.AddArg2(destptr, v2)
		v0.AddArg2(destptr, v1)
		v.AddArg2(destptr, v0)
		return true
	}
	// match: (Zero [s] destptr mem)
	// cond: s > 64 && s <= 1024 && s%16 == 0
	// result: (DUFFZERO [s] destptr mem)
	for {
		s := auxIntToInt64(v.AuxInt)
		destptr := v_0
		mem := v_1
		if !(s > 64 && s <= 1024 && s%16 == 0) {
			break
		}
		v.reset(OpAMD64DUFFZERO)
		v.AuxInt = int64ToAuxInt(s)
		v.AddArg2(destptr, mem)
		return true
	}
	// match: (Zero [s] destptr mem)
	// cond: s > 1024 && s%8 == 0
	// result: (REPSTOSQ destptr (MOVQconst [s/8]) (MOVQconst [0]) mem)
	for {
		s := auxIntToInt64(v.AuxInt)
		destptr := v_0
		mem := v_1
		if !(s > 1024 && s%8 == 0) {
			break
		}
		v.reset(OpAMD64REPSTOSQ)
		v0 := b.NewValue0(v.Pos, OpAMD64MOVQconst, typ.UInt64)
		v0.AuxInt = int64ToAuxInt(s / 8)
		v1 := b.NewValue0(v.Pos, OpAMD64MOVQconst, typ.UInt64)
		v1.AuxInt = int64ToAuxInt(0)
		v.AddArg4(destptr, v0, v1, mem)
		return true
	}
	return false
}
func rewriteValueAMD64_OpZeroSIMD(v *Value) bool {
	// match: (ZeroSIMD <t>)
	// cond: t.Size() == 16
	// result: (Zero128 <t>)
	for {
		t := v.Type
		if !(t.Size() == 16) {
			break
		}
		v.reset(OpAMD64Zero128)
		v.Type = t
		return true
	}
	// match: (ZeroSIMD <t>)
	// cond: t.Size() == 32
	// result: (Zero256 <t>)
	for {
		t := v.Type
		if !(t.Size() == 32) {
			break
		}
		v.reset(OpAMD64Zero256)
		v.Type = t
		return true
	}
	// match: (ZeroSIMD <t>)
	// cond: t.Size() == 64
	// result: (Zero512 <t>)
	for {
		t := v.Type
		if !(t.Size() == 64) {
			break
		}
		v.reset(OpAMD64Zero512)
		v.Type = t
		return true
	}
	return false
}
func rewriteBlockAMD64(b *Block) bool {
	typ := &b.Func.Config.Types
	switch b.Kind {
	case BlockAMD64EQ:
		// match: (EQ (TESTL (SHLL (MOVLconst [1]) x) y))
		// result: (UGE (BTL x y))
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				if v_0_0.Op != OpAMD64SHLL {
					continue
				}
				x := v_0_0.Args[1]
				v_0_0_0 := v_0_0.Args[0]
				if v_0_0_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_0_0_0.AuxInt) != 1 {
					continue
				}
				y := v_0_1
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTL, types.TypeFlags)
				v0.AddArg2(x, y)
				b.resetWithControl(BlockAMD64UGE, v0)
				return true
			}
			break
		}
		// match: (EQ (TESTQ (SHLQ (MOVQconst [1]) x) y))
		// result: (UGE (BTQ x y))
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				if v_0_0.Op != OpAMD64SHLQ {
					continue
				}
				x := v_0_0.Args[1]
				v_0_0_0 := v_0_0.Args[0]
				if v_0_0_0.Op != OpAMD64MOVQconst || auxIntToInt64(v_0_0_0.AuxInt) != 1 {
					continue
				}
				y := v_0_1
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQ, types.TypeFlags)
				v0.AddArg2(x, y)
				b.resetWithControl(BlockAMD64UGE, v0)
				return true
			}
			break
		}
		// match: (EQ (TESTLconst [c] x))
		// cond: isUint32PowerOfTwo(int64(c))
		// result: (UGE (BTLconst [int8(log32(c))] x))
		for b.Controls[0].Op == OpAMD64TESTLconst {
			v_0 := b.Controls[0]
			c := auxIntToInt32(v_0.AuxInt)
			x := v_0.Args[0]
			if !(isUint32PowerOfTwo(int64(c))) {
				break
			}
			v0 := b.NewValue0(v_0.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(int8(log32(c)))
			v0.AddArg(x)
			b.resetWithControl(BlockAMD64UGE, v0)
			return true
		}
		// match: (EQ (TESTQconst [c] x))
		// cond: isUint64PowerOfTwo(int64(c))
		// result: (UGE (BTQconst [int8(log32(c))] x))
		for b.Controls[0].Op == OpAMD64TESTQconst {
			v_0 := b.Controls[0]
			c := auxIntToInt32(v_0.AuxInt)
			x := v_0.Args[0]
			if !(isUint64PowerOfTwo(int64(c))) {
				break
			}
			v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(int8(log32(c)))
			v0.AddArg(x)
			b.resetWithControl(BlockAMD64UGE, v0)
			return true
		}
		// match: (EQ (TESTQ (MOVQconst [c]) x))
		// cond: isUint64PowerOfTwo(c)
		// result: (UGE (BTQconst [int8(log64(c))] x))
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				if v_0_0.Op != OpAMD64MOVQconst {
					continue
				}
				c := auxIntToInt64(v_0_0.AuxInt)
				x := v_0_1
				if !(isUint64PowerOfTwo(c)) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(int8(log64(c)))
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64UGE, v0)
				return true
			}
			break
		}
		// match: (EQ (TESTQ z1:(SHLQconst [63] (SHRQconst [63] x)) z2))
		// cond: z1==z2
		// result: (UGE (BTQconst [63] x))
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHLQconst || auxIntToInt8(z1.AuxInt) != 63 {
					continue
				}
				z1_0 := z1.Args[0]
				if z1_0.Op != OpAMD64SHRQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
					continue
				}
				x := z1_0.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(63)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64UGE, v0)
				return true
			}
			break
		}
		// match: (EQ (TESTL z1:(SHLLconst [31] (SHRQconst [31] x)) z2))
		// cond: z1==z2
		// result: (UGE (BTQconst [31] x))
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHLLconst || auxIntToInt8(z1.AuxInt) != 31 {
					continue
				}
				z1_0 := z1.Args[0]
				if z1_0.Op != OpAMD64SHRQconst || auxIntToInt8(z1_0.AuxInt) != 31 {
					continue
				}
				x := z1_0.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(31)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64UGE, v0)
				return true
			}
			break
		}
		// match: (EQ (TESTQ z1:(SHRQconst [63] (SHLQconst [63] x)) z2))
		// cond: z1==z2
		// result: (UGE (BTQconst [0] x))
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
					continue
				}
				z1_0 := z1.Args[0]
				if z1_0.Op != OpAMD64SHLQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
					continue
				}
				x := z1_0.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(0)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64UGE, v0)
				return true
			}
			break
		}
		// match: (EQ (TESTL z1:(SHRLconst [31] (SHLLconst [31] x)) z2))
		// cond: z1==z2
		// result: (UGE (BTLconst [0] x))
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
					continue
				}
				z1_0 := z1.Args[0]
				if z1_0.Op != OpAMD64SHLLconst || auxIntToInt8(z1_0.AuxInt) != 31 {
					continue
				}
				x := z1_0.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTLconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(0)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64UGE, v0)
				return true
			}
			break
		}
		// match: (EQ (TESTQ z1:(SHRQconst [63] x) z2))
		// cond: z1==z2
		// result: (UGE (BTQconst [63] x))
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
					continue
				}
				x := z1.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(63)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64UGE, v0)
				return true
			}
			break
		}
		// match: (EQ (TESTL z1:(SHRLconst [31] x) z2))
		// cond: z1==z2
		// result: (UGE (BTLconst [31] x))
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
					continue
				}
				x := z1.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTLconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(31)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64UGE, v0)
				return true
			}
			break
		}
		// match: (EQ (InvertFlags cmp) yes no)
		// result: (EQ cmp yes no)
		for b.Controls[0].Op == OpAMD64InvertFlags {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64EQ, cmp)
			return true
		}
		// match: (EQ (FlagEQ) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagEQ {
			b.Reset(BlockFirst)
			return true
		}
		// match: (EQ (FlagLT_ULT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagLT_ULT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (EQ (FlagLT_UGT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagLT_UGT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (EQ (FlagGT_ULT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagGT_ULT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (EQ (FlagGT_UGT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagGT_UGT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (EQ (TESTQ s:(Select0 blsr:(BLSRQ _)) s) yes no)
		// result: (EQ (Select1 <types.TypeFlags> blsr) yes no)
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				s := v_0_0
				if s.Op != OpSelect0 {
					continue
				}
				blsr := s.Args[0]
				if blsr.Op != OpAMD64BLSRQ || s != v_0_1 {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpSelect1, types.TypeFlags)
				v0.AddArg(blsr)
				b.resetWithControl(BlockAMD64EQ, v0)
				return true
			}
			break
		}
		// match: (EQ (TESTL s:(Select0 blsr:(BLSRL _)) s) yes no)
		// result: (EQ (Select1 <types.TypeFlags> blsr) yes no)
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				s := v_0_0
				if s.Op != OpSelect0 {
					continue
				}
				blsr := s.Args[0]
				if blsr.Op != OpAMD64BLSRL || s != v_0_1 {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpSelect1, types.TypeFlags)
				v0.AddArg(blsr)
				b.resetWithControl(BlockAMD64EQ, v0)
				return true
			}
			break
		}
		// match: (EQ t:(TESTQ a:(ADDQconst [c] x) a))
		// cond: t.Uses == 1 && flagify(a)
		// result: (EQ (Select1 <types.TypeFlags> a.Args[0]))
		for b.Controls[0].Op == OpAMD64TESTQ {
			t := b.Controls[0]
			_ = t.Args[1]
			t_0 := t.Args[0]
			t_1 := t.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, t_0, t_1 = _i0+1, t_1, t_0 {
				a := t_0
				if a.Op != OpAMD64ADDQconst {
					continue
				}
				if a != t_1 || !(t.Uses == 1 && flagify(a)) {
					continue
				}
				v0 := b.NewValue0(t.Pos, OpSelect1, types.TypeFlags)
				v0.AddArg(a.Args[0])
				b.resetWithControl(BlockAMD64EQ, v0)
				return true
			}
			break
		}
		// match: (EQ t:(TESTL a:(ADDLconst [c] x) a))
		// cond: t.Uses == 1 && flagify(a)
		// result: (EQ (Select1 <types.TypeFlags> a.Args[0]))
		for b.Controls[0].Op == OpAMD64TESTL {
			t := b.Controls[0]
			_ = t.Args[1]
			t_0 := t.Args[0]
			t_1 := t.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, t_0, t_1 = _i0+1, t_1, t_0 {
				a := t_0
				if a.Op != OpAMD64ADDLconst {
					continue
				}
				if a != t_1 || !(t.Uses == 1 && flagify(a)) {
					continue
				}
				v0 := b.NewValue0(t.Pos, OpSelect1, types.TypeFlags)
				v0.AddArg(a.Args[0])
				b.resetWithControl(BlockAMD64EQ, v0)
				return true
			}
			break
		}
	case BlockAMD64GE:
		// match: (GE c:(CMPQconst [128] z) yes no)
		// cond: c.Uses == 1
		// result: (GT (CMPQconst [127] z) yes no)
		for b.Controls[0].Op == OpAMD64CMPQconst {
			c := b.Controls[0]
			if auxIntToInt32(c.AuxInt) != 128 {
				break
			}
			z := c.Args[0]
			if !(c.Uses == 1) {
				break
			}
			v0 := b.NewValue0(c.Pos, OpAMD64CMPQconst, types.TypeFlags)
			v0.AuxInt = int32ToAuxInt(127)
			v0.AddArg(z)
			b.resetWithControl(BlockAMD64GT, v0)
			return true
		}
		// match: (GE c:(CMPLconst [128] z) yes no)
		// cond: c.Uses == 1
		// result: (GT (CMPLconst [127] z) yes no)
		for b.Controls[0].Op == OpAMD64CMPLconst {
			c := b.Controls[0]
			if auxIntToInt32(c.AuxInt) != 128 {
				break
			}
			z := c.Args[0]
			if !(c.Uses == 1) {
				break
			}
			v0 := b.NewValue0(c.Pos, OpAMD64CMPLconst, types.TypeFlags)
			v0.AuxInt = int32ToAuxInt(127)
			v0.AddArg(z)
			b.resetWithControl(BlockAMD64GT, v0)
			return true
		}
		// match: (GE (InvertFlags cmp) yes no)
		// result: (LE cmp yes no)
		for b.Controls[0].Op == OpAMD64InvertFlags {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64LE, cmp)
			return true
		}
		// match: (GE (FlagEQ) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagEQ {
			b.Reset(BlockFirst)
			return true
		}
		// match: (GE (FlagLT_ULT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagLT_ULT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (GE (FlagLT_UGT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagLT_UGT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (GE (FlagGT_ULT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagGT_ULT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (GE (FlagGT_UGT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagGT_UGT {
			b.Reset(BlockFirst)
			return true
		}
	case BlockAMD64GT:
		// match: (GT (InvertFlags cmp) yes no)
		// result: (LT cmp yes no)
		for b.Controls[0].Op == OpAMD64InvertFlags {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64LT, cmp)
			return true
		}
		// match: (GT (FlagEQ) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagEQ {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (GT (FlagLT_ULT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagLT_ULT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (GT (FlagLT_UGT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagLT_UGT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (GT (FlagGT_ULT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagGT_ULT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (GT (FlagGT_UGT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagGT_UGT {
			b.Reset(BlockFirst)
			return true
		}
	case BlockIf:
		// match: (If (SETL cmp) yes no)
		// result: (LT cmp yes no)
		for b.Controls[0].Op == OpAMD64SETL {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64LT, cmp)
			return true
		}
		// match: (If (SETLE cmp) yes no)
		// result: (LE cmp yes no)
		for b.Controls[0].Op == OpAMD64SETLE {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64LE, cmp)
			return true
		}
		// match: (If (SETG cmp) yes no)
		// result: (GT cmp yes no)
		for b.Controls[0].Op == OpAMD64SETG {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64GT, cmp)
			return true
		}
		// match: (If (SETGE cmp) yes no)
		// result: (GE cmp yes no)
		for b.Controls[0].Op == OpAMD64SETGE {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64GE, cmp)
			return true
		}
		// match: (If (SETEQ cmp) yes no)
		// result: (EQ cmp yes no)
		for b.Controls[0].Op == OpAMD64SETEQ {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64EQ, cmp)
			return true
		}
		// match: (If (SETNE cmp) yes no)
		// result: (NE cmp yes no)
		for b.Controls[0].Op == OpAMD64SETNE {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64NE, cmp)
			return true
		}
		// match: (If (SETB cmp) yes no)
		// result: (ULT cmp yes no)
		for b.Controls[0].Op == OpAMD64SETB {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64ULT, cmp)
			return true
		}
		// match: (If (SETBE cmp) yes no)
		// result: (ULE cmp yes no)
		for b.Controls[0].Op == OpAMD64SETBE {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64ULE, cmp)
			return true
		}
		// match: (If (SETA cmp) yes no)
		// result: (UGT cmp yes no)
		for b.Controls[0].Op == OpAMD64SETA {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64UGT, cmp)
			return true
		}
		// match: (If (SETAE cmp) yes no)
		// result: (UGE cmp yes no)
		for b.Controls[0].Op == OpAMD64SETAE {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64UGE, cmp)
			return true
		}
		// match: (If (SETO cmp) yes no)
		// result: (OS cmp yes no)
		for b.Controls[0].Op == OpAMD64SETO {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64OS, cmp)
			return true
		}
		// match: (If (SETGF cmp) yes no)
		// result: (UGT cmp yes no)
		for b.Controls[0].Op == OpAMD64SETGF {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64UGT, cmp)
			return true
		}
		// match: (If (SETGEF cmp) yes no)
		// result: (UGE cmp yes no)
		for b.Controls[0].Op == OpAMD64SETGEF {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64UGE, cmp)
			return true
		}
		// match: (If (SETEQF cmp) yes no)
		// result: (EQF cmp yes no)
		for b.Controls[0].Op == OpAMD64SETEQF {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64EQF, cmp)
			return true
		}
		// match: (If (SETNEF cmp) yes no)
		// result: (NEF cmp yes no)
		for b.Controls[0].Op == OpAMD64SETNEF {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64NEF, cmp)
			return true
		}
		// match: (If cond yes no)
		// result: (NE (TESTB cond cond) yes no)
		for {
			cond := b.Controls[0]
			v0 := b.NewValue0(cond.Pos, OpAMD64TESTB, types.TypeFlags)
			v0.AddArg2(cond, cond)
			b.resetWithControl(BlockAMD64NE, v0)
			return true
		}
	case BlockJumpTable:
		// match: (JumpTable idx)
		// result: (JUMPTABLE {makeJumpTableSym(b)} idx (LEAQ <typ.Uintptr> {makeJumpTableSym(b)} (SB)))
		for {
			idx := b.Controls[0]
			v0 := b.NewValue0(b.Pos, OpAMD64LEAQ, typ.Uintptr)
			v0.Aux = symToAux(makeJumpTableSym(b))
			v1 := b.NewValue0(b.Pos, OpSB, typ.Uintptr)
			v0.AddArg(v1)
			b.resetWithControl2(BlockAMD64JUMPTABLE, idx, v0)
			b.Aux = symToAux(makeJumpTableSym(b))
			return true
		}
	case BlockAMD64LE:
		// match: (LE (InvertFlags cmp) yes no)
		// result: (GE cmp yes no)
		for b.Controls[0].Op == OpAMD64InvertFlags {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64GE, cmp)
			return true
		}
		// match: (LE (FlagEQ) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagEQ {
			b.Reset(BlockFirst)
			return true
		}
		// match: (LE (FlagLT_ULT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagLT_ULT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (LE (FlagLT_UGT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagLT_UGT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (LE (FlagGT_ULT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagGT_ULT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (LE (FlagGT_UGT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagGT_UGT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
	case BlockAMD64LT:
		// match: (LT c:(CMPQconst [128] z) yes no)
		// cond: c.Uses == 1
		// result: (LE (CMPQconst [127] z) yes no)
		for b.Controls[0].Op == OpAMD64CMPQconst {
			c := b.Controls[0]
			if auxIntToInt32(c.AuxInt) != 128 {
				break
			}
			z := c.Args[0]
			if !(c.Uses == 1) {
				break
			}
			v0 := b.NewValue0(c.Pos, OpAMD64CMPQconst, types.TypeFlags)
			v0.AuxInt = int32ToAuxInt(127)
			v0.AddArg(z)
			b.resetWithControl(BlockAMD64LE, v0)
			return true
		}
		// match: (LT c:(CMPLconst [128] z) yes no)
		// cond: c.Uses == 1
		// result: (LE (CMPLconst [127] z) yes no)
		for b.Controls[0].Op == OpAMD64CMPLconst {
			c := b.Controls[0]
			if auxIntToInt32(c.AuxInt) != 128 {
				break
			}
			z := c.Args[0]
			if !(c.Uses == 1) {
				break
			}
			v0 := b.NewValue0(c.Pos, OpAMD64CMPLconst, types.TypeFlags)
			v0.AuxInt = int32ToAuxInt(127)
			v0.AddArg(z)
			b.resetWithControl(BlockAMD64LE, v0)
			return true
		}
		// match: (LT (InvertFlags cmp) yes no)
		// result: (GT cmp yes no)
		for b.Controls[0].Op == OpAMD64InvertFlags {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64GT, cmp)
			return true
		}
		// match: (LT (FlagEQ) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagEQ {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (LT (FlagLT_ULT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagLT_ULT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (LT (FlagLT_UGT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagLT_UGT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (LT (FlagGT_ULT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagGT_ULT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (LT (FlagGT_UGT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagGT_UGT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
	case BlockAMD64NE:
		// match: (NE (TESTB (SETL cmp) (SETL cmp)) yes no)
		// result: (LT cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETL {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETL || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64LT, cmp)
			return true
		}
		// match: (NE (TESTB (SETLE cmp) (SETLE cmp)) yes no)
		// result: (LE cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETLE {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETLE || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64LE, cmp)
			return true
		}
		// match: (NE (TESTB (SETG cmp) (SETG cmp)) yes no)
		// result: (GT cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETG {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETG || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64GT, cmp)
			return true
		}
		// match: (NE (TESTB (SETGE cmp) (SETGE cmp)) yes no)
		// result: (GE cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETGE {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETGE || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64GE, cmp)
			return true
		}
		// match: (NE (TESTB (SETEQ cmp) (SETEQ cmp)) yes no)
		// result: (EQ cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETEQ {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETEQ || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64EQ, cmp)
			return true
		}
		// match: (NE (TESTB (SETNE cmp) (SETNE cmp)) yes no)
		// result: (NE cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETNE {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETNE || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64NE, cmp)
			return true
		}
		// match: (NE (TESTB (SETB cmp) (SETB cmp)) yes no)
		// result: (ULT cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETB {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETB || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64ULT, cmp)
			return true
		}
		// match: (NE (TESTB (SETBE cmp) (SETBE cmp)) yes no)
		// result: (ULE cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETBE {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETBE || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64ULE, cmp)
			return true
		}
		// match: (NE (TESTB (SETA cmp) (SETA cmp)) yes no)
		// result: (UGT cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETA {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETA || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64UGT, cmp)
			return true
		}
		// match: (NE (TESTB (SETAE cmp) (SETAE cmp)) yes no)
		// result: (UGE cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETAE {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETAE || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64UGE, cmp)
			return true
		}
		// match: (NE (TESTB (SETO cmp) (SETO cmp)) yes no)
		// result: (OS cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETO {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETO || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64OS, cmp)
			return true
		}
		// match: (NE (TESTL (SHLL (MOVLconst [1]) x) y))
		// result: (ULT (BTL x y))
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				if v_0_0.Op != OpAMD64SHLL {
					continue
				}
				x := v_0_0.Args[1]
				v_0_0_0 := v_0_0.Args[0]
				if v_0_0_0.Op != OpAMD64MOVLconst || auxIntToInt32(v_0_0_0.AuxInt) != 1 {
					continue
				}
				y := v_0_1
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTL, types.TypeFlags)
				v0.AddArg2(x, y)
				b.resetWithControl(BlockAMD64ULT, v0)
				return true
			}
			break
		}
		// match: (NE (TESTQ (SHLQ (MOVQconst [1]) x) y))
		// result: (ULT (BTQ x y))
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				if v_0_0.Op != OpAMD64SHLQ {
					continue
				}
				x := v_0_0.Args[1]
				v_0_0_0 := v_0_0.Args[0]
				if v_0_0_0.Op != OpAMD64MOVQconst || auxIntToInt64(v_0_0_0.AuxInt) != 1 {
					continue
				}
				y := v_0_1
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQ, types.TypeFlags)
				v0.AddArg2(x, y)
				b.resetWithControl(BlockAMD64ULT, v0)
				return true
			}
			break
		}
		// match: (NE (TESTLconst [c] x))
		// cond: isUint32PowerOfTwo(int64(c))
		// result: (ULT (BTLconst [int8(log32(c))] x))
		for b.Controls[0].Op == OpAMD64TESTLconst {
			v_0 := b.Controls[0]
			c := auxIntToInt32(v_0.AuxInt)
			x := v_0.Args[0]
			if !(isUint32PowerOfTwo(int64(c))) {
				break
			}
			v0 := b.NewValue0(v_0.Pos, OpAMD64BTLconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(int8(log32(c)))
			v0.AddArg(x)
			b.resetWithControl(BlockAMD64ULT, v0)
			return true
		}
		// match: (NE (TESTQconst [c] x))
		// cond: isUint64PowerOfTwo(int64(c))
		// result: (ULT (BTQconst [int8(log32(c))] x))
		for b.Controls[0].Op == OpAMD64TESTQconst {
			v_0 := b.Controls[0]
			c := auxIntToInt32(v_0.AuxInt)
			x := v_0.Args[0]
			if !(isUint64PowerOfTwo(int64(c))) {
				break
			}
			v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
			v0.AuxInt = int8ToAuxInt(int8(log32(c)))
			v0.AddArg(x)
			b.resetWithControl(BlockAMD64ULT, v0)
			return true
		}
		// match: (NE (TESTQ (MOVQconst [c]) x))
		// cond: isUint64PowerOfTwo(c)
		// result: (ULT (BTQconst [int8(log64(c))] x))
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				if v_0_0.Op != OpAMD64MOVQconst {
					continue
				}
				c := auxIntToInt64(v_0_0.AuxInt)
				x := v_0_1
				if !(isUint64PowerOfTwo(c)) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(int8(log64(c)))
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64ULT, v0)
				return true
			}
			break
		}
		// match: (NE (TESTQ z1:(SHLQconst [63] (SHRQconst [63] x)) z2))
		// cond: z1==z2
		// result: (ULT (BTQconst [63] x))
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHLQconst || auxIntToInt8(z1.AuxInt) != 63 {
					continue
				}
				z1_0 := z1.Args[0]
				if z1_0.Op != OpAMD64SHRQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
					continue
				}
				x := z1_0.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(63)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64ULT, v0)
				return true
			}
			break
		}
		// match: (NE (TESTL z1:(SHLLconst [31] (SHRQconst [31] x)) z2))
		// cond: z1==z2
		// result: (ULT (BTQconst [31] x))
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHLLconst || auxIntToInt8(z1.AuxInt) != 31 {
					continue
				}
				z1_0 := z1.Args[0]
				if z1_0.Op != OpAMD64SHRQconst || auxIntToInt8(z1_0.AuxInt) != 31 {
					continue
				}
				x := z1_0.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(31)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64ULT, v0)
				return true
			}
			break
		}
		// match: (NE (TESTQ z1:(SHRQconst [63] (SHLQconst [63] x)) z2))
		// cond: z1==z2
		// result: (ULT (BTQconst [0] x))
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
					continue
				}
				z1_0 := z1.Args[0]
				if z1_0.Op != OpAMD64SHLQconst || auxIntToInt8(z1_0.AuxInt) != 63 {
					continue
				}
				x := z1_0.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(0)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64ULT, v0)
				return true
			}
			break
		}
		// match: (NE (TESTL z1:(SHRLconst [31] (SHLLconst [31] x)) z2))
		// cond: z1==z2
		// result: (ULT (BTLconst [0] x))
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
					continue
				}
				z1_0 := z1.Args[0]
				if z1_0.Op != OpAMD64SHLLconst || auxIntToInt8(z1_0.AuxInt) != 31 {
					continue
				}
				x := z1_0.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTLconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(0)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64ULT, v0)
				return true
			}
			break
		}
		// match: (NE (TESTQ z1:(SHRQconst [63] x) z2))
		// cond: z1==z2
		// result: (ULT (BTQconst [63] x))
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHRQconst || auxIntToInt8(z1.AuxInt) != 63 {
					continue
				}
				x := z1.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTQconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(63)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64ULT, v0)
				return true
			}
			break
		}
		// match: (NE (TESTL z1:(SHRLconst [31] x) z2))
		// cond: z1==z2
		// result: (ULT (BTLconst [31] x))
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				z1 := v_0_0
				if z1.Op != OpAMD64SHRLconst || auxIntToInt8(z1.AuxInt) != 31 {
					continue
				}
				x := z1.Args[0]
				z2 := v_0_1
				if !(z1 == z2) {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpAMD64BTLconst, types.TypeFlags)
				v0.AuxInt = int8ToAuxInt(31)
				v0.AddArg(x)
				b.resetWithControl(BlockAMD64ULT, v0)
				return true
			}
			break
		}
		// match: (NE (TESTB (SETGF cmp) (SETGF cmp)) yes no)
		// result: (UGT cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETGF {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETGF || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64UGT, cmp)
			return true
		}
		// match: (NE (TESTB (SETGEF cmp) (SETGEF cmp)) yes no)
		// result: (UGE cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETGEF {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETGEF || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64UGE, cmp)
			return true
		}
		// match: (NE (TESTB (SETEQF cmp) (SETEQF cmp)) yes no)
		// result: (EQF cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETEQF {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETEQF || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64EQF, cmp)
			return true
		}
		// match: (NE (TESTB (SETNEF cmp) (SETNEF cmp)) yes no)
		// result: (NEF cmp yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			if v_0_0.Op != OpAMD64SETNEF {
				break
			}
			cmp := v_0_0.Args[0]
			v_0_1 := v_0.Args[1]
			if v_0_1.Op != OpAMD64SETNEF || cmp != v_0_1.Args[0] {
				break
			}
			b.resetWithControl(BlockAMD64NEF, cmp)
			return true
		}
		// match: (NE (InvertFlags cmp) yes no)
		// result: (NE cmp yes no)
		for b.Controls[0].Op == OpAMD64InvertFlags {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64NE, cmp)
			return true
		}
		// match: (NE (FlagEQ) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagEQ {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (NE (FlagLT_ULT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagLT_ULT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (NE (FlagLT_UGT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagLT_UGT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (NE (FlagGT_ULT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagGT_ULT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (NE (FlagGT_UGT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagGT_UGT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (NE (TESTQ s:(Select0 blsr:(BLSRQ _)) s) yes no)
		// result: (NE (Select1 <types.TypeFlags> blsr) yes no)
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				s := v_0_0
				if s.Op != OpSelect0 {
					continue
				}
				blsr := s.Args[0]
				if blsr.Op != OpAMD64BLSRQ || s != v_0_1 {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpSelect1, types.TypeFlags)
				v0.AddArg(blsr)
				b.resetWithControl(BlockAMD64NE, v0)
				return true
			}
			break
		}
		// match: (NE (TESTL s:(Select0 blsr:(BLSRL _)) s) yes no)
		// result: (NE (Select1 <types.TypeFlags> blsr) yes no)
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			_ = v_0.Args[1]
			v_0_0 := v_0.Args[0]
			v_0_1 := v_0.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
				s := v_0_0
				if s.Op != OpSelect0 {
					continue
				}
				blsr := s.Args[0]
				if blsr.Op != OpAMD64BLSRL || s != v_0_1 {
					continue
				}
				v0 := b.NewValue0(v_0.Pos, OpSelect1, types.TypeFlags)
				v0.AddArg(blsr)
				b.resetWithControl(BlockAMD64NE, v0)
				return true
			}
			break
		}
		// match: (NE t:(TESTQ a:(ADDQconst [c] x) a))
		// cond: t.Uses == 1 && flagify(a)
		// result: (NE (Select1 <types.TypeFlags> a.Args[0]))
		for b.Controls[0].Op == OpAMD64TESTQ {
			t := b.Controls[0]
			_ = t.Args[1]
			t_0 := t.Args[0]
			t_1 := t.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, t_0, t_1 = _i0+1, t_1, t_0 {
				a := t_0
				if a.Op != OpAMD64ADDQconst {
					continue
				}
				if a != t_1 || !(t.Uses == 1 && flagify(a)) {
					continue
				}
				v0 := b.NewValue0(t.Pos, OpSelect1, types.TypeFlags)
				v0.AddArg(a.Args[0])
				b.resetWithControl(BlockAMD64NE, v0)
				return true
			}
			break
		}
		// match: (NE t:(TESTL a:(ADDLconst [c] x) a))
		// cond: t.Uses == 1 && flagify(a)
		// result: (NE (Select1 <types.TypeFlags> a.Args[0]))
		for b.Controls[0].Op == OpAMD64TESTL {
			t := b.Controls[0]
			_ = t.Args[1]
			t_0 := t.Args[0]
			t_1 := t.Args[1]
			for _i0 := 0; _i0 <= 1; _i0, t_0, t_1 = _i0+1, t_1, t_0 {
				a := t_0
				if a.Op != OpAMD64ADDLconst {
					continue
				}
				if a != t_1 || !(t.Uses == 1 && flagify(a)) {
					continue
				}
				v0 := b.NewValue0(t.Pos, OpSelect1, types.TypeFlags)
				v0.AddArg(a.Args[0])
				b.resetWithControl(BlockAMD64NE, v0)
				return true
			}
			break
		}
	case BlockAMD64UGE:
		// match: (UGE (TESTQ x x) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			x := v_0.Args[1]
			if x != v_0.Args[0] {
				break
			}
			b.Reset(BlockFirst)
			return true
		}
		// match: (UGE (TESTL x x) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			x := v_0.Args[1]
			if x != v_0.Args[0] {
				break
			}
			b.Reset(BlockFirst)
			return true
		}
		// match: (UGE (TESTW x x) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64TESTW {
			v_0 := b.Controls[0]
			x := v_0.Args[1]
			if x != v_0.Args[0] {
				break
			}
			b.Reset(BlockFirst)
			return true
		}
		// match: (UGE (TESTB x x) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			x := v_0.Args[1]
			if x != v_0.Args[0] {
				break
			}
			b.Reset(BlockFirst)
			return true
		}
		// match: (UGE (InvertFlags cmp) yes no)
		// result: (ULE cmp yes no)
		for b.Controls[0].Op == OpAMD64InvertFlags {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64ULE, cmp)
			return true
		}
		// match: (UGE (FlagEQ) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagEQ {
			b.Reset(BlockFirst)
			return true
		}
		// match: (UGE (FlagLT_ULT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagLT_ULT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (UGE (FlagLT_UGT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagLT_UGT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (UGE (FlagGT_ULT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagGT_ULT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (UGE (FlagGT_UGT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagGT_UGT {
			b.Reset(BlockFirst)
			return true
		}
	case BlockAMD64UGT:
		// match: (UGT (InvertFlags cmp) yes no)
		// result: (ULT cmp yes no)
		for b.Controls[0].Op == OpAMD64InvertFlags {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64ULT, cmp)
			return true
		}
		// match: (UGT (FlagEQ) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagEQ {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (UGT (FlagLT_ULT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagLT_ULT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (UGT (FlagLT_UGT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagLT_UGT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (UGT (FlagGT_ULT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagGT_ULT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (UGT (FlagGT_UGT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagGT_UGT {
			b.Reset(BlockFirst)
			return true
		}
	case BlockAMD64ULE:
		// match: (ULE (InvertFlags cmp) yes no)
		// result: (UGE cmp yes no)
		for b.Controls[0].Op == OpAMD64InvertFlags {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64UGE, cmp)
			return true
		}
		// match: (ULE (FlagEQ) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagEQ {
			b.Reset(BlockFirst)
			return true
		}
		// match: (ULE (FlagLT_ULT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagLT_ULT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (ULE (FlagLT_UGT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagLT_UGT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (ULE (FlagGT_ULT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagGT_ULT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (ULE (FlagGT_UGT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagGT_UGT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
	case BlockAMD64ULT:
		// match: (ULT (TESTQ x x) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64TESTQ {
			v_0 := b.Controls[0]
			x := v_0.Args[1]
			if x != v_0.Args[0] {
				break
			}
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (ULT (TESTL x x) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64TESTL {
			v_0 := b.Controls[0]
			x := v_0.Args[1]
			if x != v_0.Args[0] {
				break
			}
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (ULT (TESTW x x) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64TESTW {
			v_0 := b.Controls[0]
			x := v_0.Args[1]
			if x != v_0.Args[0] {
				break
			}
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (ULT (TESTB x x) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64TESTB {
			v_0 := b.Controls[0]
			x := v_0.Args[1]
			if x != v_0.Args[0] {
				break
			}
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (ULT (InvertFlags cmp) yes no)
		// result: (UGT cmp yes no)
		for b.Controls[0].Op == OpAMD64InvertFlags {
			v_0 := b.Controls[0]
			cmp := v_0.Args[0]
			b.resetWithControl(BlockAMD64UGT, cmp)
			return true
		}
		// match: (ULT (FlagEQ) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagEQ {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (ULT (FlagLT_ULT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagLT_ULT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (ULT (FlagLT_UGT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagLT_UGT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
		// match: (ULT (FlagGT_ULT) yes no)
		// result: (First yes no)
		for b.Controls[0].Op == OpAMD64FlagGT_ULT {
			b.Reset(BlockFirst)
			return true
		}
		// match: (ULT (FlagGT_UGT) yes no)
		// result: (First no yes)
		for b.Controls[0].Op == OpAMD64FlagGT_UGT {
			b.Reset(BlockFirst)
			b.swapSuccessors()
			return true
		}
	}
	return false
}
