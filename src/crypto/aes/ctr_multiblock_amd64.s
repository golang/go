// Code generated by ctr_multiblock_amd64_gen.go. DO NOT EDIT.

#include "textflag.h"

// See https://golang.org/src/crypto/aes/gcm_amd64.s
#define NR CX
#define XK AX
#define DST DX
#define SRC R10
#define IV_PTR BX
#define BLOCK_INDEX R11
#define IV_LOW_LE R12
#define IV_HIGH_LE R13
#define BSWAP X15

DATA bswapMask<>+0x00(SB)/8, $0x08090a0b0c0d0e0f
DATA bswapMask<>+0x08(SB)/8, $0x0001020304050607

GLOBL bswapMask<>(SB), (NOPTR+RODATA), $16

// func ctrBlocks1Asm(nr int, xk *uint32, dst, src, ivRev *byte, blockIndex uint64)
TEXT ·ctrBlocks1Asm(SB), NOSPLIT, $0
	MOVQ   nr+0(FP), NR
	MOVQ   xk+8(FP), XK
	MOVUPS 0(XK), X1
	MOVQ   dst+16(FP), DST
	MOVQ   src+24(FP), SRC
	MOVQ   ivRev+32(FP), IV_PTR
	MOVQ   0(IV_PTR), IV_LOW_LE
	MOVQ   8(IV_PTR), IV_HIGH_LE
	MOVQ   blockIndex+40(FP), BLOCK_INDEX

	MOVOU bswapMask<>(SB), BSWAP

	ADDQ BLOCK_INDEX, IV_LOW_LE
	ADCQ $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X0
	PINSRQ $1, IV_HIGH_LE, X0
	PSHUFB BSWAP, X0

	PXOR X1, X0

	ADDQ $16, XK

	SUBQ $12, NR
	JE   Lenc192
	JB   Lenc128

Lenc256:
	MOVUPS 0(XK), X1
	AESENC X1, X0

	MOVUPS 16(XK), X1
	AESENC X1, X0

	ADDQ $32, XK

Lenc192:
	MOVUPS 0(XK), X1
	AESENC X1, X0

	MOVUPS 16(XK), X1
	AESENC X1, X0

	ADDQ $32, XK

Lenc128:
	MOVUPS 0(XK), X1
	AESENC X1, X0

	MOVUPS 16(XK), X1
	AESENC X1, X0

	MOVUPS 32(XK), X1
	AESENC X1, X0

	MOVUPS 48(XK), X1
	AESENC X1, X0

	MOVUPS 64(XK), X1
	AESENC X1, X0

	MOVUPS 80(XK), X1
	AESENC X1, X0

	MOVUPS 96(XK), X1
	AESENC X1, X0

	MOVUPS 112(XK), X1
	AESENC X1, X0

	MOVUPS 128(XK), X1
	AESENC X1, X0

	MOVUPS     144(XK), X1
	AESENCLAST X1, X0

	MOVUPS 0(SRC), X8
	PXOR   X0, X8
	MOVUPS X8, 0(DST)

	RET

// func ctrBlocks2Asm(nr int, xk *uint32, dst, src, ivRev *byte, blockIndex uint64)
TEXT ·ctrBlocks2Asm(SB), NOSPLIT, $0
	MOVQ   nr+0(FP), NR
	MOVQ   xk+8(FP), XK
	MOVUPS 0(XK), X2
	MOVQ   dst+16(FP), DST
	MOVQ   src+24(FP), SRC
	MOVQ   ivRev+32(FP), IV_PTR
	MOVQ   0(IV_PTR), IV_LOW_LE
	MOVQ   8(IV_PTR), IV_HIGH_LE
	MOVQ   blockIndex+40(FP), BLOCK_INDEX

	MOVOU bswapMask<>(SB), BSWAP

	ADDQ BLOCK_INDEX, IV_LOW_LE
	ADCQ $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X0
	PINSRQ $1, IV_HIGH_LE, X0
	PSHUFB BSWAP, X0
	ADDQ   $1, IV_LOW_LE
	ADCQ   $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X1
	PINSRQ $1, IV_HIGH_LE, X1
	PSHUFB BSWAP, X1

	PXOR X2, X0
	PXOR X2, X1

	ADDQ $16, XK

	SUBQ $12, NR
	JE   Lenc192
	JB   Lenc128

Lenc256:
	MOVUPS 0(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	MOVUPS 16(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	ADDQ $32, XK

Lenc192:
	MOVUPS 0(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	MOVUPS 16(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	ADDQ $32, XK

Lenc128:
	MOVUPS 0(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	MOVUPS 16(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	MOVUPS 32(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	MOVUPS 48(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	MOVUPS 64(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	MOVUPS 80(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	MOVUPS 96(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	MOVUPS 112(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	MOVUPS 128(XK), X2
	AESENC X2, X0
	AESENC X2, X1

	MOVUPS     144(XK), X2
	AESENCLAST X2, X0
	AESENCLAST X2, X1

	MOVUPS 0(SRC), X8
	PXOR   X0, X8
	MOVUPS X8, 0(DST)

	MOVUPS 16(SRC), X9
	PXOR   X1, X9
	MOVUPS X9, 16(DST)

	RET

// func ctrBlocks4Asm(nr int, xk *uint32, dst, src, ivRev *byte, blockIndex uint64)
TEXT ·ctrBlocks4Asm(SB), NOSPLIT, $0
	MOVQ   nr+0(FP), NR
	MOVQ   xk+8(FP), XK
	MOVUPS 0(XK), X4
	MOVQ   dst+16(FP), DST
	MOVQ   src+24(FP), SRC
	MOVQ   ivRev+32(FP), IV_PTR
	MOVQ   0(IV_PTR), IV_LOW_LE
	MOVQ   8(IV_PTR), IV_HIGH_LE
	MOVQ   blockIndex+40(FP), BLOCK_INDEX

	MOVOU bswapMask<>(SB), BSWAP

	ADDQ BLOCK_INDEX, IV_LOW_LE
	ADCQ $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X0
	PINSRQ $1, IV_HIGH_LE, X0
	PSHUFB BSWAP, X0
	ADDQ   $1, IV_LOW_LE
	ADCQ   $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X1
	PINSRQ $1, IV_HIGH_LE, X1
	PSHUFB BSWAP, X1
	ADDQ   $1, IV_LOW_LE
	ADCQ   $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X2
	PINSRQ $1, IV_HIGH_LE, X2
	PSHUFB BSWAP, X2
	ADDQ   $1, IV_LOW_LE
	ADCQ   $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X3
	PINSRQ $1, IV_HIGH_LE, X3
	PSHUFB BSWAP, X3

	PXOR X4, X0
	PXOR X4, X1
	PXOR X4, X2
	PXOR X4, X3

	ADDQ $16, XK

	SUBQ $12, NR
	JE   Lenc192
	JB   Lenc128

Lenc256:
	MOVUPS 0(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	MOVUPS 16(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	ADDQ $32, XK

Lenc192:
	MOVUPS 0(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	MOVUPS 16(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	ADDQ $32, XK

Lenc128:
	MOVUPS 0(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	MOVUPS 16(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	MOVUPS 32(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	MOVUPS 48(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	MOVUPS 64(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	MOVUPS 80(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	MOVUPS 96(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	MOVUPS 112(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	MOVUPS 128(XK), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3

	MOVUPS     144(XK), X4
	AESENCLAST X4, X0
	AESENCLAST X4, X1
	AESENCLAST X4, X2
	AESENCLAST X4, X3

	MOVUPS 0(SRC), X8
	PXOR   X0, X8
	MOVUPS X8, 0(DST)

	MOVUPS 16(SRC), X9
	PXOR   X1, X9
	MOVUPS X9, 16(DST)

	MOVUPS 32(SRC), X10
	PXOR   X2, X10
	MOVUPS X10, 32(DST)

	MOVUPS 48(SRC), X11
	PXOR   X3, X11
	MOVUPS X11, 48(DST)

	RET

// func ctrBlocks8Asm(nr int, xk *uint32, dst, src, ivRev *byte, blockIndex uint64)
TEXT ·ctrBlocks8Asm(SB), NOSPLIT, $0
	MOVQ   nr+0(FP), NR
	MOVQ   xk+8(FP), XK
	MOVUPS 0(XK), X8
	MOVQ   dst+16(FP), DST
	MOVQ   src+24(FP), SRC
	MOVQ   ivRev+32(FP), IV_PTR
	MOVQ   0(IV_PTR), IV_LOW_LE
	MOVQ   8(IV_PTR), IV_HIGH_LE
	MOVQ   blockIndex+40(FP), BLOCK_INDEX

	MOVOU bswapMask<>(SB), BSWAP

	ADDQ BLOCK_INDEX, IV_LOW_LE
	ADCQ $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X0
	PINSRQ $1, IV_HIGH_LE, X0
	PSHUFB BSWAP, X0
	ADDQ   $1, IV_LOW_LE
	ADCQ   $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X1
	PINSRQ $1, IV_HIGH_LE, X1
	PSHUFB BSWAP, X1
	ADDQ   $1, IV_LOW_LE
	ADCQ   $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X2
	PINSRQ $1, IV_HIGH_LE, X2
	PSHUFB BSWAP, X2
	ADDQ   $1, IV_LOW_LE
	ADCQ   $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X3
	PINSRQ $1, IV_HIGH_LE, X3
	PSHUFB BSWAP, X3
	ADDQ   $1, IV_LOW_LE
	ADCQ   $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X4
	PINSRQ $1, IV_HIGH_LE, X4
	PSHUFB BSWAP, X4
	ADDQ   $1, IV_LOW_LE
	ADCQ   $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X5
	PINSRQ $1, IV_HIGH_LE, X5
	PSHUFB BSWAP, X5
	ADDQ   $1, IV_LOW_LE
	ADCQ   $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X6
	PINSRQ $1, IV_HIGH_LE, X6
	PSHUFB BSWAP, X6
	ADDQ   $1, IV_LOW_LE
	ADCQ   $0, IV_HIGH_LE

	MOVQ   IV_LOW_LE, X7
	PINSRQ $1, IV_HIGH_LE, X7
	PSHUFB BSWAP, X7

	PXOR X8, X0
	PXOR X8, X1
	PXOR X8, X2
	PXOR X8, X3
	PXOR X8, X4
	PXOR X8, X5
	PXOR X8, X6
	PXOR X8, X7

	ADDQ $16, XK

	SUBQ $12, NR
	JE   Lenc192
	JB   Lenc128

Lenc256:
	MOVUPS 0(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	MOVUPS 16(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	ADDQ $32, XK

Lenc192:
	MOVUPS 0(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	MOVUPS 16(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	ADDQ $32, XK

Lenc128:
	MOVUPS 0(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	MOVUPS 16(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	MOVUPS 32(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	MOVUPS 48(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	MOVUPS 64(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	MOVUPS 80(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	MOVUPS 96(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	MOVUPS 112(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	MOVUPS 128(XK), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7

	MOVUPS     144(XK), X8
	AESENCLAST X8, X0
	AESENCLAST X8, X1
	AESENCLAST X8, X2
	AESENCLAST X8, X3
	AESENCLAST X8, X4
	AESENCLAST X8, X5
	AESENCLAST X8, X6
	AESENCLAST X8, X7

	MOVUPS 0(SRC), X8
	PXOR   X0, X8
	MOVUPS X8, 0(DST)

	MOVUPS 16(SRC), X9
	PXOR   X1, X9
	MOVUPS X9, 16(DST)

	MOVUPS 32(SRC), X10
	PXOR   X2, X10
	MOVUPS X10, 32(DST)

	MOVUPS 48(SRC), X11
	PXOR   X3, X11
	MOVUPS X11, 48(DST)

	MOVUPS 64(SRC), X12
	PXOR   X4, X12
	MOVUPS X12, 64(DST)

	MOVUPS 80(SRC), X13
	PXOR   X5, X13
	MOVUPS X13, 80(DST)

	MOVUPS 96(SRC), X14
	PXOR   X6, X14
	MOVUPS X14, 96(DST)

	MOVUPS 112(SRC), X15
	PXOR   X7, X15
	MOVUPS X15, 112(DST)

	RET

// func rev16Asm(iv *byte)
TEXT ·rev16Asm(SB), NOSPLIT, $0
	MOVQ   iv+0(FP), IV_PTR
	MOVUPS 0(IV_PTR), X0
	MOVOU  bswapMask<>(SB), BSWAP
	PSHUFB BSWAP, X0
	MOVUPS X0, 0(IV_PTR)

	RET
